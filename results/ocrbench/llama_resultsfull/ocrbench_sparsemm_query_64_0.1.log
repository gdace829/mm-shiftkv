The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/sjs/.conda/envs/vtd/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[32m2026-01-06 09:25:57.559[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m324[0m - [1mVerbosity set to DEBUG[0m
[32m2026-01-06 09:25:57.659[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m452[0m - [34m[1m`group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lmms-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.[0m
[32m2026-01-06 09:25:57.670[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-06 09:25:57.672[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-06 09:25:57.825[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m478[0m - [34m[1mFile illusionvqa.yaml in /home/sjs/MM-ShiftKV/lmms_eval/tasks/illusionvqa could not be loaded as a task or group[0m
[32m2026-01-06 09:25:59.126[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m407[0m - [1mEvaluation tracker args: {'output_path': './logs/'}[0m
[32m2026-01-06 09:25:59.127[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m496[0m - [1mSelected Tasks: ['ocrbench'][0m
[32m2026-01-06 09:25:59.127[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-06 09:25:59.127[0m | [33m[1mWARNING [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m161[0m - [33m[1mgeneration_kwargs specified through cli, these settings will be used over set parameters in yaml tasks.[0m
sparsemm_query
ssss
Use Full KV
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:02,  1.65it/s]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:01,  1.72it/s]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:01,  1.79it/s]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:02<00:00,  1.76it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.36it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.04it/s]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[32m2026-01-06 09:26:18.161[0m | [1mINFO    [0m | [36mlmms_eval.evaluator_utils[0m:[36mfrom_taskdict[0m:[36m91[0m - [1mNo metadata found in task config for ocrbench, using default n_shot=0[0m
[32m2026-01-06 09:26:18.161[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.caching.cache[0m:[36mload_from_cache[0m:[36m34[0m - [34m[1mrequests-ocrbench-0shot-rank0-world_size1-tokenizer is not cached, generating...[0m
[32m2026-01-06 09:26:18.161[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for ocrbench on rank 0...[0m
  0%|          | 0/1000 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 33347.68it/s]
[32m2026-01-06 09:26:18.192[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m426[0m - [34m[1mTask: ocrbench; number of requests on this rank: 1000[0m
[32m2026-01-06 09:26:18.192[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m
Model Responding:   0%|          | 0/1000 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/sjs/.conda/envs/vtd/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sjs/.conda/envs/vtd/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/sjs/MM-ShiftKV/lmms_eval/__main__.py", line 562, in <module>
    cli_evaluate()
  File "/home/sjs/MM-ShiftKV/lmms_eval/__main__.py", line 375, in cli_evaluate
    raise e
  File "/home/sjs/MM-ShiftKV/lmms_eval/__main__.py", line 359, in cli_evaluate
    results, samples = cli_evaluate_single(args)
  File "/home/sjs/MM-ShiftKV/lmms_eval/__main__.py", line 500, in cli_evaluate_single
    results = evaluator.simple_evaluate(
  File "/home/sjs/MM-ShiftKV/lmms_eval/utils.py", line 533, in _wrapper
    return fn(*args, **kwargs)
  File "/home/sjs/MM-ShiftKV/lmms_eval/evaluator.py", line 248, in simple_evaluate
    results = evaluate(
  File "/home/sjs/MM-ShiftKV/lmms_eval/utils.py", line 533, in _wrapper
    return fn(*args, **kwargs)
  File "/home/sjs/MM-ShiftKV/lmms_eval/evaluator.py", line 458, in evaluate
    resps = getattr(lm, reqtype)(cloned_reqs)  # Choiszt run generate until
  File "/home/sjs/MM-ShiftKV/lmms_eval/models/qwen2_5_vl.py", line 365, in generate_until
    cont = self.model.generate(
  File "/home/sjs/.conda/envs/vtd/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/home/sjs/.conda/envs/vtd/lib/python3.10/site-packages/transformers/generation/utils.py", line 2086, in generate
    self._validate_model_kwargs(model_kwargs.copy())
  File "/home/sjs/.conda/envs/vtd/lib/python3.10/site-packages/transformers/generation/utils.py", line 1450, in _validate_model_kwargs
    raise ValueError(
ValueError: The following `model_kwargs` are not used by the model: ['image_token_positions', 'image_token_lengths'] (note: typos in the generate arguments will also show up in this list)
Model Responding:   0%|          | 0/1000 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/sjs/.conda/envs/vtd/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sjs/.conda/envs/vtd/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/sjs/.conda/envs/vtd/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1291, in <module>
    main()
  File "/home/sjs/.conda/envs/vtd/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1287, in main
    launch_command(args)
  File "/home/sjs/.conda/envs/vtd/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1281, in launch_command
    simple_launcher(args)
  File "/home/sjs/.conda/envs/vtd/lib/python3.10/site-packages/accelerate/commands/launch.py", line 869, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/sjs/.conda/envs/vtd/bin/python3', '-m', 'lmms_eval', '--model', 'qwen2_5_vl', '--model_args', 'pretrained=Qwen/Qwen2.5-VL-7B-Instruct,use_flash_attention_2=True', '--tasks', 'ocrbench', '--batch_size', '1', '--log_samples', '--log_samples_suffix', 'llava_v1.6_mix', '--output_path', './logs/', '--gen_kwargs', 'temperature=0', '--verbosity=DEBUG']' returned non-zero exit status 1.
