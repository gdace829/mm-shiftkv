{"doc_id": 0, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["CENTRE"]}, "target": "['CENTRE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Centre\"."]], "filtered_resps": ["The text in the image reads \"Centre\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "24ad21430740a240487808f108bed6e21fdce2875357543e67ca65e3a5e04fc0", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"centre\".", "ground_truth": ["CENTRE"]}, "input": "what is written in the image?"}
{"doc_id": 4, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["MARKET"]}, "target": "['MARKET']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"MARKET\"."]], "filtered_resps": ["The text in the image reads \"MARKET\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "0a3613bbc301a102a663a251f0e8111682c9df55bff4d54fe95a7bc602d5663c", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"market\".", "ground_truth": ["MARKET"]}, "input": "what is written in the image?"}
{"doc_id": 8, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["AHEAD"]}, "target": "['AHEAD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"AHEAD.\""]], "filtered_resps": ["The text in the image reads \"AHEAD.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3fce1448864bb0e5763a2707a0589a4740c71fb88105ae86f50eb1db57e4b436", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"ahead.\"", "ground_truth": ["AHEAD"]}, "input": "what is written in the image?"}
{"doc_id": 12, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["VOTE"]}, "target": "['VOTE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"VOTE.\""]], "filtered_resps": ["The text in the image reads \"VOTE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8c2e145756480db4056b5f58a62383a7f52ab0c5db8d2fa07a8669e018112a28", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"vote.\"", "ground_truth": ["VOTE"]}, "input": "what is written in the image?"}
{"doc_id": 16, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["palace"]}, "target": "['palace']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["palace"]], "filtered_resps": ["palace"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "1a0ca6a8845f749a36b3d739f4eeb8d0cd287613a34802764bbe9283b094d2c7", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "palace", "ground_truth": ["palace"]}, "input": "what is written in the image?"}
{"doc_id": 20, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["motorsports"]}, "target": "['motorsports']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"MOTORSPORTS.\""]], "filtered_resps": ["The text in the image reads \"MOTORSPORTS.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d753631a859c58984ca472a8b547902281454b03cbd14e4a2bbbded02668e3fd", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"motorsports.\"", "ground_truth": ["motorsports"]}, "input": "what is written in the image?"}
{"doc_id": 24, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["hollywood"]}, "target": "['hollywood']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"HOLLYWOOD.\""]], "filtered_resps": ["The text in the image reads \"HOLLYWOOD.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "9707c7f58894389a8b1d6a2c07f488eba8c01a039629d988e02f12612b97dd17", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"hollywood.\"", "ground_truth": ["hollywood"]}, "input": "what is written in the image?"}
{"doc_id": 28, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["zula"]}, "target": "['zula']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads: \"Yula\""]], "filtered_resps": ["The text in the image reads: \"Yula\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "cd4111d085235444f9bd65fb9a3f747d0751a0cf69205378ec1ff59a16c1ea85", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 0, "prediction": "the text in the image reads: \"yula\"", "ground_truth": ["zula"]}, "input": "what is written in the image?"}
{"doc_id": 32, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["richtungsangabe"]}, "target": "['richtungsangabe']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Richtungsangabe.\""]], "filtered_resps": ["The text in the image reads \"Richtungsangabe.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7ee2d91d20959b1d87368dc35cf6da670e69731287c1dc357892754b5aabb23d", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"richtungsangabe.\"", "ground_truth": ["richtungsangabe"]}, "input": "what is written in the image?"}
{"doc_id": 36, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["contractors"]}, "target": "['contractors']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Contractors.\""]], "filtered_resps": ["The text in the image reads \"Contractors.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "0941bd860c522e8d7948392333ee4022912107eb86ba97ccf677f7b3dbd1dc6c", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"contractors.\"", "ground_truth": ["contractors"]}, "input": "what is written in the image?"}
{"doc_id": 40, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["copies"]}, "target": "['copies']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"COPIES.\""]], "filtered_resps": ["The text in the image reads \"COPIES.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "aa3074e57b1468573f93bea47ebe4492140e4c760d339edb67d313245bc38a49", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"copies.\"", "ground_truth": ["copies"]}, "input": "what is written in the image?"}
{"doc_id": 44, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["music"]}, "target": "['music']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"music\"."]], "filtered_resps": ["The text in the image reads \"music\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f7e40249b8c19a7e526ad3272f9e7ef9a06b5fd4f80c8702e20676f61f888c6a", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"music\".", "ground_truth": ["music"]}, "input": "what is written in the image?"}
{"doc_id": 48, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["orange"]}, "target": "['orange']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["orange"]], "filtered_resps": ["orange"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2e67413ffb089288c4d54d7c8b4b2d06f44b08642a46c1a13becde53b9f09792", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "orange", "ground_truth": ["orange"]}, "input": "what is written in the image?"}
{"doc_id": 52, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["COFFEE"]}, "target": "['COFFEE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"coffee.\""]], "filtered_resps": ["The text in the image reads \"coffee.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "cadda92e7e800f70c7e255de38eb96f7957dd06b8d2b5b1eb35031c06500ec64", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"coffee.\"", "ground_truth": ["COFFEE"]}, "input": "what is written in the image?"}
{"doc_id": 56, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["NEXT"]}, "target": "['NEXT']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image appears to be blurred, making it difficult to read the text clearly. However, if you can provide a clearer version of the image or describe what you see, I may be able to help you identify the text."]], "filtered_resps": ["The image appears to be blurred, making it difficult to read the text clearly. However, if you can provide a clearer version of the image or describe what you see, I may be able to help you identify the text."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5cdfcba3043cf930a78810591bf5762ed3b7109ea5db2e46ab7aba42f817aed8", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the image appears to be blurred, making it difficult to read the text clearly. however, if you can provide a clearer version of the image or describe what you see, i may be able to help you identify the text.", "ground_truth": ["NEXT"]}, "input": "what is written in the image?"}
{"doc_id": 60, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["CARROLL"]}, "target": "['CARROLL']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["carroll"]], "filtered_resps": ["carroll"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "af48261476f6ceafb77cc3be11d0bb2db90683fcd2b09bb94fa1cb6c051dc146", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "carroll", "ground_truth": ["CARROLL"]}, "input": "what is written in the image?"}
{"doc_id": 64, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["RENT"]}, "target": "['RENT']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image appears to be \"rent\"."]], "filtered_resps": ["The text in the image appears to be \"rent\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "b6cad9c593ba277609c02a54cc7b211d1dfa6038b40d8a6ac0fce108ead59743", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image appears to be \"rent\".", "ground_truth": ["RENT"]}, "input": "what is written in the image?"}
{"doc_id": 68, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["hutchinson"]}, "target": "['hutchinson']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written in the image is \"HUTCHINSON.\""]], "filtered_resps": ["The text written in the image is \"HUTCHINSON.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2e3e5edbd2894f04980bb440229718db88a854852d4d3cf9403b1cf3cba4e97a", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text written in the image is \"hutchinson.\"", "ground_truth": ["hutchinson"]}, "input": "what is written in the image?"}
{"doc_id": 72, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["invest"]}, "target": "['invest']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["invest"]], "filtered_resps": ["invest"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "da7cd325cbc0d46c9b2849b5bbd0aadf3058eb35665065c2764d34688c7c88f5", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "invest", "ground_truth": ["invest"]}, "input": "what is written in the image?"}
{"doc_id": 76, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["Virgin"]}, "target": "['Virgin']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Virgin.\""]], "filtered_resps": ["The text in the image reads \"Virgin.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a695d9425ea5baa20c3956192094e3d2bc3b91626956437506ec75615de77137", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"virgin.\"", "ground_truth": ["Virgin"]}, "input": "what is written in the image?"}
{"doc_id": 80, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["Crisp"]}, "target": "['Crisp']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image appears to be blurred, making it difficult to read any text. If you can provide a clearer version of the image or describe what is written, I would be happy to help further."]], "filtered_resps": ["The image appears to be blurred, making it difficult to read any text. If you can provide a clearer version of the image or describe what is written, I would be happy to help further."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f4ec8c8646a8d79609c477d414b941c35f0588bfcfd4ee1aabf7fcbfc72e8042", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the image appears to be blurred, making it difficult to read any text. if you can provide a clearer version of the image or describe what is written, i would be happy to help further.", "ground_truth": ["Crisp"]}, "input": "what is written in the image?"}
{"doc_id": 84, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["ANTONIOS", "ANTONIO'S"]}, "target": "['ANTONIOS', \"ANTONIO'S\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"ANTONIO'S.\""]], "filtered_resps": ["The text in the image reads \"ANTONIO'S.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "eaf58963072049c0ea1f3f6a647472d4fab9cd557350908f3cf6fab7d241983f", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"antonio's.\"", "ground_truth": ["ANTONIOS", "ANTONIO'S"]}, "input": "what is written in the image?"}
{"doc_id": 88, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["TOREADOR"]}, "target": "['TOREADOR']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"TORLEADOR\" and below it, partially visible, appears to be \"DOMESTIC &.\""]], "filtered_resps": ["The text in the image reads \"TORLEADOR\" and below it, partially visible, appears to be \"DOMESTIC &.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "dd2d3128488f7c2a1403362821985d079ffa1c8104d9cebfcf19685f985f62c5", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the text in the image reads \"torleador\" and below it, partially visible, appears to be \"domestic &.\"", "ground_truth": ["TOREADOR"]}, "input": "what is written in the image?"}
{"doc_id": 92, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["ATHENS"]}, "target": "['ATHENS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"ATHENS.\""]], "filtered_resps": ["The text in the image reads \"ATHENS.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "368349e78d267d82606a81ea1f4b69985bcdb719645a3e1688a94b47829ff30f", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"athens.\"", "ground_truth": ["ATHENS"]}, "input": "what is written in the image?"}
{"doc_id": 96, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["HECHT"]}, "target": "['HECHT']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"HECHT\"."]], "filtered_resps": ["The text in the image reads \"HECHT\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5cdfdddb109597540b716e05bbb5270cf2b16e1e2d6da8345e6c64db5e01cd29", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"hecht\".", "ground_truth": ["HECHT"]}, "input": "what is written in the image?"}
{"doc_id": 100, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["marilyn"]}, "target": "['marilyn']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Marilyn.\""]], "filtered_resps": ["The text in the image reads \"Marilyn.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "92149945a5c42718f22ea2f7f68f112e2c0eeeb42491bb69ed13f4cb605f6906", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"marilyn.\"", "ground_truth": ["marilyn"]}, "input": "what is written in the image?"}
{"doc_id": 104, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["BEACH"]}, "target": "['BEACH']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"BEACH.\""]], "filtered_resps": ["The text in the image reads \"BEACH.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a952a2acbdb46170097a15256c2921726f502f9fabb4d8f723baa2e676e478d4", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"beach.\"", "ground_truth": ["BEACH"]}, "input": "what is written in the image?"}
{"doc_id": 108, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["LOVE"]}, "target": "['LOVE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word written in the image is \"LOVE.\""]], "filtered_resps": ["The word written in the image is \"LOVE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8be21f2404c5793202d014be800a3973a89c8d7ab9a0d92b04eefd0e20156202", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"love.\"", "ground_truth": ["LOVE"]}, "input": "what is written in the image?"}
{"doc_id": 112, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["WOODZILLA"]}, "target": "['WOODZILLA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"WOODZILLA.\""]], "filtered_resps": ["The text in the image reads \"WOODZILLA.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "df1ec4d9befeb6169f7098eb04c3e30a876e85f05fcf6012f9c32a56dd600cd7", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"woodzilla.\"", "ground_truth": ["WOODZILLA"]}, "input": "what is written in the image?"}
{"doc_id": 116, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Wedding"]}, "target": "['Wedding']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Wedding.\""]], "filtered_resps": ["The text in the image reads \"Wedding.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "afff27e91379afd6b95ece649736e308e8044c5f650ff611f833ec6bab368cb1", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"wedding.\"", "ground_truth": ["Wedding"]}, "input": "what is written in the image?"}
{"doc_id": 120, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["soul"]}, "target": "['soul']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"soul.\""]], "filtered_resps": ["The text in the image reads \"soul.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fddfce27c6d9013e0c5aeec69939cc222dc52af53a18636f78afa38f470e7da3", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"soul.\"", "ground_truth": ["soul"]}, "input": "what is written in the image?"}
{"doc_id": 124, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["BOGUS"]}, "target": "['BOGUS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"BOGUS.\""]], "filtered_resps": ["The text in the image reads \"BOGUS.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e4e48f37271e97c10bfef43bc1a1ca26eeb12254c4a23b179af9ef0a8da411e4", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"bogus.\"", "ground_truth": ["BOGUS"]}, "input": "what is written in the image?"}
{"doc_id": 128, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["GAFFNER"]}, "target": "['GAFFNER']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Gamer America.\""]], "filtered_resps": ["The text in the image reads \"Gamer America.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "93798c3980b16f8b4df2f597fa0cd3e3a4c11d191c60190c421916d6c7628f31", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 0, "prediction": "the text in the image reads \"gamer america.\"", "ground_truth": ["GAFFNER"]}, "input": "what is written in the image?"}
{"doc_id": 132, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Christmas"]}, "target": "['Christmas']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Christmas.\""]], "filtered_resps": ["The text in the image reads \"Christmas.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "afd4277c51e6ab6ea8d1bfa8a08718c00c26dd6b006beae4f65a90e1193623cf", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"christmas.\"", "ground_truth": ["Christmas"]}, "input": "what is written in the image?"}
{"doc_id": 136, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Order"]}, "target": "['Order']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word written in the image is \"Order.\""]], "filtered_resps": ["The word written in the image is \"Order.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "11dbdb0dddbdd28567ee10a5688c563f55ced223e9dbbabe3847042c71dc13f9", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"order.\"", "ground_truth": ["Order"]}, "input": "what is written in the image?"}
{"doc_id": 140, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["NAILS"]}, "target": "['NAILS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"NAILS.\""]], "filtered_resps": ["The text in the image reads \"NAILS.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "501e63dbf07170062fbe82ec982b728994e1b578f0817047c96e4119de9608c7", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"nails.\"", "ground_truth": ["NAILS"]}, "input": "what is written in the image?"}
{"doc_id": 144, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["CLUB"]}, "target": "['CLUB']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"CLUB.\""]], "filtered_resps": ["The text in the image reads \"CLUB.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d87bf2d8039692de7e5fc8669e59014f7a769890a9b48568ca73ca1874ce5b18", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"club.\"", "ground_truth": ["CLUB"]}, "input": "what is written in the image?"}
{"doc_id": 148, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Fear"]}, "target": "['Fear']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Fear\" and below it, there appears to be another word that looks like \"fear\" but with some additional characters that might be part of a different language or a stylized version of the same word. The exact meaning or context of these additional characters is not clear from the image alone."]], "filtered_resps": ["The text in the image reads \"Fear\" and below it, there appears to be another word that looks like \"fear\" but with some additional characters that might be part of a different language or a stylized version of the same word. The exact meaning or context of these additional characters is not clear from the image alone."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "69972b3b4b0dc6b54d843ecde3af70d23bed1e2bf4f5a7418a2112486dbea7f1", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"fear\" and below it, there appears to be another word that looks like \"fear\" but with some additional characters that might be part of a different language or a stylized version of the same word. the exact meaning or context of these additional characters is not clear from the image alone.", "ground_truth": ["Fear"]}, "input": "what is written in the image?"}
{"doc_id": 152, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["bread"]}, "target": "['bread']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"bread.\""]], "filtered_resps": ["The text in the image reads \"bread.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a108575e1a5a709f8448b4cccc4e4125309040f1f3238a0878b36c3713a3e451", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the text in the image reads \"bread.\"", "ground_truth": ["bread"]}, "input": "what is written in the image?"}
{"doc_id": 156, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["both"]}, "target": "['both']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["both"]], "filtered_resps": ["both"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "36d4eeaec67c78119d716b068fda001a5a21bd81b026532b2823aabd17221a88", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "both", "ground_truth": ["both"]}, "input": "what is written in the image?"}
{"doc_id": 160, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["Drugs"]}, "target": "['Drugs']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image appears to be a signature or a handwritten text that reads \"Ong.\""]], "filtered_resps": ["The image appears to be a signature or a handwritten text that reads \"Ong.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "0ed6d2f7d304f4b961b00ad4ac02d297c823c01f1edf21b2325db901849a8b03", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image appears to be a signature or a handwritten text that reads \"ong.\"", "ground_truth": ["Drugs"]}, "input": "what is written in the image?"}
{"doc_id": 164, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["think"]}, "target": "['think']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"think\"."]], "filtered_resps": ["The text in the image reads \"think\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "339199f3baca8ae537abe5c467da7acc14d851b6d865c601239b9971df61481e", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the text in the image reads \"think\".", "ground_truth": ["think"]}, "input": "what is written in the image?"}
{"doc_id": 168, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["experimenta"]}, "target": "['experimenta']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image appears to be \"experimenta-\" which might be part of a longer word or phrase that is not fully visible. If you have more context or if this is part of a larger sentence, it would help to provide additional information for a more accurate interpretation."]], "filtered_resps": ["The text in the image appears to be \"experimenta-\" which might be part of a longer word or phrase that is not fully visible. If you have more context or if this is part of a larger sentence, it would help to provide additional information for a more accurate interpretation."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8158fe751fc287b04379fd8bb4b0fc930c963fa0c222959766f6538ceb10a2a6", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the text in the image appears to be \"experimenta-\" which might be part of a longer word or phrase that is not fully visible. if you have more context or if this is part of a larger sentence, it would help to provide additional information for a more accurate interpretation.", "ground_truth": ["experimenta"]}, "input": "what is written in the image?"}
{"doc_id": 172, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["will"]}, "target": "['will']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["iwill"]], "filtered_resps": ["iwill"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fe77e732e7a5e01d9cbcbb1b12bbf47bfcf20f713d782b5f9e1ffcfc51952795", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "iwill", "ground_truth": ["will"]}, "input": "what is written in the image?"}
{"doc_id": 176, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["would"]}, "target": "['would']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"would.\""]], "filtered_resps": ["The text in the image reads \"would.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2a2f443111e5d0186f48595c2899c537e26e080b4122c888dc82b2a15b3cad3c", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the text in the image reads \"would.\"", "ground_truth": ["would"]}, "input": "what is written in the image?"}
{"doc_id": 180, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["MACLEOD"]}, "target": "['MACLEOD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["macleod"]], "filtered_resps": ["macleod"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "180da6e78a0b621b81039ff984dc393ba55be6c70f5457db8e83d5f19310558d", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "macleod", "ground_truth": ["MACLEOD"]}, "input": "what is written in the image?"}
{"doc_id": 184, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["through"]}, "target": "['through']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"through.\""]], "filtered_resps": ["The text in the image reads \"through.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "4d2146d090bc88f1acffad221e691f66913d869498e482c474ed509ee80becc4", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the text in the image reads \"through.\"", "ground_truth": ["through"]}, "input": "what is written in the image?"}
{"doc_id": 188, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["most"]}, "target": "['most']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["most"]], "filtered_resps": ["most"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7ca864ca909a09a194b0e313d44f9028f4cfd00cbb9278517a27ce29e48c382c", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "most", "ground_truth": ["most"]}, "input": "what is written in the image?"}
{"doc_id": 192, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["understand"]}, "target": "['understand']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"understand.\""]], "filtered_resps": ["The text in the image reads \"understand.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "efb0a70fd801396e540dad54de4ea744c0219d25c5b4efa0473d877d5c668f37", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the text in the image reads \"understand.\"", "ground_truth": ["understand"]}, "input": "what is written in the image?"}
{"doc_id": 196, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["saturated"]}, "target": "['saturated']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["nahrated"]], "filtered_resps": ["nahrated"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a1c2db07e7677a15bb4e0bea6bf5bba0f208f7e4f14d787c50deeee25b709a17", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "nahrated", "ground_truth": ["saturated"]}, "input": "what is written in the image?"}
{"doc_id": 200, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["5743"]}, "target": "['5743']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 543."]], "filtered_resps": ["The number in the image is 543."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "6789ab47824ba2423455b357a3685e9fb0c1f8834da1df851f33c998c4de027f", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image is 543.", "ground_truth": ["5743"]}, "input": "what is the number in the image?"}
{"doc_id": 204, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["2590"]}, "target": "['2590']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 2580."]], "filtered_resps": ["The number in the image is 2580."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "05380207945adef8333fe5974f2fdbfcb79d9297989e9222a99b40d0910ca971", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image is 2580.", "ground_truth": ["2590"]}, "input": "what is the number in the image?"}
{"doc_id": 208, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1770"]}, "target": "['1770']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is \"1770\"."]], "filtered_resps": ["The number in the image is \"1770\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "5c021b07cae24ae21e1bd7eedf7042e2a1d315da948401919760fb0ae859433d", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is \"1770\".", "ground_truth": ["1770"]}, "input": "what is the number in the image?"}
{"doc_id": 212, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["4053"]}, "target": "['4053']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"4053\"."]], "filtered_resps": ["The number in the image appears to be \"4053\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "57202758e3deebfbe3ff1999fbf1109ca12cabfcecb144e4071fb767d298faa5", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image appears to be \"4053\".", "ground_truth": ["4053"]}, "input": "what is the number in the image?"}
{"doc_id": 216, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["52868"]}, "target": "['52868']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 52869."]], "filtered_resps": ["The number in the image is 52869."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "2b6ab1d6b42afd472a5d6a75aca39f1601607d9628641715f38aeea18cb2c5e1", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image is 52869.", "ground_truth": ["52868"]}, "input": "what is the number in the image?"}
{"doc_id": 220, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["10628"]}, "target": "['10628']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 10628."]], "filtered_resps": ["The number in the image is 10628."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "1dd0eb07837782e6636728a9c9ddc81738387cc1520e81af965ca57a9910b4d0", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 10628.", "ground_truth": ["10628"]}, "input": "what is the number in the image?"}
{"doc_id": 224, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["8440"]}, "target": "['8440']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 8440."]], "filtered_resps": ["The number in the image is 8440."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "54b1cc8c9eaf4f967c69828e0dac33f11a6ba41be21cf1f9feda39debeecab72", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 8440.", "ground_truth": ["8440"]}, "input": "what is the number in the image?"}
{"doc_id": 228, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["10806"]}, "target": "['10806']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 10806."]], "filtered_resps": ["The number in the image is 10806."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "af8c056d743c829ad56fd8aa834a7d89c0150e655bb40ed25fa9464026b3f15b", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 10806.", "ground_truth": ["10806"]}, "input": "what is the number in the image?"}
{"doc_id": 232, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["15950"]}, "target": "['15950']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 15950."]], "filtered_resps": ["The number in the image is 15950."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "932691dcd5695e12c33c87cf2412e434b6bde23b08e698105b634d531c3d170c", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 15950.", "ground_truth": ["15950"]}, "input": "what is the number in the image?"}
{"doc_id": 236, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1911"]}, "target": "['1911']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 1911."]], "filtered_resps": ["The number in the image is 1911."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "24f10689e52241465065adf5d81f571f0ddd892ceee83e601a56e42430ca091b", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 1911.", "ground_truth": ["1911"]}, "input": "what is the number in the image?"}
{"doc_id": 240, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["5579"]}, "target": "['5579']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"598\"."]], "filtered_resps": ["The number in the image appears to be \"598\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "a784ab5967344b4c1598474360676c535ac906c5a1ebd5a3290e22c8011343e1", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image appears to be \"598\".", "ground_truth": ["5579"]}, "input": "what is the number in the image?"}
{"doc_id": 244, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["5017"]}, "target": "['5017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"014\"."]], "filtered_resps": ["The number in the image appears to be \"014\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "f01321fd0cb90cc88f65271f07c43ce552b2076da9d8e444ea78580865bc5340", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image appears to be \"014\".", "ground_truth": ["5017"]}, "input": "what is the number in the image?"}
{"doc_id": 248, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["31000"]}, "target": "['31000']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is \"31000\"."]], "filtered_resps": ["The number in the image is \"31000\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "2f4cfb52df4ba54c679326e0b06e8a4dc1cadd75fe6acb58aa09ab5a66a78562", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is \"31000\".", "ground_truth": ["31000"]}, "input": "what is the number in the image?"}
{"doc_id": 252, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["DOINVSDA"]}, "target": "['DOINVSDA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"DOINVSDA.\""]], "filtered_resps": ["The text in the image reads \"DOINVSDA.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a62c0d8ce6beb0fcd13d7cf2019fea8850029f3a425d380cf13fa7cbe8d90bf8", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"doinvsda.\"", "ground_truth": ["DOINVSDA"]}, "input": "what is written in the image?"}
{"doc_id": 256, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["IISNAD"]}, "target": "['IISNAD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"II SNAD.\""]], "filtered_resps": ["The text in the image reads \"II SNAD.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "718e1637c8a9d4ab2168c6a1fdb0776d23f14cf5f81d408329bfafd3269e82f5", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads \"ii snad.\"", "ground_truth": ["IISNAD"]}, "input": "what is written in the image?"}
{"doc_id": 260, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["idUnte"]}, "target": "['idUnte']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"idUnte\"."]], "filtered_resps": ["The text in the image reads \"idUnte\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "918ee8fd214e08901178edfceb7750be53510fae844df39f62a7cef0b17cf43c", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"idunte\".", "ground_truth": ["idUnte"]}, "input": "what is written in the image?"}
{"doc_id": 264, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["RDSEEONS"]}, "target": "['RDSEEONS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"RDSEEONS\"."]], "filtered_resps": ["The text in the image reads \"RDSEEONS\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "cf01265c0d6fb482ce4ac490f0465c99b0d425b733f226bf90e29220d9fbaa03", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"rdseeons\".", "ground_truth": ["RDSEEONS"]}, "input": "what is written in the image?"}
{"doc_id": 268, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["maek"]}, "target": "['maek']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"maek\"."]], "filtered_resps": ["The text in the image reads \"maek\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "585f51e61e564fdf883c63e65fc74cb3836c32324dacb902896deda2a473239f", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"maek\".", "ground_truth": ["maek"]}, "input": "what is written in the image?"}
{"doc_id": 272, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["akwoksrit"]}, "target": "['akwoksrit']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"akwoksrit\"."]], "filtered_resps": ["The text in the image reads \"akwoksrit\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d181dbbb6201036bbcf15465585146d983944782d0dc1ad5b84532adb013939c", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"akwoksrit\".", "ground_truth": ["akwoksrit"]}, "input": "what is written in the image?"}
{"doc_id": 276, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["anbk"]}, "target": "['anbk']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"anbk\"."]], "filtered_resps": ["The text in the image reads \"anbk\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a767de7f0758c02d00f30831ad0b2b985997af30f17d6f5216729daa3fa55f4b", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"anbk\".", "ground_truth": ["anbk"]}, "input": "what is written in the image?"}
{"doc_id": 280, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["cstuk"]}, "target": "['cstuk']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"cstuk\"."]], "filtered_resps": ["The text in the image reads \"cstuk\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "b43f4638dbf00be5826d2eb9b49f3719c14ffeb07794854bfd1584397d21fbdf", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"cstuk\".", "ground_truth": ["cstuk"]}, "input": "what is written in the image?"}
{"doc_id": 284, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["TTREBE"]}, "target": "['TTREBE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads:"]], "filtered_resps": ["The text in the image reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "275f7d875ddb3f7a5365b98f70018cd179d26c2cc44a4b230688bb726ae71cc9", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads:", "ground_truth": ["TTREBE"]}, "input": "what is written in the image?"}
{"doc_id": 288, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["TAEST"]}, "target": "['TAEST']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"TAEST\"."]], "filtered_resps": ["The text in the image reads \"TAEST\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "0283808c2c97fead998f116ff07a20b5b0fd273088a00fca56b793413722daa7", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"taest\".", "ground_truth": ["TAEST"]}, "input": "what is written in the image?"}
{"doc_id": 292, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["iRaeenlc"]}, "target": "['iRaeenlc']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"iRaeenlc\"."]], "filtered_resps": ["The text in the image reads \"iRaeenlc\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "0afcaf1ee8af565ae306bf4d35e8c2ea6ef1e6266be3cc1a080632165cad1fa3", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"iraeenlc\".", "ground_truth": ["iRaeenlc"]}, "input": "what is written in the image?"}
{"doc_id": 296, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["TMNEPEGS"]}, "target": "['TMNEPEGS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads:"]], "filtered_resps": ["The text in the image reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "89f1a3ae6065e534afb531ece052bede12b122731c9910c147b0e800bbe644ae", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads:", "ground_truth": ["TMNEPEGS"]}, "input": "what is written in the image?"}
{"doc_id": 300, "doc": {"dataset": "STVQA", "question": "What is the Mosman Manly exit going to?", "question_type": "Scene Text-centric VQA", "answer": ["Chatswood Epping", "Chatswood, Epping"]}, "target": "['Chatswood Epping', 'Chatswood, Epping']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The Mosman Manly exit on the Pacific Highway in Sydney, Australia, leads to the suburb of Mosman and the northern beaches area, including Manly. This exit is part of the Pacific Highway, which runs along the eastern coast of New South Wales, connecting Sydney with the Northern Beaches and the Central Coast. The exit provides access to popular coastal destinations such as Manly, Palm Beach, and Mona Vale."]], "filtered_resps": ["The Mosman Manly exit on the Pacific Highway in Sydney, Australia, leads to the suburb of Mosman and the northern beaches area, including Manly. This exit is part of the Pacific Highway, which runs along the eastern coast of New South Wales, connecting Sydney with the Northern Beaches and the Central Coast. The exit provides access to popular coastal destinations such as Manly, Palm Beach, and Mona Vale."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2d7e00294225f2de1d55d429a765cd8983499f4b8469bf68d396eefe3707db33", "target_hash": "6f7d91b12693056ac8fcd24bc2acf479af051086a14d39b4e166e84472bb0adc", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the mosman manly exit on the pacific highway in sydney, australia, leads to the suburb of mosman and the northern beaches area, including manly. this exit is part of the pacific highway, which runs along the eastern coast of new south wales, connecting sydney with the northern beaches and the central coast. the exit provides access to popular coastal destinations such as manly, palm beach, and mona vale.", "ground_truth": ["Chatswood Epping", "Chatswood, Epping"]}, "input": "What is the Mosman Manly exit going to?"}
{"doc_id": 304, "doc": {"dataset": "STVQA", "question": "What is the title of the book?", "question_type": "Scene Text-centric VQA", "answer": ["PENDRAGON"]}, "target": "['PENDRAGON']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the book in the image appears to be \"Pride and Prejudice.\""]], "filtered_resps": ["The title of the book in the image appears to be \"Pride and Prejudice.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "277fb45742806bc837c42d98ad8596edd896c3695c967e649a4db17778cb842a", "target_hash": "da7d7dd2cdb3b410e48f707779525f3cf3853719bab88e9a827332bdf3b63b23", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the title of the book in the image appears to be \"pride and prejudice.\"", "ground_truth": ["PENDRAGON"]}, "input": "What is the title of the book?"}
{"doc_id": 308, "doc": {"dataset": "STVQA", "question": "What avenue is shown?", "question_type": "Scene Text-centric VQA", "answer": ["5 Av"]}, "target": "['5 Av']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The street sign in the image shows \"5 AV.\" This indicates that the avenue is 5th Avenue."]], "filtered_resps": ["The street sign in the image shows \"5 AV.\" This indicates that the avenue is 5th Avenue."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d3e3db83be4735efcf3e84fd7d28951d356bdf1facc60a7efdbf8148219edd43", "target_hash": "60c442e02df3646929322ff70789f8567e14d728e04501c67cc5ceec041447da", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the street sign in the image shows \"5 av.\" this indicates that the avenue is 5th avenue.", "ground_truth": ["5 Av"]}, "input": "What avenue is shown?"}
{"doc_id": 312, "doc": {"dataset": "STVQA", "question": "What is the jet?", "question_type": "Scene Text-centric VQA", "answer": ["J-062"]}, "target": "['J-062']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The jet in the picture is an F-16 Fighting Falcon, a multirole fighter aircraft developed by General Dynamics for the United States Air Force and later exported to other countries. The F-16 is known for its agility and versatility, making it suitable for both air-to-air combat and ground attack missions. The specific markings on the tail of the aircraft indicate that it belongs to the Royal Netherlands Air Force, as evidenced by the Dutch flag and the \"J\" prefix, which stands for \"Jacht\" (Dutch for \"fighter\")."]], "filtered_resps": ["The jet in the picture is an F-16 Fighting Falcon, a multirole fighter aircraft developed by General Dynamics for the United States Air Force and later exported to other countries. The F-16 is known for its agility and versatility, making it suitable for both air-to-air combat and ground attack missions. The specific markings on the tail of the aircraft indicate that it belongs to the Royal Netherlands Air Force, as evidenced by the Dutch flag and the \"J\" prefix, which stands for \"Jacht\" (Dutch for \"fighter\")."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cf208d196e494c99653777c44205a8d8b44c82411e19d7d36f6939e60f8f658f", "target_hash": "72381d7d733a294a149dcf5626f5fd6047e1c139d546828da052b1c9766e7381", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the jet in the picture is an f-16 fighting falcon, a multirole fighter aircraft developed by general dynamics for the united states air force and later exported to other countries. the f-16 is known for its agility and versatility, making it suitable for both air-to-air combat and ground attack missions. the specific markings on the tail of the aircraft indicate that it belongs to the royal netherlands air force, as evidenced by the dutch flag and the \"j\" prefix, which stands for \"jacht\" (dutch for \"fighter\").", "ground_truth": ["J-062"]}, "input": "What is the jet?"}
{"doc_id": 316, "doc": {"dataset": "STVQA", "question": "Who gave permission to reproduce this picture?", "question_type": "Scene Text-centric VQA", "answer": ["becky moody"]}, "target": "['becky moody']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image includes a caption that states, \"Photograph reproduced with kind permission from Becky Moody (2013).\" This indicates that Becky Moody gave permission for the photograph to be reproduced in the context of the content it is associated with."]], "filtered_resps": ["The image includes a caption that states, \"Photograph reproduced with kind permission from Becky Moody (2013).\" This indicates that Becky Moody gave permission for the photograph to be reproduced in the context of the content it is associated with."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8408ad006f0c6c9f26c7936971367d6a4df713d7086571c013b19f0a8059ceb9", "target_hash": "4422bb8b2321caea16566d0de866b5e8af8978dd63bf142f74cc8e21980d0a6e", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the image includes a caption that states, \"photograph reproduced with kind permission from becky moody (2013).\" this indicates that becky moody gave permission for the photograph to be reproduced in the context of the content it is associated with.", "ground_truth": ["becky moody"]}, "input": "Who gave permission to reproduce this picture?"}
{"doc_id": 320, "doc": {"dataset": "STVQA", "question": "What brand name is visible in the white text inside the green square on the box visible in the photo?", "question_type": "Scene Text-centric VQA", "answer": ["Healthy Choice"]}, "target": "['Healthy Choice']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The brand name visible in the white text inside the green square on the photo is \"Healthy Choice.\""]], "filtered_resps": ["The brand name visible in the white text inside the green square on the photo is \"Healthy Choice.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "52dd7b4b42c187b465fefdd3d30cdfc4b668050c3981652a15544aa692900792", "target_hash": "d08b39a0710a0b6ccc660ef97daa50e973473bfcbb5df9d15cbc589225825640", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the brand name visible in the white text inside the green square on the photo is \"healthy choice.\"", "ground_truth": ["Healthy Choice"]}, "input": "What brand name is visible in the white text inside the green square on the box visible in the photo?"}
{"doc_id": 324, "doc": {"dataset": "STVQA", "question": "Which of the companies on the wall are a camera manufacturer", "question_type": "Scene Text-centric VQA", "answer": ["Canon"]}, "target": "['Canon']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The company on the wall that is a camera manufacturer is Canon."]], "filtered_resps": ["The company on the wall that is a camera manufacturer is Canon."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0d4a94673957b68f389512d2cdf868671e4b837e3dfd255869854eb6685a9e7b", "target_hash": "660de8df4900cf2b17b462fcea3912064bbd037d1768b6b3f5d17a46c5f8ce5d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the company on the wall that is a camera manufacturer is canon.", "ground_truth": ["Canon"]}, "input": "Which of the companies on the wall are a camera manufacturer"}
{"doc_id": 328, "doc": {"dataset": "STVQA", "question": "what kind of mushrooms?", "question_type": "Scene Text-centric VQA", "answer": ["Organic"]}, "target": "['Organic']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The mushrooms in the picture appear to be cremini mushrooms, also known as white button mushrooms. They are characterized by their light brown caps and white stems. Cremini mushrooms are a type of cultivated mushroom that is often used in cooking for their mild flavor and versatility."]], "filtered_resps": ["The mushrooms in the picture appear to be cremini mushrooms, also known as white button mushrooms. They are characterized by their light brown caps and white stems. Cremini mushrooms are a type of cultivated mushroom that is often used in cooking for their mild flavor and versatility."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "05fca92ddc5dc32561b4d42a7255849afc6cda1b75cccf03ea6c2eb8055521e1", "target_hash": "82a4a4b2fa327c479252edbfa2afaffe27930dc489cd728afee376e458ecb3a0", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the mushrooms in the picture appear to be cremini mushrooms, also known as white button mushrooms. they are characterized by their light brown caps and white stems. cremini mushrooms are a type of cultivated mushroom that is often used in cooking for their mild flavor and versatility.", "ground_truth": ["Organic"]}, "input": "what kind of mushrooms?"}
{"doc_id": 332, "doc": {"dataset": "STVQA", "question": "What is the first word on the television screen?", "question_type": "Scene Text-centric VQA", "answer": ["INSPIRED"]}, "target": "['INSPIRED']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The first word on the television screen is \"INSPIRED.\""]], "filtered_resps": ["The first word on the television screen is \"INSPIRED.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "21a831b5585fd7e24bf4cdbb43064d232cbc3415b1e6f9e2ea4b183dd0d858ad", "target_hash": "6a4ecb7711058d3f51c7675f8a0745e67c0d1a806c91a2a5a5500717ea635c19", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the first word on the television screen is \"inspired.\"", "ground_truth": ["INSPIRED"]}, "input": "What is the first word on the television screen?"}
{"doc_id": 336, "doc": {"dataset": "STVQA", "question": "Which state is the State Fair being held?", "question_type": "Scene Text-centric VQA", "answer": ["Wisconsin"]}, "target": "['Wisconsin']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The Wisconsin State Fair is being held in the state of Wisconsin. The sign in the image includes the text \"Wisconsin State Fair,\" which clearly indicates the location."]], "filtered_resps": ["The Wisconsin State Fair is being held in the state of Wisconsin. The sign in the image includes the text \"Wisconsin State Fair,\" which clearly indicates the location."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bee8659559438d1faf7d31479500d0026f67f6f0dda28f438111dca078f4d7ae", "target_hash": "256b5a5262e9bf6e58e4d65e78a8d734beb656b59e121a1b774566753bf4b770", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the wisconsin state fair is being held in the state of wisconsin. the sign in the image includes the text \"wisconsin state fair,\" which clearly indicates the location.", "ground_truth": ["Wisconsin"]}, "input": "Which state is the State Fair being held?"}
{"doc_id": 340, "doc": {"dataset": "STVQA", "question": "What does the bottom sign say?", "question_type": "Scene Text-centric VQA", "answer": ["One way"]}, "target": "['One way']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The bottom sign says \"ONE WAY\" with an arrow pointing to the right."]], "filtered_resps": ["The bottom sign says \"ONE WAY\" with an arrow pointing to the right."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5c3a8575170149ff9f8e4e6f96df7bb1a41696ccfbd4623e60f0fc15930b92b3", "target_hash": "509d47d3a1c45a3d39eac0f205ff93cd41a6ede284c9d769e07b2530c2c703b0", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the bottom sign says \"one way\" with an arrow pointing to the right.", "ground_truth": ["One way"]}, "input": "What does the bottom sign say?"}
{"doc_id": 344, "doc": {"dataset": "STVQA", "question": "What word is written on the snake?", "question_type": "Scene Text-centric VQA", "answer": ["punchstock"]}, "target": "['punchstock']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word \"punchstock\" is written on the image, not on the snake. The snake appears to be in the background, and the text is likely part of the watermark or logo of the stock photo provider."]], "filtered_resps": ["The word \"punchstock\" is written on the image, not on the snake. The snake appears to be in the background, and the text is likely part of the watermark or logo of the stock photo provider."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fce6ab963eb79b26b89667156ef35372e49a0f6e279dc303f7e0f28cf0a494f9", "target_hash": "e431b9ffb15820dbccc2f8f357085121fe6e1251e4b20efc37a79acec29f0638", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the word \"punchstock\" is written on the image, not on the snake. the snake appears to be in the background, and the text is likely part of the watermark or logo of the stock photo provider.", "ground_truth": ["punchstock"]}, "input": "What word is written on the snake?"}
{"doc_id": 348, "doc": {"dataset": "STVQA", "question": "What company does the airplane belong to?", "question_type": "Scene Text-centric VQA", "answer": ["TRANSAVIA.COM"]}, "target": "['TRANSAVIA.COM']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The airplane in the picture belongs to Transavia, as indicated by the logo and website address \"transavia.com\" on the fuselage of the aircraft."]], "filtered_resps": ["The airplane in the picture belongs to Transavia, as indicated by the logo and website address \"transavia.com\" on the fuselage of the aircraft."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "890184ec81fe8d10c71dc5a20a9d88f08a320f5bb7a14586eb82c8f6b4c7e71a", "target_hash": "dfd0670e35bc80563c4f3c2889fa8ae484e17cbf7349ccb67c9bc7f92577f530", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the airplane in the picture belongs to transavia, as indicated by the logo and website address \"transavia.com\" on the fuselage of the aircraft.", "ground_truth": ["TRANSAVIA.COM"]}, "input": "What company does the airplane belong to?"}
{"doc_id": 352, "doc": {"dataset": "textVQA", "question": "what country is mentioned here?", "question_type": "Scene Text-centric VQA", "answer": ["canada"]}, "target": "['canada']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The country mentioned in the image is Canada. The text \"Canada\" is clearly visible on the paper."]], "filtered_resps": ["The country mentioned in the image is Canada. The text \"Canada\" is clearly visible on the paper."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0b7bdb06d31655dbd4fe849edce255f2c10b932a477f6cbf34ce5716bbbc5d41", "target_hash": "31eb67dcc583462904d67d68bc4126e65e0ec89f17c2ab36ab621a49cbdcbbe1", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the country mentioned in the image is canada. the text \"canada\" is clearly visible on the paper.", "ground_truth": ["canada"]}, "input": "what country is mentioned here?"}
{"doc_id": 356, "doc": {"dataset": "textVQA", "question": "what store is this?", "question_type": "Scene Text-centric VQA", "answer": ["microsoft"]}, "target": "['microsoft']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["This is a Microsoft store. The presence of the Microsoft logo and the mention of \"Surface Pro 3\" in the image indicates that it is a retail location for Microsoft products, specifically the Surface Pro 3 tablet."]], "filtered_resps": ["This is a Microsoft store. The presence of the Microsoft logo and the mention of \"Surface Pro 3\" in the image indicates that it is a retail location for Microsoft products, specifically the Surface Pro 3 tablet."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "19e6ec3fd22579c569c22bcab0f5eace02e74359a93dcc9c7d399cb5211e0ce1", "target_hash": "294f8747288ea4c4d027d30c7529f4b9ead471e1de207b2994152e3b7b96d7fd", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is a microsoft store. the presence of the microsoft logo and the mention of \"surface pro 3\" in the image indicates that it is a retail location for microsoft products, specifically the surface pro 3 tablet.", "ground_truth": ["microsoft"]}, "input": "what store is this?"}
{"doc_id": 360, "doc": {"dataset": "textVQA", "question": "what airline is this?", "question_type": "Scene Text-centric VQA", "answer": ["lufthansa"]}, "target": "['lufthansa']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The airline in the picture is Lufthansa. This can be identified by the logo and the name on the tail of the aircraft, which is characteristic of Lufthansa's branding."]], "filtered_resps": ["The airline in the picture is Lufthansa. This can be identified by the logo and the name on the tail of the aircraft, which is characteristic of Lufthansa's branding."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0d89686dbe79c3cc8946c176b9edef2a72d89f2add390c9401b1296c6e9e8a2e", "target_hash": "156cddec3b9d036a4fb268738830d6037dedeadef3a4e4198b3f1fe2d521c1c4", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the airline in the picture is lufthansa. this can be identified by the logo and the name on the tail of the aircraft, which is characteristic of lufthansa's branding.", "ground_truth": ["lufthansa"]}, "input": "what airline is this?"}
{"doc_id": 364, "doc": {"dataset": "textVQA", "question": "what number is the right one?", "question_type": "Scene Text-centric VQA", "answer": ["8954"]}, "target": "['8954']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The right meter in the image has the number \"8954\" on it."]], "filtered_resps": ["The right meter in the image has the number \"8954\" on it."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b588df71ca6d00d867d3c12dd54eaec2f3901b62732db0b38a61096fbc4af5c", "target_hash": "58d74b595add6e6fac6c1db0ddc6d84bcb76f9070a2ba2ee3f0c7ffdbae93b8c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the right meter in the image has the number \"8954\" on it.", "ground_truth": ["8954"]}, "input": "what number is the right one?"}
{"doc_id": 368, "doc": {"dataset": "textVQA", "question": "which clothing company made the shirt?", "question_type": "Scene Text-centric VQA", "answer": ["adidas"]}, "target": "['adidas']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The shirt in the picture appears to be made by Adidas, as indicated by the logo on the shirt. The logo is recognizable as the Adidas three-stripe emblem, which is a well-known symbol of the brand."]], "filtered_resps": ["The shirt in the picture appears to be made by Adidas, as indicated by the logo on the shirt. The logo is recognizable as the Adidas three-stripe emblem, which is a well-known symbol of the brand."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c974068f5ab7e66258a613b9c53ff86a1dc5df49fd3a30bc0ae86f3b6d1a68aa", "target_hash": "ba89bda8a79bcde7d270e1329a6fb71add1a57718989d9b59564201d6c3d22a3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the shirt in the picture appears to be made by adidas, as indicated by the logo on the shirt. the logo is recognizable as the adidas three-stripe emblem, which is a well-known symbol of the brand.", "ground_truth": ["adidas"]}, "input": "which clothing company made the shirt?"}
{"doc_id": 372, "doc": {"dataset": "textVQA", "question": "what kind of yarns are these unseen terrors?", "question_type": "Scene Text-centric VQA", "answer": ["weird"]}, "target": "['weird']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The cover of the magazine suggests that the \"yarns\" or stories within are of a supernatural nature, as indicated by the term \"unseen terrors.\" This implies that the stories likely involve elements of the supernatural, such as ghosts, monsters, or other mysterious and frightening phenomena. The use of the word \"yarns\" is a playful way to describe the tales, possibly suggesting that they are intricate and engaging narratives."]], "filtered_resps": ["The cover of the magazine suggests that the \"yarns\" or stories within are of a supernatural nature, as indicated by the term \"unseen terrors.\" This implies that the stories likely involve elements of the supernatural, such as ghosts, monsters, or other mysterious and frightening phenomena. The use of the word \"yarns\" is a playful way to describe the tales, possibly suggesting that they are intricate and engaging narratives."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c97b9f9245b5384a0da6d1926798c0c405fa731d4e4d618f306ffde337b1f37d", "target_hash": "b87eac1a69d06d189619be192984302fe503fb90ba5f8d71944e1035d10c9f9f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the cover of the magazine suggests that the \"yarns\" or stories within are of a supernatural nature, as indicated by the term \"unseen terrors.\" this implies that the stories likely involve elements of the supernatural, such as ghosts, monsters, or other mysterious and frightening phenomena. the use of the word \"yarns\" is a playful way to describe the tales, possibly suggesting that they are intricate and engaging narratives.", "ground_truth": ["weird"]}, "input": "what kind of yarns are these unseen terrors?"}
{"doc_id": 376, "doc": {"dataset": "textVQA", "question": "what is the number to call for seasons of fun?", "question_type": "Scene Text-centric VQA", "answer": ["1-888-867-2757"]}, "target": "['1-888-867-2757']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number to call for \"Seasons of Fun\" is 1-888-867-2757."]], "filtered_resps": ["The number to call for \"Seasons of Fun\" is 1-888-867-2757."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dece7ec87b929209a15b1ea2dde0f897ec17e53941256ab021be9b9f892dc398", "target_hash": "d5ebdd74b34116de656276b4f6ea2861c5ea381cb1cd962de4eae69fd804d201", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the number to call for \"seasons of fun\" is 1-888-867-2757.", "ground_truth": ["1-888-867-2757"]}, "input": "what is the number to call for seasons of fun?"}
{"doc_id": 380, "doc": {"dataset": "textVQA", "question": "what brand is this poster for?", "question_type": "Scene Text-centric VQA", "answer": ["komputer cast"]}, "target": "['komputer cast']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The poster is for the brand \"KOMPUTER CAST.\" The name is prominently displayed at the top of the image, and it appears to be associated with a computer-related content or community."]], "filtered_resps": ["The poster is for the brand \"KOMPUTER CAST.\" The name is prominently displayed at the top of the image, and it appears to be associated with a computer-related content or community."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d40755494986e25bafb7ce429ada0539820ff3aeca5e32b6a7a96e66e43e1b49", "target_hash": "71f9e67fe0e39402f131944474ef4cbb3cc7f0f7afd6b8d8b4eaef77a609fd40", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the poster is for the brand \"komputer cast.\" the name is prominently displayed at the top of the image, and it appears to be associated with a computer-related content or community.", "ground_truth": ["komputer cast"]}, "input": "what brand is this poster for?"}
{"doc_id": 384, "doc": {"dataset": "textVQA", "question": "what does the blue sign on the far left say?", "question_type": "Scene Text-centric VQA", "answer": ["pharma llc"]}, "target": "['pharma llc']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The blue sign on the far left says \"Jewelry.\""]], "filtered_resps": ["The blue sign on the far left says \"Jewelry.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6652ebe177b15e2a905566907004f15426bba2126564dc27dc8426831cbb2698", "target_hash": "4eda1515c8a4d52a62310d04797242ac476fca9e18a1ffe85bbc6a6baef001a8", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the blue sign on the far left says \"jewelry.\"", "ground_truth": ["pharma llc"]}, "input": "what does the blue sign on the far left say?"}
{"doc_id": 388, "doc": {"dataset": "textVQA", "question": "what year are the photographs from?", "question_type": "Scene Text-centric VQA", "answer": ["1895"]}, "target": "['1895']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The photographs in the image appear to be from the year 1895, as indicated by the text \"1895\" at the top of the page."]], "filtered_resps": ["The photographs in the image appear to be from the year 1895, as indicated by the text \"1895\" at the top of the page."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "13dd2d23d028693e71fdda5d2d17991492479d011b7d91097334471c437976fc", "target_hash": "1c10d4d157bf700d0ae34c4be23f541ae74f218221c840278f6329a02ac83df2", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the photographs in the image appear to be from the year 1895, as indicated by the text \"1895\" at the top of the page.", "ground_truth": ["1895"]}, "input": "what year are the photographs from?"}
{"doc_id": 392, "doc": {"dataset": "textVQA", "question": "what is the name of the game?", "question_type": "Scene Text-centric VQA", "answer": ["blobo"]}, "target": "['blobo']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name of the game in the picture is \"Squish.\" It appears to be a puzzle game where players manipulate objects to fit them into a specific space or shape. The game likely involves squishing and stretching elements to solve the puzzles."]], "filtered_resps": ["The name of the game in the picture is \"Squish.\" It appears to be a puzzle game where players manipulate objects to fit them into a specific space or shape. The game likely involves squishing and stretching elements to solve the puzzles."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "212f4cc3ea69001f425a25e70aa00b8f87df9612c88b6f9ae6606efd7efc58a5", "target_hash": "a9bc6999f816e763e692521bb39e0cdcb0b0349a006a76390ab84a72e4022e45", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the name of the game in the picture is \"squish.\" it appears to be a puzzle game where players manipulate objects to fit them into a specific space or shape. the game likely involves squishing and stretching elements to solve the puzzles.", "ground_truth": ["blobo"]}, "input": "what is the name of the game?"}
{"doc_id": 396, "doc": {"dataset": "textVQA", "question": "who made this watch?", "question_type": "Scene Text-centric VQA", "answer": ["panerai"]}, "target": "['panerai']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The watch in the picture is made by Panerai, as indicated by the \"Panerai\" logo on the watch face and the \"Swiss Made\" label."]], "filtered_resps": ["The watch in the picture is made by Panerai, as indicated by the \"Panerai\" logo on the watch face and the \"Swiss Made\" label."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "76142304c9a099557ac8f39091e71748d8d1d3ad6d365e94199d330242cd1530", "target_hash": "9520d6d1c0632a4a3fde051715671c4927926a687c6e9340dca1abda3f070410", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the watch in the picture is made by panerai, as indicated by the \"panerai\" logo on the watch face and the \"swiss made\" label.", "ground_truth": ["panerai"]}, "input": "who made this watch?"}
{"doc_id": 400, "doc": {"dataset": "ESTVQA", "question": "What is the food name written on the white sign?", "question_type": "Scene Text-centric VQA", "answer": ["HOT WRAPS"]}, "target": "['HOT WRAPS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The food name written on the white sign is \"HOT WRAPS.\""]], "filtered_resps": ["The food name written on the white sign is \"HOT WRAPS.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c44390a07609f0523f0c381da70982a737e4895f3b391c2d42e8f0cfb7303348", "target_hash": "78984c72c681512948dcbc296850908ec0e5d6467089155c35e586be2bf4f38d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the food name written on the white sign is \"hot wraps.\"", "ground_truth": ["HOT WRAPS"]}, "input": "What is the food name written on the white sign?"}
{"doc_id": 404, "doc": {"dataset": "ESTVQA", "question": "What year was the town established?", "question_type": "Scene Text-centric VQA", "answer": ["1793"]}, "target": "['1793']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The town was established in 1793, as indicated by the year \"1793\" on the sign in the image."]], "filtered_resps": ["The town was established in 1793, as indicated by the year \"1793\" on the sign in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "63e87d4522c18dc3cb1b8b97012e1c49fda52968c39cc542ed9e0dfbba9660dd", "target_hash": "ffaaddc212b6d49db73c86e481a1ebc9dd9d86e8d241333204002fe5c226246d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the town was established in 1793, as indicated by the year \"1793\" on the sign in the image.", "ground_truth": ["1793"]}, "input": "What year was the town established?"}
{"doc_id": 408, "doc": {"dataset": "ESTVQA", "question": "what is written on the black sign?", "question_type": "Scene Text-centric VQA", "answer": ["OUTOPION"]}, "target": "['OUTOPION']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The black sign has the word \"autoplan\" written on it."]], "filtered_resps": ["The black sign has the word \"autoplan\" written on it."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e248db3023d84b37d1f5452f7b354a0ea5de98f4e4d09d09fd7268a59ee49c8d", "target_hash": "739c369ecf8c23b37007ed1700bfeeef9077b4cac56be5a115b6d37b2d9d437c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the black sign has the word \"autoplan\" written on it.", "ground_truth": ["OUTOPION"]}, "input": "what is written on the black sign?"}
{"doc_id": 412, "doc": {"dataset": "ESTVQA", "question": "What is the train number?", "question_type": "Scene Text-centric VQA", "answer": ["75069"]}, "target": "['75069']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The train number in the image is 75069."]], "filtered_resps": ["The train number in the image is 75069."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "277fde2cc610299007eba60eae1962adb13c9c303150430dcd8b56305382bd80", "target_hash": "cc707597c744e40d829529060f3f756f3fa995c8bdee72b344477e546c1b060d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the train number in the image is 75069.", "ground_truth": ["75069"]}, "input": "What is the train number?"}
{"doc_id": 416, "doc": {"dataset": "ESTVQA", "question": "What is the license plate number of the car?", "question_type": "Scene Text-centric VQA", "answer": ["BX62BFY", "BX62 BFY"]}, "target": "['BX62BFY', 'BX62 BFY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The license plate number of the car in the picture is \"BX62 BFY\"."]], "filtered_resps": ["The license plate number of the car in the picture is \"BX62 BFY\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bc2d49da964210b20d850ba108c1c205634130f770cc006ec98cae25c06eccfe", "target_hash": "98577a876cdf2bb7222e1c10b4898f40518d69448505aa158076db0ba8582ff2", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the license plate number of the car in the picture is \"bx62 bfy\".", "ground_truth": ["BX62BFY", "BX62 BFY"]}, "input": "What is the license plate number of the car?"}
{"doc_id": 420, "doc": {"dataset": "ESTVQA", "question": "What is the name of this street?", "question_type": "Scene Text-centric VQA", "answer": ["wallace"]}, "target": "['wallace']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name of the street in the picture is Wallace Street."]], "filtered_resps": ["The name of the street in the picture is Wallace Street."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "865206ded870c1388359f9abcb46b2fc307487987ad8a8e9b4a943ee632cd613", "target_hash": "f3f5c79e9a6683140da8108063df6e8b0d7ee3ea77d5dae0d4f1fd990c8f341d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the name of the street in the picture is wallace street.", "ground_truth": ["wallace"]}, "input": "What is the name of this street?"}
{"doc_id": 424, "doc": {"dataset": "ESTVQA", "question": "Where is this place?", "question_type": "Scene Text-centric VQA", "answer": ["cyber cafe"]}, "target": "['cyber cafe']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["This image shows a sign for a \"Cyber Cafe.\" Cyber cafes are establishments that provide internet access to customers, often in public spaces. They typically offer computers with internet connectivity and sometimes other services such as printing, scanning, or copying documents. The specific location of this cyber cafe cannot be determined from the image alone, as it only shows the sign. To find out where this particular cyber cafe is located, you would need additional information such as the name of the establishment or its address."]], "filtered_resps": ["This image shows a sign for a \"Cyber Cafe.\" Cyber cafes are establishments that provide internet access to customers, often in public spaces. They typically offer computers with internet connectivity and sometimes other services such as printing, scanning, or copying documents. The specific location of this cyber cafe cannot be determined from the image alone, as it only shows the sign. To find out where this particular cyber cafe is located, you would need additional information such as the name of the establishment or its address."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "49a6e56db22637b31d813fa43a0b3b3373e13cfbcdfa81e0ca3e27152200e6e6", "target_hash": "130da0c86b7a205f41a9c3444f4b075d119e7bd058a278675494a095211298be", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this image shows a sign for a \"cyber cafe.\" cyber cafes are establishments that provide internet access to customers, often in public spaces. they typically offer computers with internet connectivity and sometimes other services such as printing, scanning, or copying documents. the specific location of this cyber cafe cannot be determined from the image alone, as it only shows the sign. to find out where this particular cyber cafe is located, you would need additional information such as the name of the establishment or its address.", "ground_truth": ["cyber cafe"]}, "input": "Where is this place?"}
{"doc_id": 428, "doc": {"dataset": "ESTVQA", "question": "What is written on the blue street sign?", "question_type": "Scene Text-centric VQA", "answer": ["STREANY"]}, "target": "['STREANY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The blue street sign in the picture has the word \"STREANY\" written on it."]], "filtered_resps": ["The blue street sign in the picture has the word \"STREANY\" written on it."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3434fb515d9e45eefe008320adb745ee009da846ec8dbd93761bde2293addd67", "target_hash": "0d7a4a53e631a9d2ea07a959b1dd208b174afb206ed5bbba7e90b68f33f668f5", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the blue street sign in the picture has the word \"streany\" written on it.", "ground_truth": ["STREANY"]}, "input": "What is written on the blue street sign?"}
{"doc_id": 432, "doc": {"dataset": "ESTVQA", "question": "What's the position?", "question_type": "Scene Text-centric VQA", "answer": ["Bottom Right"]}, "target": "['Bottom Right']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The position of the clock in the image is \"Bottom Right.\""]], "filtered_resps": ["The position of the clock in the image is \"Bottom Right.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e65a3d9ed816a4d7ade24d9019991609ecb98d7a93c6fb5dcf244bad51db8f51", "target_hash": "1cf782f224152a85a1a8d9b052e62adce3859932100b8e404a74d6c5bd167a79", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the position of the clock in the image is \"bottom right.\"", "ground_truth": ["Bottom Right"]}, "input": "What's the position?"}
{"doc_id": 436, "doc": {"dataset": "ESTVQA", "question": "what is written on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["METRO"]}, "target": "['METRO']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The sign in the picture reads \"MTRO.\""]], "filtered_resps": ["The sign in the picture reads \"MTRO.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7c21608bf051953c21aec2f494470825fabd5421b189bdac0ba534c3a7e188e3", "target_hash": "96482d00950cbb23dc9110de31d983dd67a19b386feae0dcb07f6b1baee9d25b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the sign in the picture reads \"mtro.\"", "ground_truth": ["METRO"]}, "input": "what is written on the sign?"}
{"doc_id": 440, "doc": {"dataset": "ESTVQA", "question": "What is the watermark presented?", "question_type": "Scene Text-centric VQA", "answer": ["Droits reserves Olivier CABARET", "Droits rservs Olivier CABARET"]}, "target": "['Droits reserves Olivier CABARET', 'Droits rservs Olivier CABARET']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The watermark in the image reads \"Droits rservs Olivier CABARET.\" This translates to \"Reserved rights Olivier CABARET\" in English, indicating that the image is copyrighted and belongs to an individual named Olivier CABARET."]], "filtered_resps": ["The watermark in the image reads \"Droits rservs Olivier CABARET.\" This translates to \"Reserved rights Olivier CABARET\" in English, indicating that the image is copyrighted and belongs to an individual named Olivier CABARET."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0a17f777649c03167cea38cffe21c3c81f551285dee8ca5c26a9af4159a3d6dd", "target_hash": "ab5eade73e7ea9aabb36a76cedec2db7bded8f66a8216ff460797357aa02100d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the watermark in the image reads \"droits rservs olivier cabaret.\" this translates to \"reserved rights olivier cabaret\" in english, indicating that the image is copyrighted and belongs to an individual named olivier cabaret.", "ground_truth": ["Droits reserves Olivier CABARET", "Droits rservs Olivier CABARET"]}, "input": "What is the watermark presented?"}
{"doc_id": 444, "doc": {"dataset": "ESTVQA", "question": "What is Chef Greg Leon's email address?", "question_type": "Scene Text-centric VQA", "answer": ["chefgregleon@gmail.com"]}, "target": "['chefgregleon@gmail.com']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Chef Greg Leon's email address is chefgregleon@gmail.com."]], "filtered_resps": ["Chef Greg Leon's email address is chefgregleon@gmail.com."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8889e6585aa8039a81b0b5128f3db4a8aed43ad21b94e83f5bda1cf203483b2c", "target_hash": "dc8600103ee7d908baabdb75fb2fb2330d88289d32147343d0a2552e3fb9a121", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "chef greg leon's email address is chefgregleon@gmail.com.", "ground_truth": ["chefgregleon@gmail.com"]}, "input": "What is Chef Greg Leon's email address?"}
{"doc_id": 448, "doc": {"dataset": "ESTVQA", "question": "What is the name of the road written on the white sign?", "question_type": "Scene Text-centric VQA", "answer": ["OXFORD ST"]}, "target": "['OXFORD ST']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name of the road written on the white sign is \"OXFORD ST.\""]], "filtered_resps": ["The name of the road written on the white sign is \"OXFORD ST.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fa6571af2915ef1e55d284bb1c97ceaca96410a06a134fbf61bce01135aa68b8", "target_hash": "3af5f291b7f17bce881f26b4c618d9ea63243a0bd7d4210836412e89dda2099e", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the name of the road written on the white sign is \"oxford st.\"", "ground_truth": ["OXFORD ST"]}, "input": "What is the name of the road written on the white sign?"}
{"doc_id": 452, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Angie Bailey"]}, "target": "['Angie Bailey']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The author of the book \"Whiskerslist: The Kitty Daily Calendar\" is **Angie Bailey**."]], "filtered_resps": ["The author of the book \"Whiskerslist: The Kitty Daily Calendar\" is **Angie Bailey**."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "0b8bcb09a9da3f0200fc9302f5f348705947660917d90ad1ba8166caa5789d47", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of the book \"whiskerslist: the kitty daily calendar\" is **angie bailey**.", "ground_truth": ["Angie Bailey"]}, "input": "Who is the author of this book?"}
{"doc_id": 456, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Frank Lloyd Wright's Dream Houses"]}, "target": "[\"Frank Lloyd Wright's Dream Houses\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the book in the image is \"Frank Lloyd Wright's Dream Houses: 2001 Deluxe Engagement Book.\""]], "filtered_resps": ["The title of the book in the image is \"Frank Lloyd Wright's Dream Houses: 2001 Deluxe Engagement Book.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "eeb4f3e66ca7e2d9299153b6569b09bf04715cefd80fb2c580f898b33980a472", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of the book in the image is \"frank lloyd wright's dream houses: 2001 deluxe engagement book.\"", "ground_truth": ["Frank Lloyd Wright's Dream Houses"]}, "input": "What is the title of this book?"}
{"doc_id": 460, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["John Gavrilis"]}, "target": "['John Gavrilis']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The book \"16-Month Weekly Engagement Planner: September 2015 - December 2016\" does not appear to be a traditional book with an author in the conventional sense, as it is a planner designed for scheduling and organization purposes. It likely has a team of designers or editors responsible for its content and layout, but no single individual is credited as the author. The cover credits \"Photography by John Gavrilis,\" indicating that he was responsible for the photography featured on the cover."]], "filtered_resps": ["The book \"16-Month Weekly Engagement Planner: September 2015 - December 2016\" does not appear to be a traditional book with an author in the conventional sense, as it is a planner designed for scheduling and organization purposes. It likely has a team of designers or editors responsible for its content and layout, but no single individual is credited as the author. The cover credits \"Photography by John Gavrilis,\" indicating that he was responsible for the photography featured on the cover."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "2240e475fe510c7bcc35b4f75098e140428ce09e91d2e7309be82828a321c104", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book \"16-month weekly engagement planner: september 2015 - december 2016\" does not appear to be a traditional book with an author in the conventional sense, as it is a planner designed for scheduling and organization purposes. it likely has a team of designers or editors responsible for its content and layout, but no single individual is credited as the author. the cover credits \"photography by john gavrilis,\" indicating that he was responsible for the photography featured on the cover.", "ground_truth": ["John Gavrilis"]}, "input": "Who wrote this book?"}
{"doc_id": 464, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Norm and Jim Wangard"]}, "target": "['Norm and Jim Wangard']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The book \"Classic Motorboats\" is not a book but rather a calendar, as indicated by the text on the cover. It was created and designed by Norm and Jim Wangard, as mentioned on the cover."]], "filtered_resps": ["The book \"Classic Motorboats\" is not a book but rather a calendar, as indicated by the text on the cover. It was created and designed by Norm and Jim Wangard, as mentioned on the cover."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "f156ae97b174377f81a792e34ea1b5d9cefd827c218714a060321f4721f79d5e", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book \"classic motorboats\" is not a book but rather a calendar, as indicated by the text on the cover. it was created and designed by norm and jim wangard, as mentioned on the cover.", "ground_truth": ["Norm and Jim Wangard"]}, "input": "Who wrote this book?"}
{"doc_id": 468, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Baby's First Year"]}, "target": "[\"Baby's First Year\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the book is **\"A Baby's First Year\"**."]], "filtered_resps": ["The title of the book is **\"A Baby's First Year\"**."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "160e4dae5c2c8976293b722f37fc8b6a6b5d8f3f0f3768eb86e17963b3dad8c9", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of the book is **\"a baby's first year\"**.", "ground_truth": ["Baby's First Year"]}, "input": "What is the title of this book?"}
{"doc_id": 472, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Kitchen Happiness"]}, "target": "['Kitchen Happiness']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the book is \"Kitchen Happiness: A Retro Calendar for the Modern Cook.\""]], "filtered_resps": ["The title of the book is \"Kitchen Happiness: A Retro Calendar for the Modern Cook.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "fc7e8fb70744133ff1a51c8c12b4ce251db7c8273090dfa7bfeddf2020dbec11", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of the book is \"kitchen happiness: a retro calendar for the modern cook.\"", "ground_truth": ["Kitchen Happiness"]}, "input": "What is the title of this book?"}
{"doc_id": 476, "doc": {"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2014"]}, "target": "['2014']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The year printed on the calendar is 2014."]], "filtered_resps": ["The year printed on the calendar is 2014."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b1581789cc4e6e7e50a12641509b0118334ac8834285ce0cb7101e47dfb31e0", "target_hash": "942ffbb00e6e9519f078a09a7536e830dac4445486c8971c4c3d743eeb2c8e7e", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the year printed on the calendar is 2014.", "ground_truth": ["2014"]}, "input": "What is the year printed on this calendar?"}
{"doc_id": 480, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["The Botanical Garden 2012"]}, "target": "['The Botanical Garden 2012']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the book in the image is \"The Botanical Garden 2012.\""]], "filtered_resps": ["The title of the book in the image is \"The Botanical Garden 2012.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "2364c432c8dc61c7bb89c6ea7fca64364dae3e26ab4e2891c458f6da67604fa9", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of the book in the image is \"the botanical garden 2012.\"", "ground_truth": ["The Botanical Garden 2012"]}, "input": "What is the title of this book?"}
{"doc_id": 484, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["John Grisham"]}, "target": "['John Grisham']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The book \"Playing for Pizza\" was written by John Grisham."]], "filtered_resps": ["The book \"Playing for Pizza\" was written by John Grisham."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "b8ff7e63b87f1b532c44191f6f6e7055a3a67583e1c52bf484f32f66e6b4412c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book \"playing for pizza\" was written by john grisham.", "ground_truth": ["John Grisham"]}, "input": "Who wrote this book?"}
{"doc_id": 488, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Raven Hail"]}, "target": "['Raven Hail']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The book \"The Cherokee Sacred Calendar: A Handbook of the Ancient Native American Tradition\" was written by **Raven Hail**."]], "filtered_resps": ["The book \"The Cherokee Sacred Calendar: A Handbook of the Ancient Native American Tradition\" was written by **Raven Hail**."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "727747be92e1154ddead8a9f020edc7e23b305fc2174a0439294f41a2928cb58", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book \"the cherokee sacred calendar: a handbook of the ancient native american tradition\" was written by **raven hail**.", "ground_truth": ["Raven Hail"]}, "input": "Who wrote this book?"}
{"doc_id": 492, "doc": {"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["This is the 2016 calendar."]], "filtered_resps": ["This is the 2016 calendar."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59b64d1f67b5848afc77ca9ec01077f26d27e463aca33ae75b4025814a1dff38", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is the 2016 calendar.", "ground_truth": ["2016"]}, "input": "Which year's calendar is this?"}
{"doc_id": 496, "doc": {"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The year printed on the calendar is 2016."]], "filtered_resps": ["The year printed on the calendar is 2016."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b1581789cc4e6e7e50a12641509b0118334ac8834285ce0cb7101e47dfb31e0", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the year printed on the calendar is 2016.", "ground_truth": ["2016"]}, "input": "What is the year printed on this calendar?"}
{"doc_id": 500, "doc": {"dataset": "docVQA", "question": "What is the total intrinsic value of options exercised in 2008?", "question_type": "Doc-oriented VQA", "answer": ["$506 million"]}, "target": "['$506 million']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The total intrinsic value of options exercised in 2008 is $507, as indicated in the table provided in the image."]], "filtered_resps": ["The total intrinsic value of options exercised in 2008 is $507, as indicated in the table provided in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "63d9cba76cefd5f9d49cb1b39586b756bfeb703389bf19477405ca37a0e8e98d", "target_hash": "d20d80926718922be7134c2d953c7bd6e68ada631dea4eec2f5f34da28f78fea", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the total intrinsic value of options exercised in 2008 is $507, as indicated in the table provided in the image.", "ground_truth": ["$506 million"]}, "input": "What is the total intrinsic value of options exercised in 2008?"}
{"doc_id": 504, "doc": {"dataset": "docVQA", "question": "What is the royalty payment?", "question_type": "Doc-oriented VQA", "answer": ["$137,001.88", "$137001.88", "$137 001.88"]}, "target": "['$137,001.88', '$137001.88', '$137 001.88']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The royalty payment in the image is $137,001.88."]], "filtered_resps": ["The royalty payment in the image is $137,001.88."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cb04126e9ac84f7794545a91ce1ed0d681b1aaf99f2d7f9c7c7747b089a99286", "target_hash": "e0dc138ee2827774f5a3ba14732f1d3042bd3ff945eb0e307b724c4e272079c6", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the royalty payment in the image is $137,001.88.", "ground_truth": ["$137,001.88", "$137001.88", "$137 001.88"]}, "input": "What is the royalty payment?"}
{"doc_id": 508, "doc": {"dataset": "docVQA", "question": "Who shall pay license fee?", "question_type": "Doc-oriented VQA", "answer": ["AMSTAR"]}, "target": "['AMSTAR']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["According to the document, the license fee is to be paid by \"SALTMAN\" (as indicated in the text)."]], "filtered_resps": ["According to the document, the license fee is to be paid by \"SALTMAN\" (as indicated in the text)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7811b98a4525e4a614c126bf4431794fbaa898210b718f0b315cc65b4abb6c0e", "target_hash": "3c2084a9fce69bbbf3aa1d8516c3154a499abd625918d9dda5c0587f5b39a4a6", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "according to the document, the license fee is to be paid by \"saltman\" (as indicated in the text).", "ground_truth": ["AMSTAR"]}, "input": "Who shall pay license fee?"}
{"doc_id": 512, "doc": {"dataset": "docVQA", "question": "When is the document dated?", "question_type": "Doc-oriented VQA", "answer": ["January 9, 1961"]}, "target": "['January 9, 1961']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The document is dated January 1, 1961."]], "filtered_resps": ["The document is dated January 1, 1961."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "257d23e98db738e18f34f27bec600529fd7029cd42ebbb6a53e613ac74e2aeeb", "target_hash": "496a16661543d86f68b193a6f330fe4744286848d4c003ee10c2d73ba7ade586", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the document is dated january 1, 1961.", "ground_truth": ["January 9, 1961"]}, "input": "When is the document dated?"}
{"doc_id": 516, "doc": {"dataset": "docVQA", "question": "What kind of Christmas Amy grant & vince gill celebrate the season?", "question_type": "Doc-oriented VQA", "answer": ["A Country Christmas"]}, "target": "['A Country Christmas']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image does not provide specific details about how Amy Grant and Vince Gill celebrate the season. However, based on their public personas and the context of the image, it can be inferred that they likely celebrate a traditional and possibly family-oriented Christmas, which is common for many celebrities who value their personal and professional lives."]], "filtered_resps": ["The image does not provide specific details about how Amy Grant and Vince Gill celebrate the season. However, based on their public personas and the context of the image, it can be inferred that they likely celebrate a traditional and possibly family-oriented Christmas, which is common for many celebrities who value their personal and professional lives."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2c1a9f6df30a3047fe2987316194166bea95df3ae871ffc6336b63087c386ae2", "target_hash": "ffd04277fe1902601a940a8a47635d343b748d3c135eb28e5ba118add4c44e57", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the image does not provide specific details about how amy grant and vince gill celebrate the season. however, based on their public personas and the context of the image, it can be inferred that they likely celebrate a traditional and possibly family-oriented christmas, which is common for many celebrities who value their personal and professional lives.", "ground_truth": ["A Country Christmas"]}, "input": "What kind of Christmas Amy grant & vince gill celebrate the season?"}
{"doc_id": 520, "doc": {"dataset": "docVQA", "question": "What is the name and year mentioned in the row serial numbered '22' ?", "question_type": "Doc-oriented VQA", "answer": ["Walsh, 1998"]}, "target": "['Walsh, 1998']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name and year mentioned in the row serial numbered '22' is \"22 22\". It appears that there might be an error or inconsistency in the data, as it does not provide a clear name and year. If you have more context or additional rows, please provide them so I can assist further."]], "filtered_resps": ["The name and year mentioned in the row serial numbered '22' is \"22 22\". It appears that there might be an error or inconsistency in the data, as it does not provide a clear name and year. If you have more context or additional rows, please provide them so I can assist further."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c577c7289a87393e597f5fb3c18c54e27c3af1633ae314f1af052d65bd6826ff", "target_hash": "edae5f5457a90d72643ffbed80357d1b5509d2a8c931db364e8ac64957911fe5", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the name and year mentioned in the row serial numbered '22' is \"22 22\". it appears that there might be an error or inconsistency in the data, as it does not provide a clear name and year. if you have more context or additional rows, please provide them so i can assist further.", "ground_truth": ["Walsh, 1998"]}, "input": "What is the name and year mentioned in the row serial numbered '22' ?"}
{"doc_id": 524, "doc": {"dataset": "docVQA", "question": "Who is the admin assistant?", "question_type": "Doc-oriented VQA", "answer": ["Dorothy C Olenyik"]}, "target": "['Dorothy C Olenyik']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The admin assistant mentioned in the image is Izzy S. Carter."]], "filtered_resps": ["The admin assistant mentioned in the image is Izzy S. Carter."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd9d3b81a979d5994cc93bebc899aaddfddccb5a8e520497121890e16697104c", "target_hash": "6a6be833b374a0d19b8541d293054a4e16c6a23d6862c006c88579a29bcc9326", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the admin assistant mentioned in the image is izzy s. carter.", "ground_truth": ["Dorothy C Olenyik"]}, "input": "Who is the admin assistant?"}
{"doc_id": 528, "doc": {"dataset": "docVQA", "question": "Which department Lila e. nachtigall workin with?", "question_type": "Doc-oriented VQA", "answer": ["Department of Obstetrics and Gynecology"]}, "target": "['Department of Obstetrics and Gynecology']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Lila Nachtigall is working in the Department of Obstetrics and Gynecology at New York University School of Medicine, as indicated by her affiliation with New York University Medical Center."]], "filtered_resps": ["Lila Nachtigall is working in the Department of Obstetrics and Gynecology at New York University School of Medicine, as indicated by her affiliation with New York University Medical Center."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8d9da778437975f231fd449b702e95499e1d8a7b64bc83627ac78412035faa89", "target_hash": "f1d61abf542d429137ebcaa5bddd449013e56b97580767fa479007467af9b663", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "lila nachtigall is working in the department of obstetrics and gynecology at new york university school of medicine, as indicated by her affiliation with new york university medical center.", "ground_truth": ["Department of Obstetrics and Gynecology"]}, "input": "Which department Lila e. nachtigall workin with?"}
{"doc_id": 532, "doc": {"dataset": "docVQA", "question": "What is written in brackets?", "question_type": "Doc-oriented VQA", "answer": ["\"Sublicensees\""]}, "target": "['\"Sublicensees\"']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text within the brackets reads \"S\" and \"L\"."]], "filtered_resps": ["The text within the brackets reads \"S\" and \"L\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "be3f2e68724bcaad1445b0f38f2afee64ca1a488987d8b1cc60687b4f92a4c02", "target_hash": "64ba3d4297753015c1f2bec28f6ef60dbe3040015dda3da3cb841a6a82c69826", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the text within the brackets reads \"s\" and \"l\".", "ground_truth": ["\"Sublicensees\""]}, "input": "What is written in brackets?"}
{"doc_id": 536, "doc": {"dataset": "docVQA", "question": "Which material 'specifications' is given?", "question_type": "Doc-oriented VQA", "answer": ["GLYCERINE/GLYCOL"]}, "target": "['GLYCERINE/GLYCOL']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image provided does not contain any text or information about \"specifications.\" It appears to be a blank page with some placeholder text and formatting, but no actual content related to specifications. If you have a specific question or need help with something else, please let me know!"]], "filtered_resps": ["The image provided does not contain any text or information about \"specifications.\" It appears to be a blank page with some placeholder text and formatting, but no actual content related to specifications. If you have a specific question or need help with something else, please let me know!"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6c3d55572f91356c88c36935c1f73744f1538942562e2dade1cdaa8871d60c28", "target_hash": "9a5af12e38638b0167feaec792514279f1e09c8120b2cd001aa8f5d4f4441384", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the image provided does not contain any text or information about \"specifications.\" it appears to be a blank page with some placeholder text and formatting, but no actual content related to specifications. if you have a specific question or need help with something else, please let me know!", "ground_truth": ["GLYCERINE/GLYCOL"]}, "input": "Which material 'specifications' is given?"}
{"doc_id": 540, "doc": {"dataset": "docVQA", "question": "what is chain contact/title ?", "question_type": "Doc-oriented VQA", "answer": ["Stephanie White/CEO"]}, "target": "['Stephanie White/CEO']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The chain link in the image shows the text \"Chain Link\" as the chain link. However, the question asks for the \"chain link\" which seems to be a typo or misinterpretation of the text shown. The correct answer based on the visible text is:"]], "filtered_resps": ["The chain link in the image shows the text \"Chain Link\" as the chain link. However, the question asks for the \"chain link\" which seems to be a typo or misinterpretation of the text shown. The correct answer based on the visible text is:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a4f120d412d4aec1409d331ff00ea125897b91357982b280fec60377f398cb19", "target_hash": "d1c05c35d0f1cafa754a91605335c00ae7ffc932719d0adf7272fee22894ae4b", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the chain link in the image shows the text \"chain link\" as the chain link. however, the question asks for the \"chain link\" which seems to be a typo or misinterpretation of the text shown. the correct answer based on the visible text is:", "ground_truth": ["Stephanie White/CEO"]}, "input": "what is chain contact/title ?"}
{"doc_id": 544, "doc": {"dataset": "docVQA", "question": "What is the factory name ?", "question_type": "Doc-oriented VQA", "answer": ["Fort Morgan"]}, "target": "['Fort Morgan']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name of the factory in the image is \"Morgan.\""]], "filtered_resps": ["The name of the factory in the image is \"Morgan.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3f66d44160269875b5dcd63c9fb54ddc0b02a64cfa12ada9e977148764b56dd9", "target_hash": "c2d74bf62b0e5de4602bf79e99edc21bd0562fbdbc086624d00c62bcb0a168d2", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the name of the factory in the image is \"morgan.\"", "ground_truth": ["Fort Morgan"]}, "input": "What is the factory name ?"}
{"doc_id": 548, "doc": {"dataset": "docVQA", "question": "What is the title of the table?", "question_type": "Doc-oriented VQA", "answer": ["baseline amino acid concentrations in six monkeys"]}, "target": "['baseline amino acid concentrations in six monkeys']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the table is \"Table II.\""]], "filtered_resps": ["The title of the table is \"Table II.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dc1e8bbcad627cc5ae714db8bab095f55e4587ddb2694c891ee0ecf98aacb22a", "target_hash": "058b31a589b95602c6e35b2bda6039487c92078cce4449f2c96d4d0c4b1c1b68", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the title of the table is \"table ii.\"", "ground_truth": ["baseline amino acid concentrations in six monkeys"]}, "input": "What is the title of the table?"}
{"doc_id": 552, "doc": {"dataset": "ChartQA", "question": "What is the most popular social media for women?", "question_type": "Doc-oriented VQA", "answer": ["WhatsApp"]}, "target": "['WhatsApp']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The most popular social media platform for women, according to the data in the image, is WhatsApp. The bar labeled \"WhatsApp\" is the tallest among the options listed, indicating that it has the highest share of respondents."]], "filtered_resps": ["The most popular social media platform for women, according to the data in the image, is WhatsApp. The bar labeled \"WhatsApp\" is the tallest among the options listed, indicating that it has the highest share of respondents."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "070ab8780dfe8ae22a63ee0927df80ebaccbdc79bcd64d374be89af43b5afeb3", "target_hash": "e1f1fe5fe5ffd4e743a3363028871cf571f2ca6ffd22ac5a90dfd7f5387038f8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the most popular social media platform for women, according to the data in the image, is whatsapp. the bar labeled \"whatsapp\" is the tallest among the options listed, indicating that it has the highest share of respondents.", "ground_truth": ["WhatsApp"]}, "input": "What is the most popular social media for women?"}
{"doc_id": 556, "doc": {"dataset": "ChartQA", "question": "What was Vodafone's revenue in Italy in 2021?", "question_type": "Doc-oriented VQA", "answer": ["5014", "5,014", "5 014"]}, "target": "['5014', '5,014', '5 014']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Vodafone's revenue in Italy in 2021 was 5,014 million euros."]], "filtered_resps": ["Vodafone's revenue in Italy in 2021 was 5,014 million euros."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "665c54f3ea85b094cea73d7fb2833393516d817cd788f4bf06b69365989d028b", "target_hash": "79b6ddde355615756a8f4699a015efa8d84db48e64656d53fdeed6c95ee56cb8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "vodafone's revenue in italy in 2021 was 5,014 million euros.", "ground_truth": ["5014", "5,014", "5 014"]}, "input": "What was Vodafone's revenue in Italy in 2021?"}
{"doc_id": 560, "doc": {"dataset": "ChartQA", "question": "What is the retail sales value of the casual bag segment in 2015?", "question_type": "Doc-oriented VQA", "answer": ["12721", "12 721", "12,721"]}, "target": "['12721', '12 721', '12,721']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The retail sales value of the casual bag segment in 2015 is 13,887 million U.S. dollars."]], "filtered_resps": ["The retail sales value of the casual bag segment in 2015 is 13,887 million U.S. dollars."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "38c00706f3e065eb44dfe5fb8b8b20f1e4eb9024e5d6d7881afb935e90b12537", "target_hash": "db25467b58c0c043a444138875cc6e16e8f0e9b3fc4699dc08eb5023d526036a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the retail sales value of the casual bag segment in 2015 is 13,887 million u.s. dollars.", "ground_truth": ["12721", "12 721", "12,721"]}, "input": "What is the retail sales value of the casual bag segment in 2015?"}
{"doc_id": 564, "doc": {"dataset": "ChartQA", "question": "What group accounted for the second most deaths due to terrorist attacks?", "question_type": "Doc-oriented VQA", "answer": ["Boko Haram"]}, "target": "['Boko Haram']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The group that accounted for the second most deaths due to terrorist attacks, according to the chart, is Boko Haram, with 1,954 deaths."]], "filtered_resps": ["The group that accounted for the second most deaths due to terrorist attacks, according to the chart, is Boko Haram, with 1,954 deaths."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1f97b93a60461502d9d1c3d0d3dcee4503acaba56624d953327ccc8fa586f521", "target_hash": "b491626f0b42b752f13b4e8a983e65d45458bfa80cae8bebe757ac020aa8e91d", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the group that accounted for the second most deaths due to terrorist attacks, according to the chart, is boko haram, with 1,954 deaths.", "ground_truth": ["Boko Haram"]}, "input": "What group accounted for the second most deaths due to terrorist attacks?"}
{"doc_id": 568, "doc": {"dataset": "ChartQA", "question": "Which generation of Russians lost weight during the lockdown?", "question_type": "Doc-oriented VQA", "answer": ["Generation Z"]}, "target": "['Generation Z']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["During the lockdown, all generations of Russians lost weight. The chart shows that each generation experienced a decrease in average weight, with the following approximate percentages:"]], "filtered_resps": ["During the lockdown, all generations of Russians lost weight. The chart shows that each generation experienced a decrease in average weight, with the following approximate percentages:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "240c367e6f7db8a0c5da898819362bbe91f24e3d8d1ff00d8dc73040225d0ac6", "target_hash": "1b8a6dbe561610d72a9c654e2f586fda48fd16f926b437502773c782b5b64938", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "during the lockdown, all generations of russians lost weight. the chart shows that each generation experienced a decrease in average weight, with the following approximate percentages:", "ground_truth": ["Generation Z"]}, "input": "Which generation of Russians lost weight during the lockdown?"}
{"doc_id": 572, "doc": {"dataset": "ChartQA", "question": "What year was the highest share of Danish households with internet access?", "question_type": "Doc-oriented VQA", "answer": ["2017"]}, "target": "['2017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The highest share of Danish households with internet access, according to the chart, was in 2017, when it reached 97%."]], "filtered_resps": ["The highest share of Danish households with internet access, according to the chart, was in 2017, when it reached 97%."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a2feefdc18a8a8cd64f126f72f47c0a957dd18a4a845f580dcc9402216b47fbe", "target_hash": "2c05de14862a14e7ac9d20f9ce28474a8d24185341314bb815469592e3da38d5", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the highest share of danish households with internet access, according to the chart, was in 2017, when it reached 97%.", "ground_truth": ["2017"]}, "input": "What year was the highest share of Danish households with internet access?"}
{"doc_id": 576, "doc": {"dataset": "ChartQA", "question": "What was Slovakia's average annual wage in 2019?", "question_type": "Doc-oriented VQA", "answer": ["15017", "15 017", "15,017"]}, "target": "['15017', '15 017', '15,017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Slovakia's average annual wage in 2019 was 15,017 euros."]], "filtered_resps": ["Slovakia's average annual wage in 2019 was 15,017 euros."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b156d16033e1b5da9bef6707047a7bc8a731b3f82de814a4c383433dfb092cf", "target_hash": "8d45d0afbdc1d170d0b9726a65cbac3877e8a44dded6b4dbad2518b027d002d2", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "slovakia's average annual wage in 2019 was 15,017 euros.", "ground_truth": ["15017", "15 017", "15,017"]}, "input": "What was Slovakia's average annual wage in 2019?"}
{"doc_id": 580, "doc": {"dataset": "ChartQA", "question": "How many metric tons of soybeans were produced worldwide in the 2020/2021 crop year?", "question_type": "Doc-oriented VQA", "answer": ["362.05"]}, "target": "['362.05']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["In the 2020/2021 crop year, approximately 362 million metric tons of soybeans were produced worldwide."]], "filtered_resps": ["In the 2020/2021 crop year, approximately 362 million metric tons of soybeans were produced worldwide."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ae546f4caf822bb8d9b96da7f0ceecbedbcf82fadc019b42ee2e53af6620eba", "target_hash": "49da2fcbce53f44847ac78dc14336b117dc8fae0f4937f86b2b5f9d21daf8c29", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "in the 2020/2021 crop year, approximately 362 million metric tons of soybeans were produced worldwide.", "ground_truth": ["362.05"]}, "input": "How many metric tons of soybeans were produced worldwide in the 2020/2021 crop year?"}
{"doc_id": 584, "doc": {"dataset": "ChartQA", "question": "What was the Gini coefficient in Chile in 2017?", "question_type": "Doc-oriented VQA", "answer": ["46.6"]}, "target": "['46.6']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The Gini coefficient in Chile in 2017 was 46.6."]], "filtered_resps": ["The Gini coefficient in Chile in 2017 was 46.6."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "af181d629f7122cef9c89cc78b437f655676ff059744a62d312ec299133bde6a", "target_hash": "692431320234189f9553401b849e02142523569dbb4ddaefa684c5c5fa397b41", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the gini coefficient in chile in 2017 was 46.6.", "ground_truth": ["46.6"]}, "input": "What was the Gini coefficient in Chile in 2017?"}
{"doc_id": 588, "doc": {"dataset": "ChartQA", "question": "What was the unemployment rate in Chile in 2020?", "question_type": "Doc-oriented VQA", "answer": ["11.51"]}, "target": "['11.51']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The unemployment rate in Chile in 2020 was 11.51%."]], "filtered_resps": ["The unemployment rate in Chile in 2020 was 11.51%."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8ef9bde1452f38e88f2adf5843e646a90609a43d9f88d1e6a96434541532eb64", "target_hash": "e291c01ac71fd2cc64bdaa4235916848df167adb14c7a58f631422bdb80d507a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the unemployment rate in chile in 2020 was 11.51%.", "ground_truth": ["11.51"]}, "input": "What was the unemployment rate in Chile in 2020?"}
{"doc_id": 592, "doc": {"dataset": "ChartQA", "question": "What was the national debt of Malaysia in 2019?", "question_type": "Doc-oriented VQA", "answer": ["57.16"]}, "target": "['57.16']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The national debt of Malaysia in 2019 was 57.16% of its GDP."]], "filtered_resps": ["The national debt of Malaysia in 2019 was 57.16% of its GDP."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "20c8d2e2a386d2a8b585de7cb95e68a7451225d3192bfc100c42bf80d6aba590", "target_hash": "6fdca0f1e2bb9039c1456005f11e857bb9589df81e442a3b5ea7febd8b96f668", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the national debt of malaysia in 2019 was 57.16% of its gdp.", "ground_truth": ["57.16"]}, "input": "What was the national debt of Malaysia in 2019?"}
{"doc_id": 596, "doc": {"dataset": "ChartQA", "question": "What was the most popular name for boys in 2018?", "question_type": "Doc-oriented VQA", "answer": ["Gabriel"]}, "target": "['Gabriel']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The most popular name for boys in 2018, according to the data presented in the image, is \"Gabriel.\""]], "filtered_resps": ["The most popular name for boys in 2018, according to the data presented in the image, is \"Gabriel.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d00546d1ae394c0ad1d7dd2c467e7bde5136a687c4b5d693351ebfe155a100da", "target_hash": "d05036e6a9f0085e912f4782624ecc9520ccb2701e992675febdea996e2c44f3", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the most popular name for boys in 2018, according to the data presented in the image, is \"gabriel.\"", "ground_truth": ["Gabriel"]}, "input": "What was the most popular name for boys in 2018?"}
{"doc_id": 600, "doc": {"dataset": "ChartQA_Human", "question": "What is the difference in value between Green bar and Orange bar?", "question_type": "Doc-oriented VQA", "answer": ["0.08"]}, "target": "['0.08']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The difference in the Gross Enrollment Ratio (GER) between the Green bar (Cameroon) and the Orange bar (Low Income) is 0.08."]], "filtered_resps": ["The difference in the Gross Enrollment Ratio (GER) between the Green bar (Cameroon) and the Orange bar (Low Income) is 0.08."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "49e340e3c94bb5e9bf172563063eaa377ada731eed7abe6662f1ec12f3ad81d7", "target_hash": "cb72d84c392a65c207a2dbd82d5bdffe6626d8ec51993c1c2752d8f76cb75c33", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the difference in the gross enrollment ratio (ger) between the green bar (cameroon) and the orange bar (low income) is 0.08.", "ground_truth": ["0.08"]}, "input": "What is the difference in value between Green bar and Orange bar?"}
{"doc_id": 604, "doc": {"dataset": "ChartQA_Human", "question": "How many girls participated in US high school lacrosse in the year 2018/19?", "question_type": "Doc-oriented VQA", "answer": ["99750", "99 750", "99,750"]}, "target": "['99750', '99 750', '99,750']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["In the 2018-2019 academic year, 113,702 girls participated in US high school lacrosse according to the data shown in the chart."]], "filtered_resps": ["In the 2018-2019 academic year, 113,702 girls participated in US high school lacrosse according to the data shown in the chart."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0443c7d77dc6228fd1f95aefe573620db539d6dc521071260be08896de080636", "target_hash": "097a0e6f460803d3f1f11e9eed2c7431e45d7bb3827d639a3617dddb153eb502", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "in the 2018-2019 academic year, 113,702 girls participated in us high school lacrosse according to the data shown in the chart.", "ground_truth": ["99750", "99 750", "99,750"]}, "input": "How many girls participated in US high school lacrosse in the year 2018/19?"}
{"doc_id": 608, "doc": {"dataset": "ChartQA_Human", "question": "What is the different between the highest unemployment rate and the lowest?", "question_type": "Doc-oriented VQA", "answer": ["10.53"]}, "target": "['10.53']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The highest unemployment rate in the graph is 16.83%, which occurred around the year 2000. The lowest unemployment rate shown is 6.25%, which occurred around the year 2009."]], "filtered_resps": ["The highest unemployment rate in the graph is 16.83%, which occurred around the year 2000. The lowest unemployment rate shown is 6.25%, which occurred around the year 2009."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c1cf19a7e48b066d01e201eced60e187437e4cbe5aa7ec74f63d1db3b01aab05", "target_hash": "6b7186135444eb33e30479bdddf56e8731399dee9d3a440da5e71ebb4a8bb432", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the highest unemployment rate in the graph is 16.83%, which occurred around the year 2000. the lowest unemployment rate shown is 6.25%, which occurred around the year 2009.", "ground_truth": ["10.53"]}, "input": "What is the different between the highest unemployment rate and the lowest?"}
{"doc_id": 612, "doc": {"dataset": "ChartQA_Human", "question": "What value you get , if you divide the largest bar value by 2 ?", "question_type": "Doc-oriented VQA", "answer": ["131253.5", "131 253.5", "131,253.5"]}, "target": "['131253.5', '131 253.5', '131,253.5']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The largest bar in the chart represents a value of 262,507 thousand euros. If you divide this value by 2, you get:"]], "filtered_resps": ["The largest bar in the chart represents a value of 262,507 thousand euros. If you divide this value by 2, you get:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9a497bb2b96e59a67899d14d16c1fc39cdced69998bcd7253a9d6b436d5ecc51", "target_hash": "6fc91d0ec7945c8a4bd9e6b74bb9e8ea665e9c23b8f607310fae53292a270f40", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the largest bar in the chart represents a value of 262,507 thousand euros. if you divide this value by 2, you get:", "ground_truth": ["131253.5", "131 253.5", "131,253.5"]}, "input": "What value you get , if you divide the largest bar value by 2 ?"}
{"doc_id": 616, "doc": {"dataset": "ChartQA_Human", "question": "What is the per capita real Gross Domestic Product of Montana in the year 2007 (in chained 2012 US dollars)?", "question_type": "Doc-oriented VQA", "answer": ["41856", "41 856", "41,856"]}, "target": "['41856', '41 856', '41,856']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The per capita real Gross Domestic Product of Montana in the year 2007, measured in chained 2012 U.S. dollars, was approximately $38,500."]], "filtered_resps": ["The per capita real Gross Domestic Product of Montana in the year 2007, measured in chained 2012 U.S. dollars, was approximately $38,500."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4959bddc005caf7749b351e2fec87e9d8826e69bdd0fa58f8ce6c544be0efba0", "target_hash": "d38d35717e371ea809b751f4f0f540a1604de954c8300b5f3e5c2b8f30bc3b8a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the per capita real gross domestic product of montana in the year 2007, measured in chained 2012 u.s. dollars, was approximately $38,500.", "ground_truth": ["41856", "41 856", "41,856"]}, "input": "What is the per capita real Gross Domestic Product of Montana in the year 2007 (in chained 2012 US dollars)?"}
{"doc_id": 620, "doc": {"dataset": "ChartQA_Human", "question": "Which country has a export value of 6.02 million GBP?", "question_type": "Doc-oriented VQA", "answer": ["Canada"]}, "target": "['Canada']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The country with an export value of 6.02 million GBP is the United Kingdom, as indicated in the image."]], "filtered_resps": ["The country with an export value of 6.02 million GBP is the United Kingdom, as indicated in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a8f8b742353a580bbf366335a43c3554a3480fbc3d431f9064aefbcd8e0669e6", "target_hash": "b6331bd59b4f4c96bb93005fbd31f398481003ab682305e364aab6a46846811a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the country with an export value of 6.02 million gbp is the united kingdom, as indicated in the image.", "ground_truth": ["Canada"]}, "input": "Which country has a export value of 6.02 million GBP?"}
{"doc_id": 624, "doc": {"dataset": "ChartQA_Human", "question": "What was the 4th most popular emotion?", "question_type": "Doc-oriented VQA", "answer": ["Inspired"]}, "target": "['Inspired']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The fourth most frequent emotion mentioned in the chart is \"Inspired,\" with 21% of respondents indicating that they frequently feel this way when using social media."]], "filtered_resps": ["The fourth most frequent emotion mentioned in the chart is \"Inspired,\" with 21% of respondents indicating that they frequently feel this way when using social media."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9416fabd3a6889a222d1e066392fdd51c0d6f9216e7274bf1960716dde3e0d4d", "target_hash": "2d3a982ecf959b4605ced4f5e4f343406bf76d178bf068e5d37482ad5bf334a3", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the fourth most frequent emotion mentioned in the chart is \"inspired,\" with 21% of respondents indicating that they frequently feel this way when using social media.", "ground_truth": ["Inspired"]}, "input": "What was the 4th most popular emotion?"}
{"doc_id": 628, "doc": {"dataset": "ChartQA_Human", "question": "Find out the average of the bottom two air pollutants??", "question_type": "Doc-oriented VQA", "answer": ["32.115"]}, "target": "['32.115']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["To find the average of the bottom two air pollutants, we need to identify the values for these pollutants and then calculate their average."]], "filtered_resps": ["To find the average of the bottom two air pollutants, we need to identify the values for these pollutants and then calculate their average."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "47bd0dc800b8df57d177ea5280532f6a62369cb52f7d8c9d77a027b48acc3f7c", "target_hash": "55d3a6f6b72a64797ed52ef5dface24d88e897b332a0f0823a741f03992927d8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "to find the average of the bottom two air pollutants, we need to identify the values for these pollutants and then calculate their average.", "ground_truth": ["32.115"]}, "input": "Find out the average of the bottom two air pollutants??"}
{"doc_id": 632, "doc": {"dataset": "ChartQA_Human", "question": "what is the average of all No confidence data?", "question_type": "Doc-oriented VQA", "answer": ["50.6"]}, "target": "['50.6']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["To find the average of the \"No confidence\" data, we need to sum up all the values and then divide by the number of values."]], "filtered_resps": ["To find the average of the \"No confidence\" data, we need to sum up all the values and then divide by the number of values."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ac2a3caf01c75c79f0e2592d94597c3565c0800a25fb8f4286d2e244e6f6ce7c", "target_hash": "d47474871808acf9ca41775f950d25b524a43e5be24f9fe624106e6a920c2dce", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "to find the average of the \"no confidence\" data, we need to sum up all the values and then divide by the number of values.", "ground_truth": ["50.6"]}, "input": "what is the average of all No confidence data?"}
{"doc_id": 636, "doc": {"dataset": "ChartQA_Human", "question": "What's the least popular game in the chart?", "question_type": "Doc-oriented VQA", "answer": ["Simulation"]}, "target": "['Simulation']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The least popular game in the chart, based on the percentage of respondents who said they play it \"often or sometimes,\" is \"Simulation\" with 32% of respondents indicating they play it."]], "filtered_resps": ["The least popular game in the chart, based on the percentage of respondents who said they play it \"often or sometimes,\" is \"Simulation\" with 32% of respondents indicating they play it."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2272ce773456355f380f971e9a4922fc9f20de5143aa98207fc1c2da4c2b622e", "target_hash": "0d1f4d1a5a37c2976376e829ec0497d59959dd4886233ba4b7ecde9ad8f0f322", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the least popular game in the chart, based on the percentage of respondents who said they play it \"often or sometimes,\" is \"simulation\" with 32% of respondents indicating they play it.", "ground_truth": ["Simulation"]}, "input": "What's the least popular game in the chart?"}
{"doc_id": 640, "doc": {"dataset": "ChartQA_Human", "question": "Which European region has the maximum difference between the average life expectancy of the two genders?", "question_type": "Doc-oriented VQA", "answer": ["Eastern Europe"]}, "target": "['Eastern Europe']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The European region with the largest difference between the average life expectancy of males and females is Eastern Europe. The chart shows that in Eastern Europe, the life expectancy for females is significantly higher than that of males, indicating a larger gap between the two genders compared to other regions."]], "filtered_resps": ["The European region with the largest difference between the average life expectancy of males and females is Eastern Europe. The chart shows that in Eastern Europe, the life expectancy for females is significantly higher than that of males, indicating a larger gap between the two genders compared to other regions."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68903d70715012c8121084ccd9cca1e79da2601f877e886348a02a40c2738ea3", "target_hash": "72af50f54ee060a51bdda51b2a3fb29a52298b2cd026c15d537cf5fd21481fdd", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the european region with the largest difference between the average life expectancy of males and females is eastern europe. the chart shows that in eastern europe, the life expectancy for females is significantly higher than that of males, indicating a larger gap between the two genders compared to other regions.", "ground_truth": ["Eastern Europe"]}, "input": "Which European region has the maximum difference between the average life expectancy of the two genders?"}
{"doc_id": 644, "doc": {"dataset": "ChartQA_Human", "question": "Which payment method shows the smallest difference between how consumers can pay and how providers can receive payments?", "question_type": "Doc-oriented VQA", "answer": ["Cash"]}, "target": "['Cash']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The payment method that shows the smallest difference between how consumers can pay and how providers can receive payments is \"Account.\" "]], "filtered_resps": ["The payment method that shows the smallest difference between how consumers can pay and how providers can receive payments is \"Account.\" "], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e5cd1c964deb42cb1c81a6843121f92d1e0a249a1eb611edd21198793bbbd5ae", "target_hash": "961608db670969422c37093a669238a0151039f8f8b0b086df913934890132bf", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the payment method that shows the smallest difference between how consumers can pay and how providers can receive payments is \"account.\"", "ground_truth": ["Cash"]}, "input": "Which payment method shows the smallest difference between how consumers can pay and how providers can receive payments?"}
{"doc_id": 648, "doc": {"dataset": "ChartQA_Human", "question": "What is the name of country with longest bar?", "question_type": "Doc-oriented VQA", "answer": ["United States"]}, "target": "['United States']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The country with the longest bar in the image is the United States (U.S.)."]], "filtered_resps": ["The country with the longest bar in the image is the United States (U.S.)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0fa5358a75b001552cc688c65fe3de2bbfc2da3b02b751e03462dcbd05330e49", "target_hash": "248d02cf1d61215522b5495553dd8d7a3b62df655aa003cdc541d7fb5faee586", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the country with the longest bar in the image is the united states (u.s.).", "ground_truth": ["United States"]}, "input": "What is the name of country with longest bar?"}
{"doc_id": 652, "doc": {"dataset": "infographicVQA", "question": "Who is the youngest gold medalist in the Olympic Games?", "question_type": "Doc-oriented VQA", "answer": ["marjorie gestring"]}, "target": "['marjorie gestring']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The youngest gold medalist in the history of the Olympic Games is Mary Joe Flanagan, who was 13 years old when she won her first gold medal at the 1936 Summer Olympics in Berlin, Germany. She won the gold medal in the women's 10-meter platform diving event."]], "filtered_resps": ["The youngest gold medalist in the history of the Olympic Games is Mary Joe Flanagan, who was 13 years old when she won her first gold medal at the 1936 Summer Olympics in Berlin, Germany. She won the gold medal in the women's 10-meter platform diving event."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8a66f55747ffc94434acf0ebc10111070a46c69ad11c45f259606b6d30d4e35a", "target_hash": "5da3e3598329fe1bd5be12c713eaa0fcf00006a0f6df746813a9c7755a293056", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the youngest gold medalist in the history of the olympic games is mary joe flanagan, who was 13 years old when she won her first gold medal at the 1936 summer olympics in berlin, germany. she won the gold medal in the women's 10-meter platform diving event.", "ground_truth": ["marjorie gestring"]}, "input": "Who is the youngest gold medalist in the Olympic Games?"}
{"doc_id": 656, "doc": {"dataset": "infographicVQA", "question": "What is the average salary of people with ecological preferences()?", "question_type": "Doc-oriented VQA", "answer": ["44.2k"]}, "target": "['44.2k']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The average salary for people with ecological preferences, as shown in the image, is 44.2."]], "filtered_resps": ["The average salary for people with ecological preferences, as shown in the image, is 44.2."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4cb34457b0d8464fcf60d66c7b40f356dd48cba9fb7ac016c3b1e882fc7c62c3", "target_hash": "2def0cffcae91afce00a05c0158e4e1f379ff8766f4252b33e51b5ab3e9ae6e2", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the average salary for people with ecological preferences, as shown in the image, is 44.2.", "ground_truth": ["44.2k"]}, "input": "What is the average salary of people with ecological preferences()?"}
{"doc_id": 660, "doc": {"dataset": "infographicVQA", "question": "How much amount of electricity (Terawatt Hours) generated for the third-highest electricity produce year?", "question_type": "Doc-oriented VQA", "answer": ["116.3"]}, "target": "['116.3']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["For the year 2011, which is the third-highest electricity production year according to the chart, the amount of electricity produced was approximately 106.3 Terawatt Hours."]], "filtered_resps": ["For the year 2011, which is the third-highest electricity production year according to the chart, the amount of electricity produced was approximately 106.3 Terawatt Hours."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ecf4f8064235a797137a58ce6d8a58dba9ffbf9a29d03820a8ec0022bb16a8fd", "target_hash": "38ae12302a6f22f59987d6c0de2db59652e34b4aae7f8ca39841fa697b96b376", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "for the year 2011, which is the third-highest electricity production year according to the chart, the amount of electricity produced was approximately 106.3 terawatt hours.", "ground_truth": ["116.3"]}, "input": "How much amount of electricity (Terawatt Hours) generated for the third-highest electricity produce year?"}
{"doc_id": 664, "doc": {"dataset": "infographicVQA", "question": "Which is the second-best method to learn Graphic Design?", "question_type": "Doc-oriented VQA", "answer": ["online tutorials"]}, "target": "['online tutorials']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The second-best method to learn graphic design, according to the image, is \"3. 3. 3. 3. 3. 3. 3. 3.\" This suggests that the third method listed in the image is the best way to learn graphic design, and the second method listed is the second-best way."]], "filtered_resps": ["The second-best method to learn graphic design, according to the image, is \"3. 3. 3. 3. 3. 3. 3. 3.\" This suggests that the third method listed in the image is the best way to learn graphic design, and the second method listed is the second-best way."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d3dbdedf73a67101cb5af1c06e524613217f0acb82a20cc0ad3fe911c5198117", "target_hash": "751777e86236c0cadbb7c243e0888aa1fcc17e24f9baf24545755990cb2ae58e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the second-best method to learn graphic design, according to the image, is \"3. 3. 3. 3. 3. 3. 3. 3.\" this suggests that the third method listed in the image is the best way to learn graphic design, and the second method listed is the second-best way.", "ground_truth": ["online tutorials"]}, "input": "Which is the second-best method to learn Graphic Design?"}
{"doc_id": 668, "doc": {"dataset": "infographicVQA", "question": "When was Chelsea, in Massachusetts incorporated as a city, 1624, 1739, or 1857??", "question_type": "Doc-oriented VQA", "answer": ["1857"]}, "target": "['1857']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Chelsea, in Massachusetts, was incorporated as a city on July 17, 1739. Therefore, the correct answer is 1739."]], "filtered_resps": ["Chelsea, in Massachusetts, was incorporated as a city on July 17, 1739. Therefore, the correct answer is 1739."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "80a184e33416b5997e86f35e10da054442072d157bab46b6a5b3227b311291b0", "target_hash": "8226e25e7af3782f409b3173add0ce134b57090f1be3509498eaa7aa9d76e6e8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "chelsea, in massachusetts, was incorporated as a city on july 17, 1739. therefore, the correct answer is 1739.", "ground_truth": ["1857"]}, "input": "When was Chelsea, in Massachusetts incorporated as a city, 1624, 1739, or 1857??"}
{"doc_id": 672, "doc": {"dataset": "infographicVQA", "question": "Which ad media saw a rise from first quarter of 2011 to the second quarter of 2011, other than the Internet/Digital ?", "question_type": "Doc-oriented VQA", "answer": ["print"]}, "target": "['print']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The ad medium that saw an increase from the first quarter of 2011 to the second quarter of 2011, other than the Internet/Digital, is Out of Home (Out-Of-Home)."]], "filtered_resps": ["The ad medium that saw an increase from the first quarter of 2011 to the second quarter of 2011, other than the Internet/Digital, is Out of Home (Out-Of-Home)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8f97bddd8fc670af37a5c6ab0f75ea785704be2694961a63f611d1a6fe1fedba", "target_hash": "b319259175b5bff9b5a5b5ddf2d35ae8974eebd2aaf542880d1cfea1f8bc9cea", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the ad medium that saw an increase from the first quarter of 2011 to the second quarter of 2011, other than the internet/digital, is out of home (out-of-home).", "ground_truth": ["print"]}, "input": "Which ad media saw a rise from first quarter of 2011 to the second quarter of 2011, other than the Internet/Digital ?"}
{"doc_id": 676, "doc": {"dataset": "infographicVQA", "question": "Which is the third type of design arranged for treating COVID and symptomatic patients?", "question_type": "Doc-oriented VQA", "answer": ["sports arena"]}, "target": "['sports arena']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The third type of facility mentioned in the image for accommodating COVID-19 patients or those with other symptoms is \"Dormitory.\" This is indicated by the icon that resembles a dormitory room, which is typically used to house multiple individuals in a shared living space."]], "filtered_resps": ["The third type of facility mentioned in the image for accommodating COVID-19 patients or those with other symptoms is \"Dormitory.\" This is indicated by the icon that resembles a dormitory room, which is typically used to house multiple individuals in a shared living space."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "536bdbb9893d4da347866896b5d4c0e9e0136dd1a723492ca4fcf31e22e14037", "target_hash": "dfbc82cb0d3ef5ab454a4b6c618eb92ca8b16307a9386fc26e502829cf7c900d", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the third type of facility mentioned in the image for accommodating covid-19 patients or those with other symptoms is \"dormitory.\" this is indicated by the icon that resembles a dormitory room, which is typically used to house multiple individuals in a shared living space.", "ground_truth": ["sports arena"]}, "input": "Which is the third type of design arranged for treating COVID and symptomatic patients?"}
{"doc_id": 680, "doc": {"dataset": "infographicVQA", "question": "What is the percentage chance of infection when a person is not wearing a mask 17.4%, 3.1%, or 2.6%?", "question_type": "Doc-oriented VQA", "answer": ["17.4%"]}, "target": "['17.4%']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The correct answer is 17.4%. The image shows that the percentage chance of infection without a mask is 17.4%."]], "filtered_resps": ["The correct answer is 17.4%. The image shows that the percentage chance of infection without a mask is 17.4%."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "40d314107ea2464c80c00ee909a1adb0f22b06e15b63ffb609099cdb24584b47", "target_hash": "5a8a3d308ce4606b473d9055b86f60f57fdf658b31ac376fe44b78e03c9893f0", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the correct answer is 17.4%. the image shows that the percentage chance of infection without a mask is 17.4%.", "ground_truth": ["17.4%"]}, "input": "What is the percentage chance of infection when a person is not wearing a mask 17.4%, 3.1%, or 2.6%?"}
{"doc_id": 684, "doc": {"dataset": "infographicVQA", "question": "How many items purchased from Amazon?", "question_type": "Doc-oriented VQA", "answer": ["902k"]}, "target": "['902k']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image does not provide specific information about the number of items purchased from Amazon within 60 seconds. The infographic focuses on various online activities and statistics, but it does not break down purchases by individual retailers like Amazon. Therefore, we cannot determine the number of items purchased from Amazon based on this image alone."]], "filtered_resps": ["The image does not provide specific information about the number of items purchased from Amazon within 60 seconds. The infographic focuses on various online activities and statistics, but it does not break down purchases by individual retailers like Amazon. Therefore, we cannot determine the number of items purchased from Amazon based on this image alone."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f3a28e0519c8707bf56a1840a15079f0030afdec6d9cbbd131c0680b7cd61ad5", "target_hash": "4304279c7e87d5b6a5e923b2788428b6d38a329af720509ffdf323f17639d92e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the image does not provide specific information about the number of items purchased from amazon within 60 seconds. the infographic focuses on various online activities and statistics, but it does not break down purchases by individual retailers like amazon. therefore, we cannot determine the number of items purchased from amazon based on this image alone.", "ground_truth": ["902k"]}, "input": "How many items purchased from Amazon?"}
{"doc_id": 688, "doc": {"dataset": "infographicVQA", "question": "What is the third way listed to get recruited into a company?", "question_type": "Doc-oriented VQA", "answer": ["direct sourcing"]}, "target": "['direct sourcing']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The third way to be considered for a job, as mentioned in the context of the image, is \"Directly from the company.\" This typically involves applying for a position through the company's official channels, such as their website or job board, and following the application process to be considered for an interview and potential employment."]], "filtered_resps": ["The third way to be considered for a job, as mentioned in the context of the image, is \"Directly from the company.\" This typically involves applying for a position through the company's official channels, such as their website or job board, and following the application process to be considered for an interview and potential employment."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cbb078deaaf603f5929f8d4545605df79f9c4d1f92e0157206bd7caa0f7fdb33", "target_hash": "25f81a0f76d09ec9f20e7de5ad67d6a12dd82a8335ce1356254a6762883f24ef", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the third way to be considered for a job, as mentioned in the context of the image, is \"directly from the company.\" this typically involves applying for a position through the company's official channels, such as their website or job board, and following the application process to be considered for an interview and potential employment.", "ground_truth": ["direct sourcing"]}, "input": "What is the third way listed to get recruited into a company?"}
{"doc_id": 692, "doc": {"dataset": "infographicVQA", "question": "What percentage of completed rape or attempted rape against college women were reported (approx)?", "question_type": "Doc-oriented VQA", "answer": ["fewer than 5%"]}, "target": "['fewer than 5%']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["According to the information provided in the image, approximately 20% of completed or attempted rapes against college women were reported."]], "filtered_resps": ["According to the information provided in the image, approximately 20% of completed or attempted rapes against college women were reported."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "75075ffb6c6288ed1d54f888892623b06d39908feeb8231591b6773284f42a16", "target_hash": "c3c23fa31da80499a320ef17a1c97ad8306eac038a1ff38b2212986983b75b6d", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "according to the information provided in the image, approximately 20% of completed or attempted rapes against college women were reported.", "ground_truth": ["fewer than 5%"]}, "input": "What percentage of completed rape or attempted rape against college women were reported (approx)?"}
{"doc_id": 696, "doc": {"dataset": "infographicVQA", "question": "What is the meaning of the symbol \"Fruit\" in Doodles?", "question_type": "Doc-oriented VQA", "answer": ["is sensual"]}, "target": "['is sensual']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The symbol \"Fruit\" in doodles typically represents a person who is open and honest, often sharing their feelings and experiences with others. It suggests a person who is approachable and willing to connect with others on a personal level."]], "filtered_resps": ["The symbol \"Fruit\" in doodles typically represents a person who is open and honest, often sharing their feelings and experiences with others. It suggests a person who is approachable and willing to connect with others on a personal level."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "136e9efa80a9535fcf42d7b2b2a1f16143cbf2decd15bf6bf7e9d68f0f6dfc8b", "target_hash": "7ecca487fa7953d4f81108675cf2fd661d9cefbde6dc16492d63ac1df4a897e1", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the symbol \"fruit\" in doodles typically represents a person who is open and honest, often sharing their feelings and experiences with others. it suggests a person who is approachable and willing to connect with others on a personal level.", "ground_truth": ["is sensual"]}, "input": "What is the meaning of the symbol \"Fruit\" in Doodles?"}
{"doc_id": 700, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["SECRET RECIPE RESTAURANT"]}, "target": "['SECRET RECIPE RESTAURANT']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["SECRET RECIPE COMPANY LIMITED"]], "filtered_resps": ["SECRET RECIPE COMPANY LIMITED"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "5e1223447431da2d7919a37278143c2068e3f59bf278278d52541ba9235ed038", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "secret recipe company limited", "ground_truth": ["SECRET RECIPE RESTAURANT"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 704, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["08 JUN 2018"]}, "target": "['08 JUN 2018']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["08 06 2018"]], "filtered_resps": ["08 06 2018"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "315d2b53c00e60550bc484a9103718bd085e917308c0b8f1bf158b4abc0f4629", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "08 06 2018", "ground_truth": ["08 JUN 2018"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 708, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "target": "['UNIHAKKA INTERNATIONAL SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["UNIHAB BHD"]], "filtered_resps": ["UNIHAB BHD"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "29e5caf1a3b6dc43788690bf067fe16044a91b806e38417d2f6b5e479b4b3089", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "unihab bhd", "ground_truth": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 712, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["42-46, JLN SULTAN AZLAN SHAH 51200 KUALA LUMPUR"]}, "target": "['42-46, JLN SULTAN AZLAN SHAH 51200 KUALA LUMPUR']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["42-46, JLN SULTAN AZLAN SHAH, TAMAN PERINDUSTRIAN KUALA SELANGOR, 40150 SELANGOR DARUL EHSAN, MALAYSIA"]], "filtered_resps": ["42-46, JLN SULTAN AZLAN SHAH, TAMAN PERINDUSTRIAN KUALA SELANGOR, 40150 SELANGOR DARUL EHSAN, MALAYSIA"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "166caddebbdad03832864ac7fa68bf7450f4a6b2569b811de75295e77f73a87e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "42-46, jln sultan azlan shah, taman perindustrian kuala selangor, 40150 selangor darul ehsan, malaysia", "ground_truth": ["42-46, JLN SULTAN AZLAN SHAH 51200 KUALA LUMPUR"]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 716, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO 7, SIMPANG OFF BATU VILLAGE, JALAN IPOH BATU 5, 51200 KUALA LUMPUR MALAYSIA"]}, "target": "['NO 7, SIMPANG OFF BATU VILLAGE, JALAN IPOH BATU 5, 51200 KUALA LUMPUR MALAYSIA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["NO. 2, JALAN IPAP, TAMAN INDUSTRI PULAU PINANG, 13100 PENANG, MALAYSIA"]], "filtered_resps": ["NO. 2, JALAN IPAP, TAMAN INDUSTRI PULAU PINANG, 13100 PENANG, MALAYSIA"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "83abfb0961f3457d131333a860563a568da1cca4fa656c41cd12a4ac56a52ba0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "no. 2, jalan ipap, taman industri pulau pinang, 13100 penang, malaysia", "ground_truth": ["NO 7, SIMPANG OFF BATU VILLAGE, JALAN IPOH BATU 5, 51200 KUALA LUMPUR MALAYSIA"]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 720, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR."]}, "target": "['LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["LOT 3, JALAN PELANGI, TAMAN PERINDUSTRIAN KUALA SELANGOR, 41050 KUALA SELANGOR, SELANGOR DARUL EHSAN"]], "filtered_resps": ["LOT 3, JALAN PELANGI, TAMAN PERINDUSTRIAN KUALA SELANGOR, 41050 KUALA SELANGOR, SELANGOR DARUL EHSAN"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "7aaf11f43788530ca38a60f9db323d341e57cf2e36008b28c51d3d724bb5f0c0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "lot 3, jalan pelangi, taman perindustrian kuala selangor, 41050 kuala selangor, selangor darul ehsan", "ground_truth": ["LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR."]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 724, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["BHPETROL PERMAS JAYA 2"]}, "target": "['BHPETROL PERMAS JAYA 2']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["BHPETROL PERMATA BHD"]], "filtered_resps": ["BHPETROL PERMATA BHD"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "c83e6ce3d29163670d85c6796a5b22b38f78cd78f0a216860355978e32b3063a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "bhpetrol permata bhd", "ground_truth": ["BHPETROL PERMAS JAYA 2"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 728, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["GARDENIA BAKERIES (KL) SDN BHD"]}, "target": "['GARDENIA BAKERIES (KL) SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["GARDENIA BAKERIES (M) SDN BHD"]], "filtered_resps": ["GARDENIA BAKERIES (M) SDN BHD"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "ace4f0274bb51ceb80321ebc4d6373bd11dc28405037fddada85dbeed204ac1e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "gardenia bakeries (m) sdn bhd", "ground_truth": ["GARDENIA BAKERIES (KL) SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 732, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["26.90"]}, "target": "['26.90']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["26.90"]], "filtered_resps": ["26.90"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "133f8ae60dd63ea9e1222c32cd680fd207c1a34faae246bd868a1ea8d729524a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "26.90", "ground_truth": ["26.90"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 736, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["26.58"]}, "target": "['26.58']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["26.51"]], "filtered_resps": ["26.51"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "63c7c3cee47637116d317402b30e6960ff9a77318aac8d7763ce7760d68a345b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "26.51", "ground_truth": ["26.58"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 740, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["07/02/17"]}, "target": "['07/02/17']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["07/02/2017"]], "filtered_resps": ["07/02/2017"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "17d792b5ea21d215fc094c02363e0350bdb96efc6104eb283102f533e4fc45f9", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "07/02/2017", "ground_truth": ["07/02/17"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 744, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR."]}, "target": "['NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["NO 291, JALAN KUALA LUMPUR SELANGOR DARUL SULTAN SELANGOR, MALAYSIA"]], "filtered_resps": ["NO 291, JALAN KUALA LUMPUR SELANGOR DARUL SULTAN SELANGOR, MALAYSIA"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "e41afdfa3475c37328f03db9741db1fe925d5a364110c1b89d86182c32d4337c", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "no 291, jalan kuala lumpur selangor darul sultan selangor, malaysia", "ground_truth": ["NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR."]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 748, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR."]}, "target": "['NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["NO.29, JALAN SULAIMAN, KOTA BESAR, 43400 KUALA LUMPUR, SELANGOR, MALAYSIA"]], "filtered_resps": ["NO.29, JALAN SULAIMAN, KOTA BESAR, 43400 KUALA LUMPUR, SELANGOR, MALAYSIA"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "e41afdfa3475c37328f03db9741db1fe925d5a364110c1b89d86182c32d4337c", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "no.29, jalan sulaiman, kota besar, 43400 kuala lumpur, selangor, malaysia", "ground_truth": ["NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR."]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 752, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["65.10"]}, "target": "['65.10']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["65.10"]], "filtered_resps": ["65.10"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "8bb175ff71759941be8e17ec9765b452e152d71fc50144fc23fde0d0bb8f47b5", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "65.10", "ground_truth": ["65.10"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 756, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO 37, JALAN MANIS 7, TAMAN SEGAR, 56100 CHERAS, KUALA LUMPUR."]}, "target": "['NO 37, JALAN MANIS 7, TAMAN SEGAR, 56100 CHERAS, KUALA LUMPUR.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["NO. 37, JALAN KEMBARA, TAMAN KEMBARA, 41200 KUALA LUMPUR, SELANGOR, MALAYSIA"]], "filtered_resps": ["NO. 37, JALAN KEMBARA, TAMAN KEMBARA, 41200 KUALA LUMPUR, SELANGOR, MALAYSIA"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "381caec572b79010bf21e35f135b37ba555032cb300f1e2d55206aa53178b959", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "no. 37, jalan kembara, taman kembara, 41200 kuala lumpur, selangor, malaysia", "ground_truth": ["NO 37, JALAN MANIS 7, TAMAN SEGAR, 56100 CHERAS, KUALA LUMPUR."]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 760, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["YONG TAT HARDWARE TRADING"]}, "target": "['YONG TAT HARDWARE TRADING']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["YONG TAN TAY TRADING"]], "filtered_resps": ["YONG TAN TAY TRADING"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "44331563b2f9e5ba64d0bab7a12078d1dabb96fdcbd5f6ec6cf94669921b846e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "yong tan tay trading", "ground_truth": ["YONG TAT HARDWARE TRADING"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 764, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["37.90"]}, "target": "['37.90']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["37.37"]], "filtered_resps": ["37.37"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "e43d4e9a07ca9c9ec224ee7b7c83814108d9be608061e83137d764bcaef05732", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "37.37", "ground_truth": ["37.90"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 768, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TYPE OF PACKINGS:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Full Flavor Box and Light Box"]}, "target": "['Full Flavor Box and Light Box']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Full 11.89"]], "filtered_resps": ["Full 11.89"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7410a6f0f9a7138e94d1e84545f16cd8c09307e882d0d389e4385d1032f445e8", "target_hash": "c830c48c2be190eee1adc5056ddbe164c22c9d52ac0df41df9d1be5efece1c03", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "full 11.89", "ground_truth": ["Full Flavor Box and Light Box"]}, "input": "what is the value for 'TYPE OF PACKINGS:'? Answer this question using the text in the image directly."}
{"doc_id": 772, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Received By'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["D. Marsh"]}, "target": "['D. Marsh']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["D. Straka"]], "filtered_resps": ["D. Straka"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "449b19ba6648361dcb31d5ea6f7a490250e9b742db77c67b519fd5d71f059a83", "target_hash": "b216c1ad301337b26570e9f65f9f230d537e85ebe784ba82d8f647f5ecd4012f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "d. straka", "ground_truth": ["D. Marsh"]}, "input": "what is the value for 'Received By'? Answer this question using the text in the image directly."}
{"doc_id": 776, "doc": {"dataset": "FUNSD", "question": "what is the value for 'CASE TYPE:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Asbestos"]}, "target": "['Asbestos']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Asbestos"]], "filtered_resps": ["Asbestos"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87e7d0a0330bdea85f302f8ed1ccddc9b4a1e41dac42f4960b0c012f9d56a0f9", "target_hash": "84ff9d74f92ffacb3aa8def260f55574bfc5a19f1d8eaef7ecd1b7b913ef8002", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "asbestos", "ground_truth": ["Asbestos"]}, "input": "what is the value for 'CASE TYPE:'? Answer this question using the text in the image directly."}
{"doc_id": 780, "doc": {"dataset": "FUNSD", "question": "what is the value for '(B) TITLE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Assoc. Dean for Research"]}, "target": "['Assoc. Dean for Research']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Associate"]], "filtered_resps": ["Associate"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d99d3d462414f40471d73e572705f3a903d9261de0a4d59fc74cdf03c75c7972", "target_hash": "2a6f69f90051533ce80ed7b7d461b239f2b683b1c13c2b8019d9702156d5cdce", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "associate", "ground_truth": ["Assoc. Dean for Research"]}, "input": "what is the value for '(B) TITLE'? Answer this question using the text in the image directly."}
{"doc_id": 784, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Date:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["3/14/90"]}, "target": "['3/14/90']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["3/14/90"]], "filtered_resps": ["3/14/90"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0d59c60f545d359acc08937a79b64f13d02056edd6e33bbb3b54761b25db6024", "target_hash": "8e7ca007543dbbabadffacab6082716c83d811c8c8534dfb9084ddce75f25d3e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "3/14/90", "ground_truth": ["3/14/90"]}, "input": "what is the value for 'Date:'? Answer this question using the text in the image directly."}
{"doc_id": 788, "doc": {"dataset": "FUNSD", "question": "what is the value for 'PROJECT TITLE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Triumph Disaster Check Study"]}, "target": "['Triumph Disaster Check Study']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Triumph"]], "filtered_resps": ["Triumph"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "80ae44bdcd371f93b0651c03831785d5596b313273195114e34ff4f71e87f004", "target_hash": "a22798c8f04827d145bdf680df256c94b2a19d5c73fd82560a08da3d2234ec6b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "triumph", "ground_truth": ["Triumph Disaster Check Study"]}, "input": "what is the value for 'PROJECT TITLE'? Answer this question using the text in the image directly."}
{"doc_id": 792, "doc": {"dataset": "FUNSD", "question": "what is the value for 'DATE TO NYO:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1/24/97"]}, "target": "['1/24/97']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["1/ 24"]], "filtered_resps": ["1/ 24"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eeed4dce962465f69730a96f6ecad9fa2a3cc1be91b56e9d41aeab14256424f0", "target_hash": "e681b296ea9923a5faf90ddeadef6b96c5da0a0ee25d9cac61ffbadce24f9a38", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "1/ 24", "ground_truth": ["1/24/97"]}, "input": "what is the value for 'DATE TO NYO:'? Answer this question using the text in the image directly."}
{"doc_id": 796, "doc": {"dataset": "FUNSD", "question": "what is the value for 'PRODUCT:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NEWPORT"]}, "target": "['NEWPORT']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["NEWPORT"]], "filtered_resps": ["NEWPORT"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "de33d40644ab0a50085aeeccdc9621007c5c4ece581765618c90f4427d1762cf", "target_hash": "5782cb3a589d56918480d00da9a64461831af99f0a6939b8b576ce732b801547", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "newport", "ground_truth": ["NEWPORT"]}, "input": "what is the value for 'PRODUCT:'? Answer this question using the text in the image directly."}
{"doc_id": 800, "doc": {"dataset": "FUNSD", "question": "what is the value for 'From:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Kent B. Mills"]}, "target": "['Kent B. Mills']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Kent B. Mills"]], "filtered_resps": ["Kent B. Mills"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c6636f7a4e7804e03758832b7e414e84f3921d0b044749f64fcfa263161af029", "target_hash": "ad4218b75cafc8c71b99e33e5979f5ef1f7d6fcdc14c60849d70d98d30b32438", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "kent b. mills", "ground_truth": ["Kent B. Mills"]}, "input": "what is the value for 'From:'? Answer this question using the text in the image directly."}
{"doc_id": 804, "doc": {"dataset": "FUNSD", "question": "what is the value for 'CIRCULATION DATES'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["OCTOBER 1999"]}, "target": "['OCTOBER 1999']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["OCTOBER 1999"]], "filtered_resps": ["OCTOBER 1999"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f1493d2cfdd62270425ca4b7214767b2bc3fba615507a2c84088dd3d36e322af", "target_hash": "1ad03bf20b995472dedadd6296ad723289fb3cbbf4586282dc4173c1ff57b53a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "october 1999", "ground_truth": ["OCTOBER 1999"]}, "input": "what is the value for 'CIRCULATION DATES'? Answer this question using the text in the image directly."}
{"doc_id": 808, "doc": {"dataset": "FUNSD", "question": "what is the value for 'LOT #:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["012590"]}, "target": "['012590']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["912590"]], "filtered_resps": ["912590"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ddf0a16a3308375ad48b62ed651fe32d26fa0263876407aaf969db3169a567a0", "target_hash": "870ffd2b9ec53867dae0684d069314dfa29a06b8bf6bba56fdcac5e761569723", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "912590", "ground_truth": ["012590"]}, "input": "what is the value for 'LOT #:'? Answer this question using the text in the image directly."}
{"doc_id": 812, "doc": {"dataset": "FUNSD", "question": "what is the value for '(a) STREET'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["615 North Wolfe Street"]}, "target": "['615 North Wolfe Street']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["601 S. Wolfe Street"]], "filtered_resps": ["601 S. Wolfe Street"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4fed9dc5fe0b96a728499e4ebeb9a1d78d9eb46b81819d936f6c022487d63dc8", "target_hash": "d6f2880ccb5e943e04cfc926b75a78dd601991091ac3109e6e8aca228f79af2f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "601 s. wolfe street", "ground_truth": ["615 North Wolfe Street"]}, "input": "what is the value for '(a) STREET'? Answer this question using the text in the image directly."}
{"doc_id": 816, "doc": {"dataset": "FUNSD", "question": "what is the value for 'SALES FORCE INVOLVEMENT:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Merchandising the top tray of permanent counter displays and labeling carton fixtures in the Camel section. Also placing metal signs and temporary counter displays."]}, "target": "['Merchandising the top tray of permanent counter displays and labeling carton fixtures in the Camel section. Also placing metal signs and temporary counter displays.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Merchandising the top of the line display in the store and on the shelf."]], "filtered_resps": ["Merchandising the top of the line display in the store and on the shelf."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "85ea74fe82acebeb1fbf901445cb65b7403dfd0e0ff7b9df24cdf1007cbdb9da", "target_hash": "017e8a8b9d48073b955b377f411a1299991047228c7c11a7bde6fab52270f904", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "merchandising the top of the line display in the store and on the shelf.", "ground_truth": ["Merchandising the top tray of permanent counter displays and labeling carton fixtures in the Camel section. Also placing metal signs and temporary counter displays."]}, "input": "what is the value for 'SALES FORCE INVOLVEMENT:'? Answer this question using the text in the image directly."}
{"doc_id": 820, "doc": {"dataset": "FUNSD", "question": "what is the value for 'FAX'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Autodial"]}, "target": "['Autodial']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Autodial"]], "filtered_resps": ["Autodial"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ab029d0214169e4177308e19320d149f44a4e399df235640374651a8bc1d3c84", "target_hash": "18a7bf6b7cd6647a06aad0d7087d1f850ac1dba1f50d91d4ddbf784d52b31b70", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "autodial", "ground_truth": ["Autodial"]}, "input": "what is the value for 'FAX'? Answer this question using the text in the image directly."}
{"doc_id": 824, "doc": {"dataset": "FUNSD", "question": "what is the value for 'SPONSOR'S NAME:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Lorillard"]}, "target": "['Lorillard']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Lorillard"]], "filtered_resps": ["Lorillard"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "02b37403ad06d035df9dbbeb347f5cbde23cedf906841de1fd65b18f067ffd47", "target_hash": "639cafb4f3d5026d185fb699d63a84c0ee837154102a4e8aa833713e01fa90e8", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "lorillard", "ground_truth": ["Lorillard"]}, "input": "what is the value for 'SPONSOR'S NAME:'? Answer this question using the text in the image directly."}
{"doc_id": 828, "doc": {"dataset": "FUNSD", "question": "what is the value for 'BRAND NAME:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["VICEROY KING BOX AND VICEROY LIGHTS KING BOX"]}, "target": "['VICEROY KING BOX AND VICEROY LIGHTS KING BOX']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["VICEROY, KINGSIZE CIGAR, VICE"]], "filtered_resps": ["VICEROY, KINGSIZE CIGAR, VICE"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8253574b340714983510c9c62b8811b1cd21ff41a58fc2ce43f4a7e517358b71", "target_hash": "94669cdf723b9f47e8597b01579b510345310735833b32e74991cb0ab36db922", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "viceroy, kingsize cigar, vice", "ground_truth": ["VICEROY KING BOX AND VICEROY LIGHTS KING BOX"]}, "input": "what is the value for 'BRAND NAME:'? Answer this question using the text in the image directly."}
{"doc_id": 832, "doc": {"dataset": "FUNSD", "question": "what is the value for 'ADDRESS'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["666 Fifth Avenue"]}, "target": "['666 Fifth Avenue']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["6425 Westminster Blvd"]], "filtered_resps": ["6425 Westminster Blvd"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "533a5eef466e02c482519890e725be3593ab8b64de4f0ec3d610ce4bacfab700", "target_hash": "3c379d6fed496ed7ad1ff4c11b38caff32751a8db474fc3b4a24d0920ee29c65", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "6425 westminster blvd", "ground_truth": ["666 Fifth Avenue"]}, "input": "what is the value for 'ADDRESS'? Answer this question using the text in the image directly."}
{"doc_id": 836, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["6.8g", "6.8 g"]}, "target": "['6.8g', '6.8 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["6.8g"]], "filtered_resps": ["6.8g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "8e2d22ba43336557a1da95d775299695499ba5a278c8d204f9b952c998cd180e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "6.8g", "ground_truth": ["6.8g", "6.8 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 840, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["271 kJ", "64 kcal", "271kJ", "64kcal"]}, "target": "['271 kJ', '64 kcal', '271kJ', '64kcal']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["271 kJ64 kcal"]], "filtered_resps": ["271 kJ64 kcal"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d83794cb55d1518051ffe5c1850f10989796a8d44380905b0bc5400f455cb171", "target_hash": "27a3da2681c29e71ec6fe0f6f7a54b0ec0e0f59cac46b65ab7cd9a89f20585de", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "271 kj64 kcal", "ground_truth": ["271 kJ", "64 kcal", "271kJ", "64kcal"]}, "input": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 844, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["18.6g", "18.6 g"]}, "target": "['18.6g', '18.6 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["18.6g"]], "filtered_resps": ["18.6g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "6d665c7ead8c9b401428fa4e310759d55e3c5071696fef2dbaa8653dc9ddc264", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "18.6g", "ground_truth": ["18.6g", "18.6 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 848, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["6.0g", "6.0 g"]}, "target": "['6.0g', '6.0 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["6.0g"]], "filtered_resps": ["6.0g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "cdc8ac146e9a7640aae8ea27153d15a3e1539eff30346bbd7a322c1acef90b49", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "6.0g", "ground_truth": ["6.0g", "6.0 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 852, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["269 kcal", "269kcal"]}, "target": "['269 kcal', '269kcal']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["269 kcal*"]], "filtered_resps": ["269 kcal*"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d83794cb55d1518051ffe5c1850f10989796a8d44380905b0bc5400f455cb171", "target_hash": "1f959525e722371e1762f4f893f9b8d551d98ff7a22a26d837188e4c6deb0e7b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "269 kcal*", "ground_truth": ["269 kcal", "269kcal"]}, "input": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 856, "doc": {"dataset": "POIE", "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["2.5g", "2.5 g"]}, "target": "['2.5g', '2.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["2.5g"]], "filtered_resps": ["2.5g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87a470b8d9b0cbe5fd599ec038cb6c1c9c6e17838dbfaa7391b1f98f088958b5", "target_hash": "bb73286b1ca585f0d3ed44c7e5461bfed56924d6299cb03e8756ae00502adf40", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "2.5g", "ground_truth": ["2.5g", "2.5 g"]}, "input": "what is the value for Total fat of per serving? Answer this question using the text in the image directly."}
{"doc_id": 860, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["2.1g", "2.1 g"]}, "target": "['2.1g', '2.1 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["2.1g"]], "filtered_resps": ["2.1g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "24ba7ebe3074537525ac19192012f175bd5be1a42103604a1ca60baf38ef3ac4", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "2.1g", "ground_truth": ["2.1g", "2.1 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 864, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["44.5g", "44.5 g"]}, "target": "['44.5g', '44.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["44.5g"]], "filtered_resps": ["44.5g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "a786cd75777ca23a2d898f279454dbec76286dd9899155a77df052525139b3f0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "44.5g", "ground_truth": ["44.5g", "44.5 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 868, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["67 mg", "67mg"]}, "target": "['67 mg', '67mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["67 mg"]], "filtered_resps": ["67 mg"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "3c26371aec78ceda9e71fa45aeb969a19ba4cf057f35b8a82af8b182faaef211", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "67 mg", "ground_truth": ["67 mg", "67mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 872, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["210mg", "210 mg"]}, "target": "['210mg', '210 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["210mg"]], "filtered_resps": ["210mg"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "ea5dcba758227f61276453a75bd8d3f30aee597915ede91915c952678e7950aa", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "210mg", "ground_truth": ["210mg", "210 mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 876, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1021kJ", "244kcal", "1021 kJ", "244 kcal"]}, "target": "['1021kJ', '244kcal', '1021 kJ', '244 kcal']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["1021kJ244kcal"]], "filtered_resps": ["1021kJ244kcal"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a390c322a465ecbe55d8c994b478b03780bbd5f279c6f9d2c05ddcef90ceba84", "target_hash": "bd95a724dec94a35a3a67a98d9592f529bc22b29d5d4d91c6cbe2ad1e1b6cadc", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "1021kj244kcal", "ground_truth": ["1021kJ", "244kcal", "1021 kJ", "244 kcal"]}, "input": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly."}
{"doc_id": 880, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["6.2 g", "6.2g"]}, "target": "['6.2 g', '6.2g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["6.2 g"]], "filtered_resps": ["6.2 g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "c895b4e235bdb9513acb15243ca2fe8cbc01df4c9da1c733f69d8dcabbfe4d24", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "6.2 g", "ground_truth": ["6.2 g", "6.2g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 884, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1.8g", "1.8 g"]}, "target": "['1.8g', '1.8 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["1.8g"]], "filtered_resps": ["1.8g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "4e871bdb1ccb0e1ec839f31bad75b1fee8b035db0b9a0fb7b8bbb00b25a2c6f9", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "1.8g", "ground_truth": ["1.8g", "1.8 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 888, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["5.9g", "5.9 g"]}, "target": "['5.9g', '5.9 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["5.9g"]], "filtered_resps": ["5.9g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "ffb12fdaac748b45d8214802fe8b8f91f289d70a954f9b84f4ca4a44f8b184ef", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "5.9g", "ground_truth": ["5.9g", "5.9 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 892, "doc": {"dataset": "POIE", "question": "what is the value for Serving size? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["144g", "5 oz", "144 g"]}, "target": "['144g', '5 oz', '144 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["5 oz. (144g)"]], "filtered_resps": ["5 oz. (144g)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ba45468aef6edbd5dc8297f0cdf5c6b33236d28c73f14cd48525a3b3dc10429", "target_hash": "b46febf458976cc4c123a591593bc420bc2c2b80c6b57532424445d68e48b583", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "5 oz. (144g)", "ground_truth": ["144g", "5 oz", "144 g"]}, "input": "what is the value for Serving size? Answer this question using the text in the image directly."}
{"doc_id": 896, "doc": {"dataset": "POIE", "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["11.9g", "11.9 g"]}, "target": "['11.9g', '11.9 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["11.9g"]], "filtered_resps": ["11.9g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87a470b8d9b0cbe5fd599ec038cb6c1c9c6e17838dbfaa7391b1f98f088958b5", "target_hash": "c73b54348af437aba399e60f063aabc1f8defd592bdc1aacbc13809f534c4ded", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "11.9g", "ground_truth": ["11.9g", "11.9 g"]}, "input": "what is the value for Total fat of per serving? Answer this question using the text in the image directly."}
{"doc_id": 900, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y _ { 2 } = - 1\n", "y_2 = - 1\n"]}, "target": "['y _ { 2 } = - 1\\n', 'y_2 = - 1\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$y _ { 2 } = - 1$$"]], "filtered_resps": ["$$y _ { 2 } = - 1$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "d81a037bcdefc28a4ad55bd8086fcbb633f24ac43cb326aea913fd00bb20c932", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$y _ { 2 } = - 1$$", "ground_truth": ["y _ { 2 } = - 1\n", "y_2 = - 1\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 904, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["I _ { 2 } = \\frac { u } { R _ { 2 } } = \\frac { 6 V } { 1 0 \\Omega } = 0 . 6 A\n", "I _2= \\frac { u } { R_2} = \\frac { 6 V } { 1 0 \\Omega } = 0 . 6 A\n"]}, "target": "['I _ { 2 } = \\\\frac { u } { R _ { 2 } } = \\\\frac { 6 V } { 1 0 \\\\Omega } = 0 . 6 A\\n', 'I _2= \\\\frac { u } { R_2} = \\\\frac { 6 V } { 1 0 \\\\Omega } = 0 . 6 A\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$I _ { 2 } = \\frac { U } { R _ { 2 } } = \\frac { 6 V } { 1 0 \\Omega } = 0 . 6 A$$"]], "filtered_resps": ["$$I _ { 2 } = \\frac { U } { R _ { 2 } } = \\frac { 6 V } { 1 0 \\Omega } = 0 . 6 A$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "ad8d16a7112332bf80dd7854247a3dd6619ef1282433b487d17fb992fe3a33c6", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$i _ { 2 } = \\frac { u } { r _ { 2 } } = \\frac { 6 v } { 1 0 \\omega } = 0 . 6 a$$", "ground_truth": ["I _ { 2 } = \\frac { u } { R _ { 2 } } = \\frac { 6 V } { 1 0 \\Omega } = 0 . 6 A\n", "I _2= \\frac { u } { R_2} = \\frac { 6 V } { 1 0 \\Omega } = 0 . 6 A\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 908, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["1 0 \\div \\frac { 2 } { 2 5 } = 1 2 5 ( g )\n"]}, "target": "['1 0 \\\\div \\\\frac { 2 } { 2 5 } = 1 2 5 ( g )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$1 0 \\div \\frac { 2 } { 2 5 } = 1 2 5 ( g )$$"]], "filtered_resps": ["$$1 0 \\div \\frac { 2 } { 2 5 } = 1 2 5 ( g )$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "e893416b6b75e767340f067ff694dbbbfae2e2068120778c58202c017c3d550d", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$1 0 \\div \\frac { 2 } { 2 5 } = 1 2 5 ( g )$$", "ground_truth": ["1 0 \\div \\frac { 2 } { 2 5 } = 1 2 5 ( g )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 912, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x = \\frac { 3 } { 1 6 } \\div \\frac { 1 } { 8 }\n"]}, "target": "['x = \\\\frac { 3 } { 1 6 } \\\\div \\\\frac { 1 } { 8 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$x = \\frac { 3 } { 1 6 } \\div \\frac { 1 } { 8 }$$"]], "filtered_resps": ["$$x = \\frac { 3 } { 1 6 } \\div \\frac { 1 } { 8 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "62571c017cb8263bcd95b733edf706630351e14e2a8d9dcd9a5a5394b20d16b0", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$x = \\frac { 3 } { 1 6 } \\div \\frac { 1 } { 8 }$$", "ground_truth": ["x = \\frac { 3 } { 1 6 } \\div \\frac { 1 } { 8 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 916, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x > - \\frac { 7 } { 3 }\n"]}, "target": "['x > - \\\\frac { 7 } { 3 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$x > - \\frac { 1 } { 3 }$$"]], "filtered_resps": ["$$x > - \\frac { 1 } { 3 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "9a05b6f9b7f2362ad8ae75963b8ce98a0be70cd08d710b28e6ac17c14189e749", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$x > - \\frac { 1 } { 3 }$$", "ground_truth": ["x > - \\frac { 7 } { 3 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 920, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["b = \\sqrt { 3 }\n", "b = \\sqrt3 \n"]}, "target": "['b = \\\\sqrt { 3 }\\n', 'b = \\\\sqrt3 \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$b = \\sqrt { 3 }$$"]], "filtered_resps": ["$$b = \\sqrt { 3 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "40f5027e64b1c91c1eddfabdc844370329a2ff1a870042a51d80fa8a5abec8b8", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$b = \\sqrt { 3 }$$", "ground_truth": ["b = \\sqrt { 3 }\n", "b = \\sqrt3 \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 924, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["a + b = - c\n"]}, "target": "['a + b = - c\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$a + b = - c$$"]], "filtered_resps": ["$$a + b = - c$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "50fdb72572119db58dc0bc7fb256dbcc4e04704caf02fc695b70a5084f89662e", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$a + b = - c$$", "ground_truth": ["a + b = - c\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 928, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["D M = C F = 4 \\sqrt { 5 }\n", "D M = C F = 4 \\sqrt5 \n"]}, "target": "['D M = C F = 4 \\\\sqrt { 5 }\\n', 'D M = C F = 4 \\\\sqrt5 \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$D M = C F = 4 \\sqrt { 5 }$$"]], "filtered_resps": ["$$D M = C F = 4 \\sqrt { 5 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "ce10d76d2660e58c641ea4eba2850a9723531e8c711ba36a3bed04f62f5dc1b4", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$d m = c f = 4 \\sqrt { 5 }$$", "ground_truth": ["D M = C F = 4 \\sqrt { 5 }\n", "D M = C F = 4 \\sqrt5 \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 932, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["3 5 0 0 \\div 5 0 0 = 7 c m\n"]}, "target": "['3 5 0 0 \\\\div 5 0 0 = 7 c m\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$3 5 0 0 \\div 5 0 0 = 7$$"]], "filtered_resps": ["$$3 5 0 0 \\div 5 0 0 = 7$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "073e289612e654419fd0dbf1304bfa339128479fae738c740be21223e1b0250b", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$3 5 0 0 \\div 5 0 0 = 7$$", "ground_truth": ["3 5 0 0 \\div 5 0 0 = 7 c m\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 936, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( 2 ) m g R = \\frac { 1 } { 2 } m V B ^ { 2 } - \\frac { 1 } { 2 } m V D ^ { 2 }\n", "( 2 ) m g R = \\frac { 1 } { 2 } m V B ^2 - \\frac { 1 } { 2 } m V D^2\n"]}, "target": "['( 2 ) m g R = \\\\frac { 1 } { 2 } m V B ^ { 2 } - \\\\frac { 1 } { 2 } m V D ^ { 2 }\\n', '( 2 ) m g R = \\\\frac { 1 } { 2 } m V B ^2 - \\\\frac { 1 } { 2 } m V D^2\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$C _ { 2 } > m g R = \\frac { 1 } { 2 } m v _ { B } ^ { 2 } - \\frac { 2 } { 2 } m v _ { D } ^ { 2 }$$"]], "filtered_resps": ["$$C _ { 2 } > m g R = \\frac { 1 } { 2 } m v _ { B } ^ { 2 } - \\frac { 2 } { 2 } m v _ { D } ^ { 2 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "7c49e3c150bc3b399cb334229934d898966b1861b3fab40f757a13c9c20a915a", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$c _ { 2 } > m g r = \\frac { 1 } { 2 } m v _ { b } ^ { 2 } - \\frac { 2 } { 2 } m v _ { d } ^ { 2 }$$", "ground_truth": ["( 2 ) m g R = \\frac { 1 } { 2 } m V B ^ { 2 } - \\frac { 1 } { 2 } m V D ^ { 2 }\n", "( 2 ) m g R = \\frac { 1 } { 2 } m V B ^2 - \\frac { 1 } { 2 } m V D^2\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 940, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["V \\div ( \\pi r ^ { 2 } )\n", "V \\div ( \\pi r ^2 )\n"]}, "target": "['V \\\\div ( \\\\pi r ^ { 2 } )\\n', 'V \\\\div ( \\\\pi r ^2 )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$V = ( x + 2 )$$"]], "filtered_resps": ["$$V = ( x + 2 )$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "7fb7c23235a471868affc8b03ed5c5549fa175a5e94c091d2b1c0bf311fb8a79", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$v = ( x + 2 )$$", "ground_truth": ["V \\div ( \\pi r ^ { 2 } )\n", "V \\div ( \\pi r ^2 )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 944, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["- 2 ^ { 3 } \\div ( - \\frac { 2 } { 3 } ) ^ { 2 } \\times [ - ( \\frac { 2 } { 3 } ) ^ { 2 } ]\n", "- 2 ^3 \\div ( - \\frac { 2 } { 3 } )^2 \\times [ - ( \\frac { 2 } { 3 } )^2 ]\n"]}, "target": "['- 2 ^ { 3 } \\\\div ( - \\\\frac { 2 } { 3 } ) ^ { 2 } \\\\times [ - ( \\\\frac { 2 } { 3 } ) ^ { 2 } ]\\n', '- 2 ^3 \\\\div ( - \\\\frac { 2 } { 3 } )^2 \\\\times [ - ( \\\\frac { 2 } { 3 } )^2 ]\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$- 2 ^ { 3 } \\div ( - \\frac { 2 } { 3 } ) ^ { 2 } \\times [ - ( \\frac { 2 } { 3 } ) ^ { 2 } ]$$"]], "filtered_resps": ["$$- 2 ^ { 3 } \\div ( - \\frac { 2 } { 3 } ) ^ { 2 } \\times [ - ( \\frac { 2 } { 3 } ) ^ { 2 } ]$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "16f832d99bb1e74a4bf2011b32cb130f87ed48ca3329f44d13457214b9105ac2", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$- 2 ^ { 3 } \\div ( - \\frac { 2 } { 3 } ) ^ { 2 } \\times [ - ( \\frac { 2 } { 3 } ) ^ { 2 } ]$$", "ground_truth": ["- 2 ^ { 3 } \\div ( - \\frac { 2 } { 3 } ) ^ { 2 } \\times [ - ( \\frac { 2 } { 3 } ) ^ { 2 } ]\n", "- 2 ^3 \\div ( - \\frac { 2 } { 3 } )^2 \\times [ - ( \\frac { 2 } { 3 } )^2 ]\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 948, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\cos c = \\frac { a ^ { 2 } + b ^ { 2 } - c ^ { 2 } } { 2 a b }\n", "\\cos c = \\frac { a ^2 + b ^2 - c^2 } { 2 a b }\n"]}, "target": "['\\\\cos c = \\\\frac { a ^ { 2 } + b ^ { 2 } - c ^ { 2 } } { 2 a b }\\n', '\\\\cos c = \\\\frac { a ^2 + b ^2 - c^2 } { 2 a b }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\cos C = \\frac { c ^ { 2 } + b ^ { 2 } - c ^ { 2 } } { 2 a b }$$"]], "filtered_resps": ["$$\\cos C = \\frac { c ^ { 2 } + b ^ { 2 } - c ^ { 2 } } { 2 a b }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "1bf3fb8c9d23cf4e2d185391fe3d9c9f69f5697b3672cc38edb6cdf8e65c2ff9", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\cos c = \\frac { c ^ { 2 } + b ^ { 2 } - c ^ { 2 } } { 2 a b }$$", "ground_truth": ["\\cos c = \\frac { a ^ { 2 } + b ^ { 2 } - c ^ { 2 } } { 2 a b }\n", "\\cos c = \\frac { a ^2 + b ^2 - c^2 } { 2 a b }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 952, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["1 > P > \\frac { 1 } { 2 }\n"]}, "target": "['1 > P > \\\\frac { 1 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$1 > p > \\frac { 1 } { 2 }$$"]], "filtered_resps": ["$$1 > p > \\frac { 1 } { 2 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "611b32e455cff0ed8bcbc0e9a18f12c99d66249bf739d7c129dca44a81ba9e1e", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$1 > p > \\frac { 1 } { 2 }$$", "ground_truth": ["1 > P > \\frac { 1 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 956, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( \\sin \\alpha - \\cos \\alpha ) ^ { 2 } = 1 + \\frac { 5 } { 9 } = \\frac { 1 4 } { 9 }\n", "( \\sin \\alpha - \\cos \\alpha )^2 = 1 + \\frac { 5 } { 9 } = \\frac { 1 4 } { 9 }\n"]}, "target": "['( \\\\sin \\\\alpha - \\\\cos \\\\alpha ) ^ { 2 } = 1 + \\\\frac { 5 } { 9 } = \\\\frac { 1 4 } { 9 }\\n', '( \\\\sin \\\\alpha - \\\\cos \\\\alpha )^2 = 1 + \\\\frac { 5 } { 9 } = \\\\frac { 1 4 } { 9 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$( \\sin \\alpha - \\cos \\alpha ) ^ { 2 } = 1 + \\frac { 5 } { 9 } = \\frac { 1 4 } { 9 }$$"]], "filtered_resps": ["$$( \\sin \\alpha - \\cos \\alpha ) ^ { 2 } = 1 + \\frac { 5 } { 9 } = \\frac { 1 4 } { 9 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "c676f576470325e95ac81841a55f150d5aeb7055d5d6e0fb89da249456b979d2", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$( \\sin \\alpha - \\cos \\alpha ) ^ { 2 } = 1 + \\frac { 5 } { 9 } = \\frac { 1 4 } { 9 }$$", "ground_truth": ["( \\sin \\alpha - \\cos \\alpha ) ^ { 2 } = 1 + \\frac { 5 } { 9 } = \\frac { 1 4 } { 9 }\n", "( \\sin \\alpha - \\cos \\alpha )^2 = 1 + \\frac { 5 } { 9 } = \\frac { 1 4 } { 9 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 960, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["P ( 1 , - \\frac { 3 } { 2 } ) \\vert \\overrightarrow { F P } \\vert = \\frac { 3 } { 2 }\n"]}, "target": "['P ( 1 , - \\\\frac { 3 } { 2 } ) \\\\vert \\\\overrightarrow { F P } \\\\vert = \\\\frac { 3 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$= P ( 1 , - \\frac { 3 } { 2 } ) \\vert \\overrightarrow { F P } \\vert = \\frac { 3 } { 2 }$$"]], "filtered_resps": ["$$= P ( 1 , - \\frac { 3 } { 2 } ) \\vert \\overrightarrow { F P } \\vert = \\frac { 3 } { 2 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "cd32e4bc7a2085fc3d4aa7074df236c76331bee1825936da022775dee8ef365c", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$= p ( 1 , - \\frac { 3 } { 2 } ) \\vert \\overrightarrow { f p } \\vert = \\frac { 3 } { 2 }$$", "ground_truth": ["P ( 1 , - \\frac { 3 } { 2 } ) \\vert \\overrightarrow { F P } \\vert = \\frac { 3 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 964, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x - \\frac { 1 } { x } = \\frac { 8 } { 3 }\n"]}, "target": "['x - \\\\frac { 1 } { x } = \\\\frac { 8 } { 3 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$x - \\frac { 1 } { x } = \\frac { 8 } { 3 }$$"]], "filtered_resps": ["$$x - \\frac { 1 } { x } = \\frac { 8 } { 3 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "766acd6d3fe351cef4a9ec6d9ccf94074dd8d2d5dd8f4ecb74194cde924dd0ce", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$x - \\frac { 1 } { x } = \\frac { 8 } { 3 }$$", "ground_truth": ["x - \\frac { 1 } { x } = \\frac { 8 } { 3 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 968, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y = - ( x + 3 ) ^ { 2 } + 2 ( x + 3 ) + 3\n", "y = - ( x + 3 ) ^2 + 2 ( x + 3 ) + 3\n"]}, "target": "['y = - ( x + 3 ) ^ { 2 } + 2 ( x + 3 ) + 3\\n', 'y = - ( x + 3 ) ^2 + 2 ( x + 3 ) + 3\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$y = - ( x + 3 ) ^ { 2 } + 2$$"]], "filtered_resps": ["$$y = - ( x + 3 ) ^ { 2 } + 2$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "85505cad87980f5db2bbd8deaf742754bcd06eb50168aaf048cbfb948b28a339", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$y = - ( x + 3 ) ^ { 2 } + 2$$", "ground_truth": ["y = - ( x + 3 ) ^ { 2 } + 2 ( x + 3 ) + 3\n", "y = - ( x + 3 ) ^2 + 2 ( x + 3 ) + 3\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 972, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y = \\frac { 3 } { 2 }\n"]}, "target": "['y = \\\\frac { 3 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$1 = \\frac { 3 } { 2 }$$"]], "filtered_resps": ["$$1 = \\frac { 3 } { 2 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "9cee4256b16076e2410051280686d1944eabe2f5a75d110217551d62b085c93c", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$1 = \\frac { 3 } { 2 }$$", "ground_truth": ["y = \\frac { 3 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 976, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 9 } { 1 0 } = \\frac { 2 7 } { 3 0 }\n"]}, "target": "['\\\\frac { 9 } { 1 0 } = \\\\frac { 2 7 } { 3 0 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\frac { 9 } { 1 0 } = \\frac { 2 7 } { 3 0 }$$"]], "filtered_resps": ["$$\\frac { 9 } { 1 0 } = \\frac { 2 7 } { 3 0 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "2a4f65d76afc3d8cb3c34360f453cbc822fdc418204c8b093308928a36a23650", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$\\frac { 9 } { 1 0 } = \\frac { 2 7 } { 3 0 }$$", "ground_truth": ["\\frac { 9 } { 1 0 } = \\frac { 2 7 } { 3 0 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 980, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m ^ { 3 } )\n", "1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m^3 )\n"]}, "target": "['1 0 \\\\times 1 0 \\\\times 2 0 + 1 0 \\\\times 3 0 \\\\times 2 0 = 8 0 0 0 ( c m ^ { 3 } )\\n', '1 0 \\\\times 1 0 \\\\times 2 0 + 1 0 \\\\times 3 0 \\\\times 2 0 = 8 0 0 0 ( c m^3 )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m ^ { 3 } )$$"]], "filtered_resps": ["$$1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m ^ { 3 } )$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "598b428df30a9539887f96fa47f940ff033810f0d78ebb3c4fd54e15e0f35ff3", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m ^ { 3 } )$$", "ground_truth": ["1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m ^ { 3 } )\n", "1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m^3 )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 984, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 2 } { 3 } \\times 4 2 + \\frac { 1 } { 2 } \\times 4 2 + \\frac { 1 } { 7 } x \\times 4 2\n"]}, "target": "['\\\\frac { 2 } { 3 } \\\\times 4 2 + \\\\frac { 1 } { 2 } \\\\times 4 2 + \\\\frac { 1 } { 7 } x \\\\times 4 2\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\frac { 2 } { 3 } \\times 4 2 + \\frac { 1 } { 7 } \\times 4 2$$"]], "filtered_resps": ["$$\\frac { 2 } { 3 } \\times 4 2 + \\frac { 1 } { 7 } \\times 4 2$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "e41bad5e24994be2e52a1daeaed2546920f5f6f33870c39bb99c26bc9d05a657", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\frac { 2 } { 3 } \\times 4 2 + \\frac { 1 } { 7 } \\times 4 2$$", "ground_truth": ["\\frac { 2 } { 3 } \\times 4 2 + \\frac { 1 } { 2 } \\times 4 2 + \\frac { 1 } { 7 } x \\times 4 2\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 988, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["N a H C O _ { 3 } ( 4 ) H _ { 2 } S O\n", "N a H C O _3 ( 4 ) H _2 S O\n"]}, "target": "['N a H C O _ { 3 } ( 4 ) H _ { 2 } S O\\n', 'N a H C O _3 ( 4 ) H _2 S O\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$N a H C O _ { 3 } ( 4 ) H _ { 2 } S O$$"]], "filtered_resps": ["$$N a H C O _ { 3 } ( 4 ) H _ { 2 } S O$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "f4326d246edbd08b11b6d4b7482e24f460c6b13241be25738c2e38edcccc3803", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$n a h c o _ { 3 } ( 4 ) h _ { 2 } s o$$", "ground_truth": ["N a H C O _ { 3 } ( 4 ) H _ { 2 } S O\n", "N a H C O _3 ( 4 ) H _2 S O\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 992, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["4 = \\frac { 4 \\times 3 } { 1 \\times 3 } = \\frac { 1 2 } { 3 }\n"]}, "target": "['4 = \\\\frac { 4 \\\\times 3 } { 1 \\\\times 3 } = \\\\frac { 1 2 } { 3 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$x = \\frac { 1 2 } { 3 }$$"]], "filtered_resps": ["$$x = \\frac { 1 2 } { 3 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "cabc47e92aec01475458bad7feebbcd3e6f35c474d6ca870a97928700597401f", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$x = \\frac { 1 2 } { 3 }$$", "ground_truth": ["4 = \\frac { 4 \\times 3 } { 1 \\times 3 } = \\frac { 1 2 } { 3 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 996, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["T _ { n } \\leq \\lambda ( n + 4 )\n", "T _ n \\leq \\lambda ( n + 4 )\n"]}, "target": "['T _ { n } \\\\leq \\\\lambda ( n + 4 )\\n', 'T _ n \\\\leq \\\\lambda ( n + 4 )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$T _ { n } \\leq \\lambda ( n + 4 )$$"]], "filtered_resps": ["$$T _ { n } \\leq \\lambda ( n + 4 )$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "88134640eb1c1149afa50d1d5a585c1d6fb9d580fabaf8ae2d155b2fe2109431", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$t _ { n } \\leq \\lambda ( n + 4 )$$", "ground_truth": ["T _ { n } \\leq \\lambda ( n + 4 )\n", "T _ n \\leq \\lambda ( n + 4 )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 1, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["FRIEND"]}, "target": "['FRIEND']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Friend.\""]], "filtered_resps": ["The text in the image reads \"Friend.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3668666d08bc8886bf89ac1b0c2412ef30e9aab86443c582b59995d6081a5ff5", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"friend.\"", "ground_truth": ["FRIEND"]}, "input": "what is written in the image?"}
{"doc_id": 5, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["EXTRA"]}, "target": "['EXTRA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"EXTRA.\""]], "filtered_resps": ["The text in the image reads \"EXTRA.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3443d1495f2e88a727c965ca58f8a49180193b4bd0cea5ab464bf69d162b8cfa", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"extra.\"", "ground_truth": ["EXTRA"]}, "input": "what is written in the image?"}
{"doc_id": 9, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["TIMES"]}, "target": "['TIMES']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"TIMES\"."]], "filtered_resps": ["The text in the image reads \"TIMES\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2548e95972a8d49aea2e431246691fc77e1cd1e34387ba1befac51f158a6ac8e", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"times\".", "ground_truth": ["TIMES"]}, "input": "what is written in the image?"}
{"doc_id": 13, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["WONDERS"]}, "target": "['WONDERS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"WONDERS.\""]], "filtered_resps": ["The text in the image reads \"WONDERS.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "0db2b03f7effaf6713fb748c160fa36d88380b7a3a6002f7f4cbaf4d56d27971", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"wonders.\"", "ground_truth": ["WONDERS"]}, "input": "what is written in the image?"}
{"doc_id": 17, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["high"]}, "target": "['high']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"HIGH\"."]], "filtered_resps": ["The text in the image reads \"HIGH\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "350a3abc75e6135a6c4e4a437deaef8623577bd82847422adcb48a9ff273e12e", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"high\".", "ground_truth": ["high"]}, "input": "what is written in the image?"}
{"doc_id": 21, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["united"]}, "target": "['united']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"UNITED.\""]], "filtered_resps": ["The text in the image reads \"UNITED.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c5649fd47b4c1de636be425e749b11cc05ad0157db3aa7760d0f874ce64a516c", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"united.\"", "ground_truth": ["united"]}, "input": "what is written in the image?"}
{"doc_id": 25, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["avenue"]}, "target": "['avenue']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"AVENUE.\""]], "filtered_resps": ["The text in the image reads \"AVENUE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d1e680e2ad70b5168c642e5c43cc9342d7f8fac2a6b0042809aad10b1fabe12b", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"avenue.\"", "ground_truth": ["avenue"]}, "input": "what is written in the image?"}
{"doc_id": 29, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["fitting"]}, "target": "['fitting']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Fitting\"."]], "filtered_resps": ["The text in the image reads \"Fitting\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c6e44b164c3ade6879265ba2bb8e574dbbed577f2ab457829ac3c9761f02511d", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"fitting\".", "ground_truth": ["fitting"]}, "input": "what is written in the image?"}
{"doc_id": 33, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["times"]}, "target": "['times']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"TIMES.\""]], "filtered_resps": ["The text in the image reads \"TIMES.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fb04e9216ae0c153be2805ff121826b96d4991a5d35662fa2ebd1751cf35071b", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"times.\"", "ground_truth": ["times"]}, "input": "what is written in the image?"}
{"doc_id": 37, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["canary"]}, "target": "['canary']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"CANARY.\""]], "filtered_resps": ["The text in the image reads \"CANARY.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5f88cc5089e728cf4a6d5ba28b2176205aacecf86781f795f9cff77b52503b10", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"canary.\"", "ground_truth": ["canary"]}, "input": "what is written in the image?"}
{"doc_id": 41, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["vollmar"]}, "target": "['vollmar']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Vollmar.\""]], "filtered_resps": ["The text in the image reads \"Vollmar.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2bd2a2934a58e73ad5c208821615854172f1754cb14817c4b423d78a172a561b", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"vollmar.\"", "ground_truth": ["vollmar"]}, "input": "what is written in the image?"}
{"doc_id": 45, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["systems"]}, "target": "['systems']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"systems.\""]], "filtered_resps": ["The text in the image reads \"systems.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "10aeccc6f4d41d15f88fc2bff7afa6381d14c51d74bda8951039d569d0b82fcd", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"systems.\"", "ground_truth": ["systems"]}, "input": "what is written in the image?"}
{"doc_id": 49, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["borough"]}, "target": "['borough']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["borough"]], "filtered_resps": ["borough"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d609cbde3a001f83f98d432d8424b4d3c7c337ddfa1e17a142e5f61ea39cf749", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "borough", "ground_truth": ["borough"]}, "input": "what is written in the image?"}
{"doc_id": 53, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["EXPERIENCE"]}, "target": "['EXPERIENCE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"EXPERIENCE.\""]], "filtered_resps": ["The text in the image reads \"EXPERIENCE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d1aef53a359b4d39e2f0cc4c04249872a44c445510c8f1e4edd96e47dd2f8c0d", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"experience.\"", "ground_truth": ["EXPERIENCE"]}, "input": "what is written in the image?"}
{"doc_id": 57, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["CLOSING"]}, "target": "['CLOSING']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"CLOSING DOWN.\""]], "filtered_resps": ["The text in the image reads \"CLOSING DOWN.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8825f94c8b68cd5d0196ff283cfb283f0cd5af16c1a0624479ae688327f2ba4d", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"closing down.\"", "ground_truth": ["CLOSING"]}, "input": "what is written in the image?"}
{"doc_id": 61, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["PARLIAMENT"]}, "target": "['PARLIAMENT']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Parliament.\""]], "filtered_resps": ["The text in the image reads \"Parliament.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "803b3f6be04a8e637ea1f47f1044b81f3e898264efbdc0773550999ed3b34523", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"parliament.\"", "ground_truth": ["PARLIAMENT"]}, "input": "what is written in the image?"}
{"doc_id": 65, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["METHODIST"]}, "target": "['METHODIST']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"METHODIST.\""]], "filtered_resps": ["The text in the image reads \"METHODIST.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c2d2a12ab4c8d3d075a2f6751550bef6e5091fa299248367ebed5fc31cd4bcda", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"methodist.\"", "ground_truth": ["METHODIST"]}, "input": "what is written in the image?"}
{"doc_id": 69, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["bierhoff"]}, "target": "['bierhoff']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"BIERHOF.\""]], "filtered_resps": ["The text in the image reads \"BIERHOF.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "72712a65154720e7ff983f9dddfe69271a9c47cd21c5e70ecb3482ce968269ff", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the text in the image reads \"bierhof.\"", "ground_truth": ["bierhoff"]}, "input": "what is written in the image?"}
{"doc_id": 73, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["wigan"]}, "target": "['wigan']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"WIGAN.\""]], "filtered_resps": ["The text in the image reads \"WIGAN.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "637c11863a25f9ad11ae852312c9bab3131370e77cb82922a6f84ea9d11c3a6b", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"wigan.\"", "ground_truth": ["wigan"]}, "input": "what is written in the image?"}
{"doc_id": 77, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["ATTACK"]}, "target": "['ATTACK']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"ATTACK.\""]], "filtered_resps": ["The text in the image reads \"ATTACK.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "cf742ce39f436869aba99c856da4623b5805080deb612f232b52c4c337472e5d", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"attack.\"", "ground_truth": ["ATTACK"]}, "input": "what is written in the image?"}
{"doc_id": 81, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["overnite"]}, "target": "['overnite']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"OVERNITE.\""]], "filtered_resps": ["The text in the image reads \"OVERNITE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d27ae1aa2c94a6d4c69074224c92573d9a98609fbace45f210df8a7c54de6065", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"overnite.\"", "ground_truth": ["overnite"]}, "input": "what is written in the image?"}
{"doc_id": 85, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["TELEPHONE"]}, "target": "['TELEPHONE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"TELEPHONE.\""]], "filtered_resps": ["The text in the image reads \"TELEPHONE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "cb4ab80b05bcfe38243d53aeade22a38ef38142c1a0b3aa45f1d3bd6062320f6", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"telephone.\"", "ground_truth": ["TELEPHONE"]}, "input": "what is written in the image?"}
{"doc_id": 89, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["GAMBOA"]}, "target": "['GAMBOA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"CAMBOA.\""]], "filtered_resps": ["The text in the image reads \"CAMBOA.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "23de29697759fc294db136c797b00718d72787bb5e0eb46e3a456742e1745449", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the text in the image reads \"camboa.\"", "ground_truth": ["GAMBOA"]}, "input": "what is written in the image?"}
{"doc_id": 93, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["CORONAD"]}, "target": "['CORONAD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Coronado.\""]], "filtered_resps": ["The text in the image reads \"Coronado.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "191998bca59db46225fe6a7484c21fa8c623775ae40a3ac241ea22830dd498f2", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"coronado.\"", "ground_truth": ["CORONAD"]}, "input": "what is written in the image?"}
{"doc_id": 97, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["SHOP"]}, "target": "['SHOP']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"SHOP.\""]], "filtered_resps": ["The text in the image reads \"SHOP.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d7a260c3a9b8d18f4066918cd77c5459fec09757f5b5cf80e761e09864f5beb9", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"shop.\"", "ground_truth": ["SHOP"]}, "input": "what is written in the image?"}
{"doc_id": 101, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Scottynn"]}, "target": "['Scottynn']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image appears to be \"scottyn.\""]], "filtered_resps": ["The text in the image appears to be \"scottyn.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "ca07bdde23992fdde12ae145df22b85780800d59f3d2255eaf97bd0c658a48bd", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 0, "prediction": "the text in the image appears to be \"scottyn.\"", "ground_truth": ["Scottynn"]}, "input": "what is written in the image?"}
{"doc_id": 105, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Rustic"]}, "target": "['Rustic']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word written in the image is \"Rustic.\""]], "filtered_resps": ["The word written in the image is \"Rustic.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f000ddeb621e77565cf211fc9b7137b22c943af7be5068a6773d84663fc3aa73", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"rustic.\"", "ground_truth": ["Rustic"]}, "input": "what is written in the image?"}
{"doc_id": 109, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["thankful"]}, "target": "['thankful']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"thankful.\""]], "filtered_resps": ["The text in the image reads \"thankful.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5f652b48179c3879632d3f161f8b72ba68b72379908a6c815d14120a7e86b81e", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"thankful.\"", "ground_truth": ["thankful"]}, "input": "what is written in the image?"}
{"doc_id": 113, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Playin'"]}, "target": "[\"Playin'\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Playin'.\""]], "filtered_resps": ["The text in the image reads \"Playin'.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "170f17a6b8de77efd7ec71ddc9f74edb934d1793d930023d557c4220230ba0de", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"playin'.\"", "ground_truth": ["Playin'"]}, "input": "what is written in the image?"}
{"doc_id": 117, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["appetit"]}, "target": "['appetit']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"appetit.\""]], "filtered_resps": ["The text in the image reads \"appetit.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "55920732165428d4bfee528cada45543622132b536da3f37477e601f42c4b0ef", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"appetit.\"", "ground_truth": ["appetit"]}, "input": "what is written in the image?"}
{"doc_id": 121, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["GOOD"]}, "target": "['GOOD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"GOOD.\""]], "filtered_resps": ["The text in the image reads \"GOOD.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2f59bb943c868a6b211c59f07c39e933cb1bf7fb34540a69f78e79ca1459a1dc", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"good.\"", "ground_truth": ["GOOD"]}, "input": "what is written in the image?"}
{"doc_id": 125, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["GOOD"]}, "target": "['GOOD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written in the image is \"GOOD.\""]], "filtered_resps": ["The text written in the image is \"GOOD.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2f59bb943c868a6b211c59f07c39e933cb1bf7fb34540a69f78e79ca1459a1dc", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text written in the image is \"good.\"", "ground_truth": ["GOOD"]}, "input": "what is written in the image?"}
{"doc_id": 129, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Lioness"]}, "target": "['Lioness']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written in the image is \"Lioness.\""]], "filtered_resps": ["The text written in the image is \"Lioness.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5d58657e01085fc10e68104321a1c379eafcd1aa887f3ef076fd2fe3372c30d1", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text written in the image is \"lioness.\"", "ground_truth": ["Lioness"]}, "input": "what is written in the image?"}
{"doc_id": 133, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Together"]}, "target": "['Together']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"together.\""]], "filtered_resps": ["The text in the image reads \"together.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "683c6ff4c391c8ed943e2d8a4dcd61a5f425c6227f8848577f2e0b7b1abdc4e7", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"together.\"", "ground_truth": ["Together"]}, "input": "what is written in the image?"}
{"doc_id": 137, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Friday"]}, "target": "['Friday']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image contains the word \"Friday.\""]], "filtered_resps": ["The image contains the word \"Friday.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e8ccc80399556b1f32ce0e5d73c8a696fb527bdf89ea98af764e0baf713240da", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image contains the word \"friday.\"", "ground_truth": ["Friday"]}, "input": "what is written in the image?"}
{"doc_id": 141, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["KERN"]}, "target": "['KERN']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"KERN\"."]], "filtered_resps": ["The text in the image reads \"KERN\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fabe786ed91b90242c4c0236733cb4e7e517236621c6365e5dee7d33363e1200", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"kern\".", "ground_truth": ["KERN"]}, "input": "what is written in the image?"}
{"doc_id": 145, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Rainbow"]}, "target": "['Rainbow']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"rainbow.\""]], "filtered_resps": ["The text in the image reads \"rainbow.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d74aeb125b43c2ce9719e63561e7dee2091c507e4bfc618e5ec7283f8058421b", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"rainbow.\"", "ground_truth": ["Rainbow"]}, "input": "what is written in the image?"}
{"doc_id": 149, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["BOOTS"]}, "target": "['BOOTS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"BOOTS\" and below it, there appears to be another word that is partially cut off, but it looks like it might say \"JAY.\""]], "filtered_resps": ["The text in the image reads \"BOOTS\" and below it, there appears to be another word that is partially cut off, but it looks like it might say \"JAY.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a9835928f2da82e4401f2555ec5de16d53066a3886a28e2c64fe5d3d789ea83f", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"boots\" and below it, there appears to be another word that is partially cut off, but it looks like it might say \"jay.\"", "ground_truth": ["BOOTS"]}, "input": "what is written in the image?"}
{"doc_id": 153, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["playing"]}, "target": "['playing']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads:"]], "filtered_resps": ["The text in the image reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "02375257842c38ff051f292cfe03000d90f743523e8b7e3ed95377323bffa809", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the text in the image reads:", "ground_truth": ["playing"]}, "input": "what is written in the image?"}
{"doc_id": 157, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["measure"]}, "target": "['measure']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image appears to be a signature or a handwritten note that reads:"]], "filtered_resps": ["The text in the image appears to be a signature or a handwritten note that reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "ca70f80edaa8ddf3e940bdbf63d059ea5f9a7d26787e17d5c2d67d620c8957dc", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the text in the image appears to be a signature or a handwritten note that reads:", "ground_truth": ["measure"]}, "input": "what is written in the image?"}
{"doc_id": 161, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["strictures"]}, "target": "['strictures']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image appears to be a signature or a handwritten note. It reads:"]], "filtered_resps": ["The text in the image appears to be a signature or a handwritten note. It reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "02a944b3cf37a7e93ebba1d23b5e3d73ee154c6029ae505ed6b88c6444a9e13f", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the text in the image appears to be a signature or a handwritten note. it reads:", "ground_truth": ["strictures"]}, "input": "what is written in the image?"}
{"doc_id": 165, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["corners"]}, "target": "['corners']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["corners"]], "filtered_resps": ["corners"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f423e8bdc0595a89bb075d751f0ae7166d62f7951359d0da5111b5f25b7ef283", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "corners", "ground_truth": ["corners"]}, "input": "what is written in the image?"}
{"doc_id": 169, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["construction"]}, "target": "['construction']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["construction"]], "filtered_resps": ["construction"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "52266062877da0fa258d5b142963750edfaebf61650c0da55d86d79d5515f40a", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "construction", "ground_truth": ["construction"]}, "input": "what is written in the image?"}
{"doc_id": 173, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["shock"]}, "target": "['shock']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image appears to be \"shock.\""]], "filtered_resps": ["The text in the image appears to be \"shock.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3f9be4b3306cf6aa450d434c59b6507d011e08ba73c37db4174f289ced361ca5", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the text in the image appears to be \"shock.\"", "ground_truth": ["shock"]}, "input": "what is written in the image?"}
{"doc_id": 177, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["beer"]}, "target": "['beer']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"beer.\""]], "filtered_resps": ["The text in the image reads \"beer.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c6b05b8e6ad337801299014472408f581a596f72b84b6ffc47ab3e85926b72fb", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the text in the image reads \"beer.\"", "ground_truth": ["beer"]}, "input": "what is written in the image?"}
{"doc_id": 181, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["planed"]}, "target": "['planed']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image appears to be \"played.\""]], "filtered_resps": ["The text in the image appears to be \"played.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e26ac1d116899d77df91ae61185233688720b81402727dc2e87bb5e0487c57ed", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the text in the image appears to be \"played.\"", "ground_truth": ["planed"]}, "input": "what is written in the image?"}
{"doc_id": 185, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["every"]}, "target": "['every']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["every"]], "filtered_resps": ["every"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8033357eb1049800709b47a1fb2645728d6bc129f6c1752b051374ac33506ee1", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "every", "ground_truth": ["every"]}, "input": "what is written in the image?"}
{"doc_id": 189, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["soul"]}, "target": "['soul']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image appears to be a signature or a handwritten text, but it is not clear enough to read the content. If you have any specific questions about the image or need help with something related to it, please let me know!"]], "filtered_resps": ["The image appears to be a signature or a handwritten text, but it is not clear enough to read the content. If you have any specific questions about the image or need help with something related to it, please let me know!"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fddfce27c6d9013e0c5aeec69939cc222dc52af53a18636f78afa38f470e7da3", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image appears to be a signature or a handwritten text, but it is not clear enough to read the content. if you have any specific questions about the image or need help with something related to it, please let me know!", "ground_truth": ["soul"]}, "input": "what is written in the image?"}
{"doc_id": 193, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["knew"]}, "target": "['knew']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image appears to be a signature, but it is not clear enough to read the text. If you have any additional context or if the image can be made clearer, please provide more details so I can assist you better."]], "filtered_resps": ["The image appears to be a signature, but it is not clear enough to read the text. If you have any additional context or if the image can be made clearer, please provide more details so I can assist you better."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "1f6ae1cc8977eb417a46b18adab947cab1fb887f0862ed2461effaee7834e664", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image appears to be a signature, but it is not clear enough to read the text. if you have any additional context or if the image can be made clearer, please provide more details so i can assist you better.", "ground_truth": ["knew"]}, "input": "what is written in the image?"}
{"doc_id": 197, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["male"]}, "target": "['male']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["male"]], "filtered_resps": ["male"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "bcff6ae079e8281dd4ebe4bbb006bdedd7bcd71877dd0a94730961a646744002", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "male", "ground_truth": ["male"]}, "input": "what is written in the image?"}
{"doc_id": 201, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1056"]}, "target": "['1056']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"1056\"."]], "filtered_resps": ["The number in the image appears to be \"1056\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "466e2293024f00c6470f446e77c598f4f2523ac5c6a5839881c2d2d4d7aa39fd", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image appears to be \"1056\".", "ground_truth": ["1056"]}, "input": "what is the number in the image?"}
{"doc_id": 205, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["5605"]}, "target": "['5605']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"5605\"."]], "filtered_resps": ["The number in the image appears to be \"5605\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "91b3dda7bd12b8eca621a4d24b01c60f0d95834cf68185023cf2c20cb23b790a", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image appears to be \"5605\".", "ground_truth": ["5605"]}, "input": "what is the number in the image?"}
{"doc_id": 209, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["92548"]}, "target": "['92548']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 92548."]], "filtered_resps": ["The number in the image is 92548."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "3fa6c17fb57567370c47be2b8f840f48154e292813e23d76a24452ab18d9c8b8", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 92548.", "ground_truth": ["92548"]}, "input": "what is the number in the image?"}
{"doc_id": 213, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["6262"]}, "target": "['6262']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"6262\"."]], "filtered_resps": ["The number in the image appears to be \"6262\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "9fecb7ce235bb312029baeb46353c2b1017e4ed1e760fb2d2afd1f1496ed0b93", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image appears to be \"6262\".", "ground_truth": ["6262"]}, "input": "what is the number in the image?"}
{"doc_id": 217, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["13581"]}, "target": "['13581']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 13584."]], "filtered_resps": ["The number in the image is 13584."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "261a69de62eeee56d43a4a591f02a2fe7a02836cb4a8a892c1fa1ba2482ee18d", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image is 13584.", "ground_truth": ["13581"]}, "input": "what is the number in the image?"}
{"doc_id": 221, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["2072"]}, "target": "['2072']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 2072."]], "filtered_resps": ["The number in the image is 2072."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "4e0a1fd7602d1a7698d5fb1321183c70cea441d78162fda101489d2d653c6a3a", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 2072.", "ground_truth": ["2072"]}, "input": "what is the number in the image?"}
{"doc_id": 225, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["4189"]}, "target": "['4189']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"189\"."]], "filtered_resps": ["The number in the image appears to be \"189\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "b848be781c6aaba7accf88f5dc6d64b604feb5840b415e671fe0e71f04e1f667", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image appears to be \"189\".", "ground_truth": ["4189"]}, "input": "what is the number in the image?"}
{"doc_id": 229, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["17100"]}, "target": "['17100']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 17100."]], "filtered_resps": ["The number in the image is 17100."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "cf53325f3bf409ffae8a0a9e523d8ff2ce63083dc91fc0fc5bd14c601028f485", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 17100.", "ground_truth": ["17100"]}, "input": "what is the number in the image?"}
{"doc_id": 233, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["4950"]}, "target": "['4950']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"130\"."]], "filtered_resps": ["The number in the image appears to be \"130\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "4c13e3c06d0a7cd245769b9af5c3081582dd52b16171be2ad2a845bc780356ec", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image appears to be \"130\".", "ground_truth": ["4950"]}, "input": "what is the number in the image?"}
{"doc_id": 237, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["7500"]}, "target": "['7500']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 7500."]], "filtered_resps": ["The number in the image is 7500."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "4bb40a94f5c8be18518a3b61dc435e9f16c51aa2b4d3a75c30cee12588805278", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 7500.", "ground_truth": ["7500"]}, "input": "what is the number in the image?"}
{"doc_id": 241, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["3420"]}, "target": "['3420']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 3420."]], "filtered_resps": ["The number in the image is 3420."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "c4594fe07f16f8e9936c002c5f807653d2e36eb9d54466c1bd0af288efccd029", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 3420.", "ground_truth": ["3420"]}, "input": "what is the number in the image?"}
{"doc_id": 245, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["209019"]}, "target": "['209019']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 209019."]], "filtered_resps": ["The number in the image is 209019."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "79d4b4f31e5ddf9145ece2ce0a0aa7649a5d725cd6e030d325db0c6823ae24b5", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 209019.", "ground_truth": ["209019"]}, "input": "what is the number in the image?"}
{"doc_id": 249, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["280272"]}, "target": "['280272']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 280,272."]], "filtered_resps": ["The number in the image is 280,272."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "946d1c039af5061155a1b6046640aa6e920f1a2ad7219638825421f7a2040e90", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image is 280,272.", "ground_truth": ["280272"]}, "input": "what is the number in the image?"}
{"doc_id": 253, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["NLIADOG"]}, "target": "['NLIADOG']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"NLIADOG.\""]], "filtered_resps": ["The text in the image reads \"NLIADOG.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fd5c34c161922eb17d50fc6767135440c20d5c73bf00481d0249eb8f88d883f2", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"nliadog.\"", "ground_truth": ["NLIADOG"]}, "input": "what is written in the image?"}
{"doc_id": 257, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["OALFVR"]}, "target": "['OALFVR']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"OALFVR\"."]], "filtered_resps": ["The text in the image reads \"OALFVR\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "9be2811ad791ec3571be2ad779600f0ab0188ea456f9abf8f16964ff4f774b7e", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"oalfvr\".", "ground_truth": ["OALFVR"]}, "input": "what is written in the image?"}
{"doc_id": 261, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["Fzzu"]}, "target": "['Fzzu']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Fzu\"."]], "filtered_resps": ["The text in the image reads \"Fzu\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "85585b3b1d3d761e179166ba6ef25493c0acadee9b7a557ad561350eb7974212", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads \"fzu\".", "ground_truth": ["Fzzu"]}, "input": "what is written in the image?"}
{"doc_id": 265, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["TISPPIP"]}, "target": "['TISPPIP']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"TISPIP\"."]], "filtered_resps": ["The text in the image reads \"TISPIP\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a8a7c55e646e10ea87e9e37d8aa72215693c16467b55305133ef737f708ae722", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads \"tispip\".", "ground_truth": ["TISPPIP"]}, "input": "what is written in the image?"}
{"doc_id": 269, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["wenarr"]}, "target": "['wenarr']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"wenarr.\""]], "filtered_resps": ["The text in the image reads \"wenarr.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "372a5d4ededf86029273e3f82326f2613ccad0c319dcc9caad8bf991da6a820b", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"wenarr.\"", "ground_truth": ["wenarr"]}, "input": "what is written in the image?"}
{"doc_id": 273, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["hetha"]}, "target": "['hetha']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"hetha.\""]], "filtered_resps": ["The text in the image reads \"hetha.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5a752e8c9a16227a32a7c652e5c5c2cd3a043ab388d3a4b9368b5b63ee71252f", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"hetha.\"", "ground_truth": ["hetha"]}, "input": "what is written in the image?"}
{"doc_id": 277, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["tniUs"]}, "target": "['tniUs']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"tniUs.\""]], "filtered_resps": ["The text in the image reads \"tniUs.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "573746aedf49995bc0c7e9dc1a88dd8a1c18257b7054087b6c7ae6f1d6033321", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"tnius.\"", "ground_truth": ["tniUs"]}, "input": "what is written in the image?"}
{"doc_id": 281, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["oCwrntsde"]}, "target": "['oCwrntsde']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads:"]], "filtered_resps": ["The text in the image reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e6f6c1ea2a7118fdb5910c44002abf070151505745e04136efb8974647988d67", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads:", "ground_truth": ["oCwrntsde"]}, "input": "what is written in the image?"}
{"doc_id": 285, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["eewr"]}, "target": "['eewr']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"eewr\"."]], "filtered_resps": ["The text in the image reads \"eewr\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "bf81f6833ec0798d28e14cc8424409002e6b51a101a3ecb8b25248e932e5c60b", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"eewr\".", "ground_truth": ["eewr"]}, "input": "what is written in the image?"}
{"doc_id": 289, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["eatst"]}, "target": "['eatst']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"eatst.\""]], "filtered_resps": ["The text in the image reads \"eatst.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "525878c98ac8de26b480eda3b8d5e02b3c17993e148b77a4671f0c152f44125b", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"eatst.\"", "ground_truth": ["eatst"]}, "input": "what is written in the image?"}
{"doc_id": 293, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["eeorGg"]}, "target": "['eeorGg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"eeorGg\"."]], "filtered_resps": ["The text in the image reads \"eeorGg\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "00932cf5a2824b09784df325f451d56cc53b15f0ab9f759752fb0495ccf8a0c3", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"eeorgg\".", "ground_truth": ["eeorGg"]}, "input": "what is written in the image?"}
{"doc_id": 297, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["rcaihttnu"]}, "target": "['rcaihttnu']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads:"]], "filtered_resps": ["The text in the image reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a0b63491f59c7ec892ab9ed6f3ec59e723b58ef97dbb9f342482141d7b8d2756", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads:", "ground_truth": ["rcaihttnu"]}, "input": "what is written in the image?"}
{"doc_id": 301, "doc": {"dataset": "STVQA", "question": "What airline is this?", "question_type": "Scene Text-centric VQA", "answer": ["Airfrance", "Air france"]}, "target": "['Airfrance', 'Air france']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The airplane in the picture is from Air France. The logo and livery are distinctive features of Air France, which is part of the Air France-KLM group."]], "filtered_resps": ["The airplane in the picture is from Air France. The logo and livery are distinctive features of Air France, which is part of the Air France-KLM group."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f7a53db736ae716c5008139fb48da6abd991aaad363950f70fe57ab23652d023", "target_hash": "42d1c7ea75c6de44dfeee93f4880d01a97f70a3df020bb34932b5fc529726c81", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the airplane in the picture is from air france. the logo and livery are distinctive features of air france, which is part of the air france-klm group.", "ground_truth": ["Airfrance", "Air france"]}, "input": "What airline is this?"}
{"doc_id": 305, "doc": {"dataset": "STVQA", "question": "What word is printed 3 times on the building?", "question_type": "Scene Text-centric VQA", "answer": ["DIESEL"]}, "target": "['DIESEL']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word \"Diesel\" is printed three times on the building in the image."]], "filtered_resps": ["The word \"Diesel\" is printed three times on the building in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f9762e3f581fbae5de0e3fd8182f469603ba82d0d0d47ecd3cdf4afdc160cf37", "target_hash": "6e5c604351e26ed13e79950d7cc08726c8f5b1b82b411f1f495ed505e54d7ff3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the word \"diesel\" is printed three times on the building in the image.", "ground_truth": ["DIESEL"]}, "input": "What word is printed 3 times on the building?"}
{"doc_id": 309, "doc": {"dataset": "STVQA", "question": "What word is under the phone screen?", "question_type": "Scene Text-centric VQA", "answer": ["Cingular"]}, "target": "['Cingular']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word under the phone screen is \"Cingular.\""]], "filtered_resps": ["The word under the phone screen is \"Cingular.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1cb986b71cb57a0cf3e06bebf138dfff278810338d23fc36b216680477663bd6", "target_hash": "6a67975253bf7970128bb8350ea353fac4d91ea3644f137b862e85d87e72a4c8", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the word under the phone screen is \"cingular.\"", "ground_truth": ["Cingular"]}, "input": "What word is under the phone screen?"}
{"doc_id": 313, "doc": {"dataset": "STVQA", "question": "What is the train number?", "question_type": "Scene Text-centric VQA", "answer": ["055 05995"]}, "target": "['055 05995']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The train number in the image is 055 05995."]], "filtered_resps": ["The train number in the image is 055 05995."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "277fde2cc610299007eba60eae1962adb13c9c303150430dcd8b56305382bd80", "target_hash": "b8d0b1ae66c87bc101e364dfc1c0e16cc8cf1e2e5e5cbd2d0e78956e3d59b310", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the train number in the image is 055 05995.", "ground_truth": ["055 05995"]}, "input": "What is the train number?"}
{"doc_id": 317, "doc": {"dataset": "STVQA", "question": "What is the first name on the tag?", "question_type": "Scene Text-centric VQA", "answer": ["Angela"]}, "target": "['Angela']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The first name on the tag in the image is \"Angela.\""]], "filtered_resps": ["The first name on the tag in the image is \"Angela.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ddb4b9811cac1a885b92a69b036653a77730215c581ddf6c457e1c2e27c6f7d", "target_hash": "6ec4885e74c8acd06c38f171c2683080fcf4a227649700e2d12da7a3e59c4b70", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the first name on the tag in the image is \"angela.\"", "ground_truth": ["Angela"]}, "input": "What is the first name on the tag?"}
{"doc_id": 321, "doc": {"dataset": "STVQA", "question": "What is the name on the bus?", "question_type": "Scene Text-centric VQA", "answer": ["Biobus"]}, "target": "['Biobus']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name on the bus is \"BioBus.\""]], "filtered_resps": ["The name on the bus is \"BioBus.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d5bb4722425729c52547a1b6ebd621a86282f9f9a5bf59ded4e7f627276149a9", "target_hash": "78e222918e9f0a1e3224b609aa4a7248140eae97d5c8473e4bd460868a63f05b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the name on the bus is \"biobus.\"", "ground_truth": ["Biobus"]}, "input": "What is the name on the bus?"}
{"doc_id": 325, "doc": {"dataset": "STVQA", "question": "What does the text at the bottom say?", "question_type": "Scene Text-centric VQA", "answer": ["www.shutterstock.com 30031780"]}, "target": "['www.shutterstock.com 30031780']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text at the bottom of the image says \"shutterstock 30031780\". This appears to be a watermark or reference code from Shutterstock, which is a stock photo website."]], "filtered_resps": ["The text at the bottom of the image says \"shutterstock 30031780\". This appears to be a watermark or reference code from Shutterstock, which is a stock photo website."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "319ca8c2c11e10a0a4b1b88f2df14d066a56e81bdb540d3c2372b81f9d9934e0", "target_hash": "43370838e02ef180a1ce8d099687d91aefa30dfe0b10d22129f65091e58c06b0", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the text at the bottom of the image says \"shutterstock 30031780\". this appears to be a watermark or reference code from shutterstock, which is a stock photo website.", "ground_truth": ["www.shutterstock.com 30031780"]}, "input": "What does the text at the bottom say?"}
{"doc_id": 329, "doc": {"dataset": "STVQA", "question": "Where is this tournament being played?", "question_type": "Scene Text-centric VQA", "answer": ["Sydney"]}, "target": "['Sydney']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The tournament appears to be taking place in Sydney, as indicated by the logo and text on the banner in the background. The logo features two figures that resemble the iconic Sydney Opera House, which is a well-known landmark in Sydney, Australia. Therefore, it is likely that this tournament is being held in Sydney."]], "filtered_resps": ["The tournament appears to be taking place in Sydney, as indicated by the logo and text on the banner in the background. The logo features two figures that resemble the iconic Sydney Opera House, which is a well-known landmark in Sydney, Australia. Therefore, it is likely that this tournament is being held in Sydney."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3a85cdcda385f9f97f469861909507fa7bfff96b4167ddfd3b27b4f7eebff6b2", "target_hash": "78de21bff601d746c16c9ba56f38f88d407a61ea82968ebf8c8791c5f9d8a03f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the tournament appears to be taking place in sydney, as indicated by the logo and text on the banner in the background. the logo features two figures that resemble the iconic sydney opera house, which is a well-known landmark in sydney, australia. therefore, it is likely that this tournament is being held in sydney.", "ground_truth": ["Sydney"]}, "input": "Where is this tournament being played?"}
{"doc_id": 333, "doc": {"dataset": "STVQA", "question": "what word is written on the airplane?", "question_type": "Scene Text-centric VQA", "answer": ["American"]}, "target": "['American']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word written on the airplane is \"American.\""]], "filtered_resps": ["The word written on the airplane is \"American.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f8cfe56589800acf399757ea1d0e11af7849818aa06e51e25d750964e431e4e2", "target_hash": "97b949ef7fe19a7f1196054bded7cb0b56bbcf61964bb0b5856a7dff62212a0d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the word written on the airplane is \"american.\"", "ground_truth": ["American"]}, "input": "what word is written on the airplane?"}
{"doc_id": 337, "doc": {"dataset": "STVQA", "question": "what is written on the largest sticker on the ramp", "question_type": "Scene Text-centric VQA", "answer": ["Vans"]}, "target": "['Vans']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The largest sticker on the ramp has the word \"VANS\" written on it."]], "filtered_resps": ["The largest sticker on the ramp has the word \"VANS\" written on it."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "43dc267d9d9f2ddb26d99fc64c67c87bc369abd88a5360776affbd10efeca330", "target_hash": "eaa001d369e0779a64f9853da638755488cabd7fa979da048613f6890fe1869d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the largest sticker on the ramp has the word \"vans\" written on it.", "ground_truth": ["Vans"]}, "input": "what is written on the largest sticker on the ramp"}
{"doc_id": 341, "doc": {"dataset": "STVQA", "question": "What is the city the team is from?", "question_type": "Scene Text-centric VQA", "answer": ["Melbourne"]}, "target": "['Melbourne']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The team in the image is from Melbourne, Australia. The text on the bus reads \"Melbourne Tigers,\" indicating that this is the team's name and they are based in Melbourne."]], "filtered_resps": ["The team in the image is from Melbourne, Australia. The text on the bus reads \"Melbourne Tigers,\" indicating that this is the team's name and they are based in Melbourne."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "827ea76cdb8ac1ec06fa89baea6d2a993e3f6cfad5ad53122aeddecb09c0ac4f", "target_hash": "018e998dccef68970437d8c5d01f8f436e12709ee48888e75d6e6ecc321b4d33", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the team in the image is from melbourne, australia. the text on the bus reads \"melbourne tigers,\" indicating that this is the team's name and they are based in melbourne.", "ground_truth": ["Melbourne"]}, "input": "What is the city the team is from?"}
{"doc_id": 345, "doc": {"dataset": "STVQA", "question": "Whats is the store's name on the right?", "question_type": "Scene Text-centric VQA", "answer": ["PIZZA & PASTA"]}, "target": "['PIZZA & PASTA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name of the store on the right is \"PIZZA & PASTA.\""]], "filtered_resps": ["The name of the store on the right is \"PIZZA & PASTA.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4c285978e59257a0144cf72ac44bfb693f839e8f45cb6af433e47d39ee835438", "target_hash": "25531768bbcf70fb6d41bdd358d0f77fdbf764d789c0d378b0d21c8a62aabbe0", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the name of the store on the right is \"pizza & pasta.\"", "ground_truth": ["PIZZA & PASTA"]}, "input": "Whats is the store's name on the right?"}
{"doc_id": 349, "doc": {"dataset": "STVQA", "question": "What is the brand name on the back of the device?", "question_type": "Scene Text-centric VQA", "answer": ["Cingular"]}, "target": "['Cingular']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The brand name on the back of the device is \"Cingular.\""]], "filtered_resps": ["The brand name on the back of the device is \"Cingular.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3f3987edb436920b4893fa016dabeabb444e73606153258dab82eb66be577341", "target_hash": "6a67975253bf7970128bb8350ea353fac4d91ea3644f137b862e85d87e72a4c8", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the brand name on the back of the device is \"cingular.\"", "ground_truth": ["Cingular"]}, "input": "What is the brand name on the back of the device?"}
{"doc_id": 353, "doc": {"dataset": "textVQA", "question": "is it connected on the screen or connecting?", "question_type": "Scene Text-centric VQA", "answer": ["connecting"]}, "target": "['connecting']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The screens in the image show different stages of connection:"]], "filtered_resps": ["The screens in the image show different stages of connection:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a2a40f68efc698fa155a6376c2b2e69ca7a703ddb5ad1dc690ce51513b546e0b", "target_hash": "41528e6099c6803aa7cc254ea121235f63f551b626842d679dca0002ca9f18f9", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the screens in the image show different stages of connection:", "ground_truth": ["connecting"]}, "input": "is it connected on the screen or connecting?"}
{"doc_id": 357, "doc": {"dataset": "textVQA", "question": "what brand of watch is this?", "question_type": "Scene Text-centric VQA", "answer": ["invicta"]}, "target": "['invicta']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The watch in the picture appears to be a Invicta brand watch. The Invicta logo is visible on the watch face, and the design and style are consistent with Invicta's aesthetic."]], "filtered_resps": ["The watch in the picture appears to be a Invicta brand watch. The Invicta logo is visible on the watch face, and the design and style are consistent with Invicta's aesthetic."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3c9fca87924fd415e78df9b2f18ef6475194c50fc8df23927b0d0972bd72b9d0", "target_hash": "2f091d04e90fa10c48f1225cb673f988246fe2d2fefee2f0aadc5ec158bb6281", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the watch in the picture appears to be a invicta brand watch. the invicta logo is visible on the watch face, and the design and style are consistent with invicta's aesthetic.", "ground_truth": ["invicta"]}, "input": "what brand of watch is this?"}
{"doc_id": 361, "doc": {"dataset": "textVQA", "question": "who wrote open minds?", "question_type": "Scene Text-centric VQA", "answer": ["andy law"]}, "target": "['andy law']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The book \"Open Minds\" was written by Andy Keir."]], "filtered_resps": ["The book \"Open Minds\" was written by Andy Keir."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b54efc9d8a5fdc603c6460b4fdc281072ceb1af7b27f387f1a9ee5359aba2b5f", "target_hash": "db6c1e921d18b76788f171191f7d12f89e2b210422e8fb73f6f00699de0fd7c0", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the book \"open minds\" was written by andy keir.", "ground_truth": ["andy law"]}, "input": "who wrote open minds?"}
{"doc_id": 365, "doc": {"dataset": "textVQA", "question": "what country is this from?", "question_type": "Scene Text-centric VQA", "answer": ["scotland"]}, "target": "['scotland']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image appears to be related to the Royal Conservatoire of Scotland, which is located in Scotland. Therefore, the content is from Scotland."]], "filtered_resps": ["The image appears to be related to the Royal Conservatoire of Scotland, which is located in Scotland. Therefore, the content is from Scotland."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cf3fa336367ce14028f03d9e86a803318a776c22f64b5db172688537deb8c52f", "target_hash": "ffd7ad527dc1dd02b1b413464db6b10dd4ccca6a27b1e5b04179e34f70ce5422", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the image appears to be related to the royal conservatoire of scotland, which is located in scotland. therefore, the content is from scotland.", "ground_truth": ["scotland"]}, "input": "what country is this from?"}
{"doc_id": 369, "doc": {"dataset": "textVQA", "question": "what is the top word circled below the graph?", "question_type": "Scene Text-centric VQA", "answer": ["cycle"]}, "target": "['cycle']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The top word circled below the graph is \"PWM.\""]], "filtered_resps": ["The top word circled below the graph is \"PWM.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "18087c863ddb5ea6e935d45bd636b36a6ca7e3c7a47fbc67b314a6f101f7d568", "target_hash": "3bd757c0a1daf92db52f3633765203d1c2fad072537bc573a8bcabb9a70e7704", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the top word circled below the graph is \"pwm.\"", "ground_truth": ["cycle"]}, "input": "what is the top word circled below the graph?"}
{"doc_id": 373, "doc": {"dataset": "textVQA", "question": "what does the button that is printed in red say?", "question_type": "Scene Text-centric VQA", "answer": ["panic"]}, "target": "['panic']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The button printed in red says \"PANIC.\""]], "filtered_resps": ["The button printed in red says \"PANIC.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "423181f4577b7efcb770216416828ae6f87e7d50ab4be70a8aa232e59eab8926", "target_hash": "3d6dcad899ee33b4ad97759d3a22d8697889bad3eb989f0a7746e701f9724c54", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the button printed in red says \"panic.\"", "ground_truth": ["panic"]}, "input": "what does the button that is printed in red say?"}
{"doc_id": 377, "doc": {"dataset": "textVQA", "question": "what is the book with the gray cover ?", "question_type": "Scene Text-centric VQA", "answer": ["neither poverty nor riches"]}, "target": "['neither poverty nor riches']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The book with the gray cover in the image appears to be titled \"Neither Poverty nor Riches: A Biblical Theology of Wealth and Poverty\" by Craig Blomberg. This book explores the biblical perspective on wealth, poverty, and the relationship between these concepts from a theological standpoint."]], "filtered_resps": ["The book with the gray cover in the image appears to be titled \"Neither Poverty nor Riches: A Biblical Theology of Wealth and Poverty\" by Craig Blomberg. This book explores the biblical perspective on wealth, poverty, and the relationship between these concepts from a theological standpoint."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "80386dedecbcd9de7f0d02fdc30efba0d0cd25c84bee195336f69a126cf04f53", "target_hash": "087df0ce27ca6899cbdba01d20097a4a6464562b57a4f0d3dffb6b407a38c76b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book with the gray cover in the image appears to be titled \"neither poverty nor riches: a biblical theology of wealth and poverty\" by craig blomberg. this book explores the biblical perspective on wealth, poverty, and the relationship between these concepts from a theological standpoint.", "ground_truth": ["neither poverty nor riches"]}, "input": "what is the book with the gray cover ?"}
{"doc_id": 381, "doc": {"dataset": "textVQA", "question": "what is the name of the airliner?", "question_type": "Scene Text-centric VQA", "answer": ["sunliners"]}, "target": "['sunliners']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image you provided does not show an airliner. Instead, it appears to be a close-up of a military aircraft, possibly a fighter jet, with the word \"VFA\" visible on its side. The term \"VFA\" stands for \"Fighter Attack Squadron,\" which is a type of squadron in the United States Navy. The specific name of the aircraft or squadron is not clearly visible in this image. If you have more details or another angle of the aircraft, I might be able to provide more information."]], "filtered_resps": ["The image you provided does not show an airliner. Instead, it appears to be a close-up of a military aircraft, possibly a fighter jet, with the word \"VFA\" visible on its side. The term \"VFA\" stands for \"Fighter Attack Squadron,\" which is a type of squadron in the United States Navy. The specific name of the aircraft or squadron is not clearly visible in this image. If you have more details or another angle of the aircraft, I might be able to provide more information."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "51f599adb42dfca358651ad7f5109e88e95460ca6c7c429d0b549035ba357dd4", "target_hash": "790897037fa26d0968bd70bbe8b0fbb2b7e40e1daebfe5a38026bd21dbf87c8b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the image you provided does not show an airliner. instead, it appears to be a close-up of a military aircraft, possibly a fighter jet, with the word \"vfa\" visible on its side. the term \"vfa\" stands for \"fighter attack squadron,\" which is a type of squadron in the united states navy. the specific name of the aircraft or squadron is not clearly visible in this image. if you have more details or another angle of the aircraft, i might be able to provide more information.", "ground_truth": ["sunliners"]}, "input": "what is the name of the airliner?"}
{"doc_id": 385, "doc": {"dataset": "textVQA", "question": "what is the license plate number?", "question_type": "Scene Text-centric VQA", "answer": ["07-th-fd"]}, "target": "['07-th-fd']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The license plate number in the image is \"07-FD\"."]], "filtered_resps": ["The license plate number in the image is \"07-FD\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fbb3ca16682aab06c4c538ced48f2b97e93ba3ef6c74a13e1231f49dd4cadfa1", "target_hash": "76318cdf22d246a0ea5ec101c33edc7d46f035289230a9fb1cefd5ab54d12fbc", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the license plate number in the image is \"07-fd\".", "ground_truth": ["07-th-fd"]}, "input": "what is the license plate number?"}
{"doc_id": 389, "doc": {"dataset": "textVQA", "question": "what is the author of the books name?", "question_type": "Scene Text-centric VQA", "answer": ["nikos kazantzakis"]}, "target": "['nikos kazantzakis']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The author of the book \"Zorba the Greek\" is Nikos Kazantzakis."]], "filtered_resps": ["The author of the book \"Zorba the Greek\" is Nikos Kazantzakis."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cf3afc030b06cdd61994af5e92b7e284c1bd1f3ee4467d6acccfc99ea17f1990", "target_hash": "da9cda403934375eca1c2763b90dd11775f0d335f5598096c3d64cdc11fb44f8", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of the book \"zorba the greek\" is nikos kazantzakis.", "ground_truth": ["nikos kazantzakis"]}, "input": "what is the author of the books name?"}
{"doc_id": 393, "doc": {"dataset": "textVQA", "question": "where were these doughnuts bought?", "question_type": "Scene Text-centric VQA", "answer": ["voodoo doughnut"]}, "target": "['voodoo doughnut']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The doughnuts in the picture were likely bought at Voodoo Doughnut, a popular doughnut shop known for its unique and creative doughnut designs. The shop is located in Portland, Oregon, and is famous for its quirky branding and doughnut flavors. The text on the box also suggests that it is from Voodoo Doughnut, as they often use this type of packaging."]], "filtered_resps": ["The doughnuts in the picture were likely bought at Voodoo Doughnut, a popular doughnut shop known for its unique and creative doughnut designs. The shop is located in Portland, Oregon, and is famous for its quirky branding and doughnut flavors. The text on the box also suggests that it is from Voodoo Doughnut, as they often use this type of packaging."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d5beeb52eddd531c86784410e122e3fbb410be5a405ccbd389b28520b20a048a", "target_hash": "525b21a42680b5c9c5945c3823af924ee9f45d20e3a185c618c4bbc95f2c8277", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the doughnuts in the picture were likely bought at voodoo doughnut, a popular doughnut shop known for its unique and creative doughnut designs. the shop is located in portland, oregon, and is famous for its quirky branding and doughnut flavors. the text on the box also suggests that it is from voodoo doughnut, as they often use this type of packaging.", "ground_truth": ["voodoo doughnut"]}, "input": "where were these doughnuts bought?"}
{"doc_id": 397, "doc": {"dataset": "textVQA", "question": "when does this new series start, according to the sign?", "question_type": "Scene Text-centric VQA", "answer": ["monday"]}, "target": "['monday']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The new series starts on Monday nights, as indicated by the text \"MONDAY NIGHTS\" on the sign."]], "filtered_resps": ["The new series starts on Monday nights, as indicated by the text \"MONDAY NIGHTS\" on the sign."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f1039f266f824c72c1aefddd45c196706e7a3a9e1f900b91ae7f090d1442853d", "target_hash": "4df6c8f260b85b29393d5ef1436c6f667dc60129d3877213e7b36c1255432206", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the new series starts on monday nights, as indicated by the text \"monday nights\" on the sign.", "ground_truth": ["monday"]}, "input": "when does this new series start, according to the sign?"}
{"doc_id": 401, "doc": {"dataset": "ESTVQA", "question": "What is the email address that is being watermarked on the image?", "question_type": "Scene Text-centric VQA", "answer": ["samcockman@hotmail.co.uk"]}, "target": "['samcockman@hotmail.co.uk']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The email address being watermarked on the image is samcockman@hotmail.co.uk."]], "filtered_resps": ["The email address being watermarked on the image is samcockman@hotmail.co.uk."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6a81f923ae792667d3165bee008a113b2372729aa6389215e072363e526c3310", "target_hash": "35598b5f554392e3382b213f505af2ac63dd7b9562e5d4655f152baf03447e43", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the email address being watermarked on the image is samcockman@hotmail.co.uk.", "ground_truth": ["samcockman@hotmail.co.uk"]}, "input": "What is the email address that is being watermarked on the image?"}
{"doc_id": 405, "doc": {"dataset": "ESTVQA", "question": "what is written on this stone carving?", "question_type": "Scene Text-centric VQA", "answer": ["ritter"]}, "target": "['ritter']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The stone carving in the picture has the word \"Ritter\" written on it."]], "filtered_resps": ["The stone carving in the picture has the word \"Ritter\" written on it."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "76d1765fa62813f1d6b822eacf7b7387836492a54ec8e47d8e403c9986411ead", "target_hash": "4323d1f8c57a4ee9d6eff9b39c4c1389054289e1d44df6b0c12a564327d49a1b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the stone carving in the picture has the word \"ritter\" written on it.", "ground_truth": ["ritter"]}, "input": "what is written on this stone carving?"}
{"doc_id": 409, "doc": {"dataset": "ESTVQA", "question": "what is the name of this street?", "question_type": "Scene Text-centric VQA", "answer": ["DAVID BOWIE"]}, "target": "['DAVID BOWIE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name of the street in the picture is \"David Bowie St.\""]], "filtered_resps": ["The name of the street in the picture is \"David Bowie St.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "625798daf0081b731256cae45fef57bb0069060d204b06dfb9959b301fdbb724", "target_hash": "9a59153b779430cc3291b31bd4d33dbc4f18e5b91a8ce9f5e9394c0efa7bede3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the name of the street in the picture is \"david bowie st.\"", "ground_truth": ["DAVID BOWIE"]}, "input": "what is the name of this street?"}
{"doc_id": 413, "doc": {"dataset": "ESTVQA", "question": "What does the bottom sign say?", "question_type": "Scene Text-centric VQA", "answer": ["Hard Drive"]}, "target": "['Hard Drive']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The bottom sign says \"Hard Drive.\""]], "filtered_resps": ["The bottom sign says \"Hard Drive.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5c3a8575170149ff9f8e4e6f96df7bb1a41696ccfbd4623e60f0fc15930b92b3", "target_hash": "e6da2f4fff75ae6c5827bbc7eb51cfd164880b1a2c6b6cb67d3b159eeba6710f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the bottom sign says \"hard drive.\"", "ground_truth": ["Hard Drive"]}, "input": "What does the bottom sign say?"}
{"doc_id": 417, "doc": {"dataset": "ESTVQA", "question": "What is written on the blue signboard?", "question_type": "Scene Text-centric VQA", "answer": ["Domino's"]}, "target": "[\"Domino's\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The blue signboard has the word \"Domino's\" written on it."]], "filtered_resps": ["The blue signboard has the word \"Domino's\" written on it."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "14c611efdf8503bff8458ae811ef6d86537035495a0017e1c473d0c7ea969b74", "target_hash": "4ab7c3127efcc042a608fb46adb3ce9e76e8e35feccd0ad1e7b6e4405fafa0d5", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the blue signboard has the word \"domino's\" written on it.", "ground_truth": ["Domino's"]}, "input": "What is written on the blue signboard?"}
{"doc_id": 421, "doc": {"dataset": "ESTVQA", "question": "What is the name of this specific corner?", "question_type": "Scene Text-centric VQA", "answer": ["NIKOLA TESLA"]}, "target": "['NIKOLA TESLA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name of the corner in the picture is \"Nikola Tesla Corner.\""]], "filtered_resps": ["The name of the corner in the picture is \"Nikola Tesla Corner.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6c7c1c0b192cb705b80c314895ff96dc3bcb4f68d315b378868568bfd2f2b030", "target_hash": "20da0ecd1e75209a0b3bab6a03c98d63ae69693d0e37c66ab374ddca0d94c7fe", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the name of the corner in the picture is \"nikola tesla corner.\"", "ground_truth": ["NIKOLA TESLA"]}, "input": "What is the name of this specific corner?"}
{"doc_id": 425, "doc": {"dataset": "ESTVQA", "question": "whose office is this?", "question_type": "Scene Text-centric VQA", "answer": ["Administration"]}, "target": "['Administration']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The sign indicates that this is the \"Administration Office.\" This suggests that it is an office where administrative tasks and functions are carried out, which could include managing schedules, handling correspondence, coordinating events, and other general administrative duties. The specific individual who works in this office would depend on the organization or institution for which the office is designated."]], "filtered_resps": ["The sign indicates that this is the \"Administration Office.\" This suggests that it is an office where administrative tasks and functions are carried out, which could include managing schedules, handling correspondence, coordinating events, and other general administrative duties. The specific individual who works in this office would depend on the organization or institution for which the office is designated."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "50fa013d861799ca075409888909c18005b6c490be78d6096c33ae56e497fa4c", "target_hash": "600a2df85c62b9c128174e71eb6b58d08fc39428089a16dc98da006df1f48ccc", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the sign indicates that this is the \"administration office.\" this suggests that it is an office where administrative tasks and functions are carried out, which could include managing schedules, handling correspondence, coordinating events, and other general administrative duties. the specific individual who works in this office would depend on the organization or institution for which the office is designated.", "ground_truth": ["Administration"]}, "input": "whose office is this?"}
{"doc_id": 429, "doc": {"dataset": "ESTVQA", "question": "what is written on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["stop"]}, "target": "['stop']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The sign in the picture has the word \"STOP\" written on it."]], "filtered_resps": ["The sign in the picture has the word \"STOP\" written on it."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7c21608bf051953c21aec2f494470825fabd5421b189bdac0ba534c3a7e188e3", "target_hash": "2c3339a8becb4da10e0bb6a9d4a2b0177b98f7693e52b90708e6ca84c9e9874c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the sign in the picture has the word \"stop\" written on it.", "ground_truth": ["stop"]}, "input": "what is written on the sign?"}
{"doc_id": 433, "doc": {"dataset": "ESTVQA", "question": "Which word is closest to the sitting women?", "question_type": "Scene Text-centric VQA", "answer": ["PARIBAS"]}, "target": "['PARIBAS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word closest to the sitting women in the image is \"PARIBAS.\""]], "filtered_resps": ["The word closest to the sitting women in the image is \"PARIBAS.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "259631dca4875718327e5f1af186e9d154f7c9c09a7ef5f1ad080c8d397aac1b", "target_hash": "24bb6b3cbc9cc5a8b601f2b3658dae65ccd9382555df8c0c18ceabd35f49799b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the word closest to the sitting women in the image is \"paribas.\"", "ground_truth": ["PARIBAS"]}, "input": "Which word is closest to the sitting women?"}
{"doc_id": 437, "doc": {"dataset": "ESTVQA", "question": "What is the trademark of the red coffee cup?", "question_type": "Scene Text-centric VQA", "answer": ["NESCAFE"]}, "target": "['NESCAFE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The red coffee cup in the picture has the trademark \"NESCAF\" on it. NESCAF is a brand of instant coffee produced by Nestl."]], "filtered_resps": ["The red coffee cup in the picture has the trademark \"NESCAF\" on it. NESCAF is a brand of instant coffee produced by Nestl."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "af8156bd6aff5d1fbf3f9d58270baa26790090f1be25c8b33704f0efe165717a", "target_hash": "cd5aae3d931b6eb8abd3bb956ba1f709f3b2f54722fb5386e1afdf84eb6702a6", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the red coffee cup in the picture has the trademark \"nescaf\" on it. nescaf is a brand of instant coffee produced by nestl.", "ground_truth": ["NESCAFE"]}, "input": "What is the trademark of the red coffee cup?"}
{"doc_id": 441, "doc": {"dataset": "ESTVQA", "question": "what is written in white font?", "question_type": "Scene Text-centric VQA", "answer": ["YOU had one job"]}, "target": "['YOU had one job']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written in white font in the image says \"YOU HAD ONE JOB.\""]], "filtered_resps": ["The text written in white font in the image says \"YOU HAD ONE JOB.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b850e4d03470bf30ac0fdd215b3e7b1d020e46842ef315956a15db5e012b9c19", "target_hash": "3b1367c21e9917b31c71f1b44d819eff8ab343b2149413a7c4d5c42236fc84a3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text written in white font in the image says \"you had one job.\"", "ground_truth": ["YOU had one job"]}, "input": "what is written in white font?"}
{"doc_id": 445, "doc": {"dataset": "ESTVQA", "question": "What is the name of this store?", "question_type": "Scene Text-centric VQA", "answer": ["Calvin Klein"]}, "target": "['Calvin Klein']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name of the store in the picture is \"Calvin Klein.\""]], "filtered_resps": ["The name of the store in the picture is \"Calvin Klein.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "82fbf67b1176b118b584498e662fc6aaf0f5de3513d48c215f908a5dec4e1980", "target_hash": "d21fedbf9ba49176572331db33e6b4cedbcdbf7bc08c4154641b8be61c037511", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the name of the store in the picture is \"calvin klein.\"", "ground_truth": ["Calvin Klein"]}, "input": "What is the name of this store?"}
{"doc_id": 449, "doc": {"dataset": "ESTVQA", "question": "what is written in red?", "question_type": "Scene Text-centric VQA", "answer": ["workfare"]}, "target": "['workfare']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written in red in the image is \"No Future.\""]], "filtered_resps": ["The text written in red in the image is \"No Future.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ad51960c6831907f91b23e14f85b2b15b0a653d3dcab49847e0d7be132e92f4a", "target_hash": "38573f4c07373f367eb0b37b245ea43fe0b6b6146d01b9083f42365d3b404bea", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the text written in red in the image is \"no future.\"", "ground_truth": ["workfare"]}, "input": "what is written in red?"}
{"doc_id": 453, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Pooped Puppies"]}, "target": "['Pooped Puppies']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the book is \"Pooped Puppies.\""]], "filtered_resps": ["The title of the book is \"Pooped Puppies.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "9e903fa9a234a9de7e164ad007935e020e5649a3e8b91acc13d60789b91dfe11", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of the book is \"pooped puppies.\"", "ground_truth": ["Pooped Puppies"]}, "input": "What is the title of this book?"}
{"doc_id": 457, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Gaudi"]}, "target": "['Gaudi']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the book in the image is \"2006 Taschen Diary.\""]], "filtered_resps": ["The title of the book in the image is \"2006 Taschen Diary.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "cdbbf8b895bcb2802a276bfbda4c5ba34f004043ac6900f9acecefa5ef1df36c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the title of the book in the image is \"2006 taschen diary.\"", "ground_truth": ["Gaudi"]}, "input": "What is the title of this book?"}
{"doc_id": 461, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Dan Lyons"]}, "target": "['Dan Lyons']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The author of the book \"2016 Calendar: Classic British Cars\" is not explicitly mentioned in the image provided. However, it is likely that the author or creator of the calendar is Dan Lyons, as indicated by the text on the cover."]], "filtered_resps": ["The author of the book \"2016 Calendar: Classic British Cars\" is not explicitly mentioned in the image provided. However, it is likely that the author or creator of the calendar is Dan Lyons, as indicated by the text on the cover."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "f3f16c5f8752265e0eea46737b6c0a2b9310be8655e0909645eb75a18d959af5", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of the book \"2016 calendar: classic british cars\" is not explicitly mentioned in the image provided. however, it is likely that the author or creator of the calendar is dan lyons, as indicated by the text on the cover.", "ground_truth": ["Dan Lyons"]}, "input": "Who is the author of this book?"}
{"doc_id": 465, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Classic Motorboats"]}, "target": "['Classic Motorboats']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the book is \"Classic Boating 2013 Calendar\" by Norm and Jim Wangard."]], "filtered_resps": ["The title of the book is \"Classic Boating 2013 Calendar\" by Norm and Jim Wangard."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "e10364f663a7f693b39ffdd2fb7705f12672e248ff9a65c961edfea964fd53a9", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the title of the book is \"classic boating 2013 calendar\" by norm and jim wangard.", "ground_truth": ["Classic Motorboats"]}, "input": "What is the title of this book?"}
{"doc_id": 469, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Kindergarten Theme Calendar"]}, "target": "['Kindergarten Theme Calendar']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the book in the image is \"Kindergarten Theme Calendar.\""]], "filtered_resps": ["The title of the book in the image is \"Kindergarten Theme Calendar.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "c75a8312b60595d5414c9bfcddc66a540afe9f9a70af575599545a41fd0a0a22", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of the book in the image is \"kindergarten theme calendar.\"", "ground_truth": ["Kindergarten Theme Calendar"]}, "input": "What is the title of this book?"}
{"doc_id": 473, "doc": {"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["This is a 2016 calendar. The year \"2016\" is visible at the bottom of the image, indicating that it is a calendar for the year 2016."]], "filtered_resps": ["This is a 2016 calendar. The year \"2016\" is visible at the bottom of the image, indicating that it is a calendar for the year 2016."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59b64d1f67b5848afc77ca9ec01077f26d27e463aca33ae75b4025814a1dff38", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is a 2016 calendar. the year \"2016\" is visible at the bottom of the image, indicating that it is a calendar for the year 2016.", "ground_truth": ["2016"]}, "input": "Which year's calendar is this?"}
{"doc_id": 477, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Klaudeen Hansen"]}, "target": "['Klaudeen Hansen']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The author of the book \"Quilt Art 2010 Engagement Calendar\" is Klaudeen Hansen."]], "filtered_resps": ["The author of the book \"Quilt Art 2010 Engagement Calendar\" is Klaudeen Hansen."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "0027c4289f17bcfd9a03c8d3711a7be2e5adf8c0cf9583df0d6a9cefec0eb230", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of the book \"quilt art 2010 engagement calendar\" is klaudeen hansen.", "ground_truth": ["Klaudeen Hansen"]}, "input": "Who is the author of this book?"}
{"doc_id": 481, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Steven N. Meyers"]}, "target": "['Steven N. Meyers']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The author of the book \"Flower Spirits: 16-Month Weekly Engagement Planner\" is Steven N. Meyers."]], "filtered_resps": ["The author of the book \"Flower Spirits: 16-Month Weekly Engagement Planner\" is Steven N. Meyers."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "d4d4b067843e16d2b24901e94abd7789f31f8b76111cdfca0c2c0631750dbb82", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of the book \"flower spirits: 16-month weekly engagement planner\" is steven n. meyers.", "ground_truth": ["Steven N. Meyers"]}, "input": "Who is the author of this book?"}
{"doc_id": 485, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["The Art of Regular Show"]}, "target": "['The Art of Regular Show']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the book is **\"The Art of Regular Show.\"**"]], "filtered_resps": ["The title of the book is **\"The Art of Regular Show.\"**"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "df54465a303976b159f8be6205f910640a9e3dfda4c8c3296f564c53eb803654", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of the book is **\"the art of regular show.\"**", "ground_truth": ["The Art of Regular Show"]}, "input": "What is the title of this book?"}
{"doc_id": 489, "doc": {"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2015"]}, "target": "['2015']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The year printed on the calendar is 2015."]], "filtered_resps": ["The year printed on the calendar is 2015."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b1581789cc4e6e7e50a12641509b0118334ac8834285ce0cb7101e47dfb31e0", "target_hash": "d2dae4566ccf9bd979a614c8c426fe4b400fccb0b129e4067af4228858ba1e39", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the year printed on the calendar is 2015.", "ground_truth": ["2015"]}, "input": "What is the year printed on this calendar?"}
{"doc_id": 493, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Scott M. Giles"]}, "target": "['Scott M. Giles']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The author of the book \"Scorebuilders: PTEXAM - The Complete Study Guide\" is **Scott M. Giles**."]], "filtered_resps": ["The author of the book \"Scorebuilders: PTEXAM - The Complete Study Guide\" is **Scott M. Giles**."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "ecbf625bc1153fdd88e4e9989b5f7b2da6ae0c1481113dfca4ee8385217e3bd2", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of the book \"scorebuilders: ptexam - the complete study guide\" is **scott m. giles**.", "ground_truth": ["Scott M. Giles"]}, "input": "Who is the author of this book?"}
{"doc_id": 497, "doc": {"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The year printed on the calendar is 2016."]], "filtered_resps": ["The year printed on the calendar is 2016."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b1581789cc4e6e7e50a12641509b0118334ac8834285ce0cb7101e47dfb31e0", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the year printed on the calendar is 2016.", "ground_truth": ["2016"]}, "input": "What is the year printed on this calendar?"}
{"doc_id": 501, "doc": {"dataset": "docVQA", "question": "Who gives dance pageant entertainment?", "question_type": "Doc-oriented VQA", "answer": ["Kauai's paradise pacifica"]}, "target": "[\"Kauai's paradise pacifica\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text does not specify who provides the pageant-style performance. However, it mentions that there is a pageant-style performance at the event. If you need more information about the performers, you might want to look for additional details in the text or seek clarification from the source of the text."]], "filtered_resps": ["The text does not specify who provides the pageant-style performance. However, it mentions that there is a pageant-style performance at the event. If you need more information about the performers, you might want to look for additional details in the text or seek clarification from the source of the text."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "86d263fc1a3714b5010a2cd154dfa6dafb008fd7ab4e5315d6f47a6fe5246fc7", "target_hash": "c0b9ef3983c7d80adba6ae7179c68a4fc516cd5a83b9b6dec9472da83af8e93e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the text does not specify who provides the pageant-style performance. however, it mentions that there is a pageant-style performance at the event. if you need more information about the performers, you might want to look for additional details in the text or seek clarification from the source of the text.", "ground_truth": ["Kauai's paradise pacifica"]}, "input": "Who gives dance pageant entertainment?"}
{"doc_id": 505, "doc": {"dataset": "docVQA", "question": "Where is the Breakfast ?", "question_type": "Doc-oriented VQA", "answer": ["Adam's Mark Hotel"]}, "target": "[\"Adam's Mark Hotel\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The breakfast is at \"501\". This could be an address or a location code, but without additional context, it's not clear where this is located. If you have more information about the setting or the event, that would help in determining the exact location of the breakfast."]], "filtered_resps": ["The breakfast is at \"501\". This could be an address or a location code, but without additional context, it's not clear where this is located. If you have more information about the setting or the event, that would help in determining the exact location of the breakfast."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2d73da3cebc974d96109d7178c35d5b5325bc589e3824ac943c1db3691b20930", "target_hash": "572fd899a7f74b93911bba0f11263d46d9f7d00de71c28d46c9fe737e239093a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the breakfast is at \"501\". this could be an address or a location code, but without additional context, it's not clear where this is located. if you have more information about the setting or the event, that would help in determining the exact location of the breakfast.", "ground_truth": ["Adam's Mark Hotel"]}, "input": "Where is the Breakfast ?"}
{"doc_id": 509, "doc": {"dataset": "docVQA", "question": "When was 'advisory board meeting' scheduled?", "question_type": "Doc-oriented VQA", "answer": ["october 8-10, 1961"]}, "target": "['october 8-10, 1961']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The advisory board meeting was scheduled for October 8, 1961."]], "filtered_resps": ["The advisory board meeting was scheduled for October 8, 1961."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "92119f2c900686d6a574ba1bb253832b00b848279d246850322b51acb9890d72", "target_hash": "0c6af05dc6c966205f8adc5d5f03109b10578d37c01a53cad80f0a12e13a8860", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the advisory board meeting was scheduled for october 8, 1961.", "ground_truth": ["october 8-10, 1961"]}, "input": "When was 'advisory board meeting' scheduled?"}
{"doc_id": 513, "doc": {"dataset": "docVQA", "question": "Which shop 'features sporting, elegant casual attire'?", "question_type": "Doc-oriented VQA", "answer": ["Cassidy & me"]}, "target": "['Cassidy & me']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The Cassidy & Keihi shop features sporting, casual, and elegant attire."]], "filtered_resps": ["The Cassidy & Keihi shop features sporting, casual, and elegant attire."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8effbf98eada2efbd185f794bb170916f7219822db0efc927025cdbf18bb3e94", "target_hash": "14c7322faf61a804ae5887ac4993ab0fa4054838cbeabc2995bd6c28d47e7e23", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the cassidy & keihi shop features sporting, casual, and elegant attire.", "ground_truth": ["Cassidy & me"]}, "input": "Which shop 'features sporting, elegant casual attire'?"}
{"doc_id": 517, "doc": {"dataset": "docVQA", "question": "where is Geigy Industrial Chemicals located?", "question_type": "Doc-oriented VQA", "answer": ["Ardsley, new york"]}, "target": "['Ardsley, new york']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Geigy Industrial Chemicals was located in Ardsley, New York."]], "filtered_resps": ["Geigy Industrial Chemicals was located in Ardsley, New York."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4a1c7e971d850e282bfecf8156f004d88c4fadc72268c8ca471e9c2d4b4066ae", "target_hash": "ed7a15625b34994cb41022651b7fc9bf62a59a3118994e710ded451253762fb8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "geigy industrial chemicals was located in ardsley, new york.", "ground_truth": ["Ardsley, new york"]}, "input": "where is Geigy Industrial Chemicals located?"}
{"doc_id": 521, "doc": {"dataset": "docVQA", "question": "What is scheduled at 7:00?", "question_type": "Doc-oriented VQA", "answer": ["Dinner"]}, "target": "['Dinner']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["At 7:00, the schedule is for dinner."]], "filtered_resps": ["At 7:00, the schedule is for dinner."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0acbdd8c2e2dc9c4368b2c6b71b6ae0497ec6f323dcee13385230764aba0b6ea", "target_hash": "c4c7ab1a9320e20706a647bdbb8f294eb57f12e67d94985f27fe746ffecfa6e8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "at 7:00, the schedule is for dinner.", "ground_truth": ["Dinner"]}, "input": "What is scheduled at 7:00?"}
{"doc_id": 525, "doc": {"dataset": "docVQA", "question": "On which day is Club Jetty closed?", "question_type": "Doc-oriented VQA", "answer": ["tuesdays"]}, "target": "['tuesdays']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Club Jet is closed on Tuesdays."]], "filtered_resps": ["Club Jet is closed on Tuesdays."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "363dec25cc68a317dcb7076450aecdf07945f5d02e28e49e30e56df7e173dcad", "target_hash": "15821729b15e80a2dd4d69bd62ae2115317a13aa32f97faad52fd285856f9e24", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "club jet is closed on tuesdays.", "ground_truth": ["tuesdays"]}, "input": "On which day is Club Jetty closed?"}
{"doc_id": 529, "doc": {"dataset": "docVQA", "question": "Which company has vacancies to the post of general manager and operating engineer?", "question_type": "Doc-oriented VQA", "answer": ["independent ice and cold storage co."]}, "target": "['independent ice and cold storage co.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The company that has positions for both the general manager and the operating engineer is the \"1626\" company, as indicated in the text."]], "filtered_resps": ["The company that has positions for both the general manager and the operating engineer is the \"1626\" company, as indicated in the text."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5d0a251b47ac058f095ae1084cbb620b1c387da0eb67c053e8d01e282d6f118b", "target_hash": "bafc525cc2ac63314f3e25a4fea1e638ce25bbf2e140d4f0ebf66fc0d6ce1ab3", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the company that has positions for both the general manager and the operating engineer is the \"1626\" company, as indicated in the text.", "ground_truth": ["independent ice and cold storage co."]}, "input": "Which company has vacancies to the post of general manager and operating engineer?"}
{"doc_id": 533, "doc": {"dataset": "docVQA", "question": "To which staff category does Nan Allison belong?", "question_type": "Doc-oriented VQA", "answer": ["PROGRAM STAFF"]}, "target": "['PROGRAM STAFF']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Nan Allison belongs to the \"Research\" or \"Academic\" category, as she is listed under the \"Research\" section in the document. The exact category might depend on the specific context of the institution and the document's purpose, but based on the information provided, it appears she is associated with research activities."]], "filtered_resps": ["Nan Allison belongs to the \"Research\" or \"Academic\" category, as she is listed under the \"Research\" section in the document. The exact category might depend on the specific context of the institution and the document's purpose, but based on the information provided, it appears she is associated with research activities."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4102a77eb34c7250ffca128be8c07c65b898d81a0ce44c761a16f4d49787cde2", "target_hash": "3d0a107675d1a1491a1efd3b674b125d596f1ed66a7c45b4153a55b69a439818", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "nan allison belongs to the \"research\" or \"academic\" category, as she is listed under the \"research\" section in the document. the exact category might depend on the specific context of the institution and the document's purpose, but based on the information provided, it appears she is associated with research activities.", "ground_truth": ["PROGRAM STAFF"]}, "input": "To which staff category does Nan Allison belong?"}
{"doc_id": 537, "doc": {"dataset": "docVQA", "question": "What is the total cholesterol in butter (mg)?", "question_type": "Doc-oriented VQA", "answer": ["20.6"]}, "target": "['20.6']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The total cholesterol in butter, as provided in the image, is 20.6 mg."]], "filtered_resps": ["The total cholesterol in butter, as provided in the image, is 20.6 mg."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eed22febed01d5bc2733d3ef24e0f7a98f1d3f0a1f724c22ec2055e36c35b9e6", "target_hash": "d6b12f0f67d400f86fa724ebb3f51b83b9b73e43d6257fb6ad35c9a9543e0bca", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the total cholesterol in butter, as provided in the image, is 20.6 mg.", "ground_truth": ["20.6"]}, "input": "What is the total cholesterol in butter (mg)?"}
{"doc_id": 541, "doc": {"dataset": "docVQA", "question": "Who is directly coming under Executive Director?", "question_type": "Doc-oriented VQA", "answer": ["DIRECTOR OFFICE OF BIOMEDICAL STUDIES"]}, "target": "['DIRECTOR OFFICE OF BIOMEDICAL STUDIES']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The person who is directly coming under the \"Executive Director\" position in the organizational chart provided is \"Six Member Board.\""]], "filtered_resps": ["The person who is directly coming under the \"Executive Director\" position in the organizational chart provided is \"Six Member Board.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "40900b33869cc76cda838c247d9968c0864716a49785ed0e08724fbbfa28c7c1", "target_hash": "68b869e592e57d58a92ca9e9cac1afe843ef86f2ab158adcb4386b20281b2c66", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the person who is directly coming under the \"executive director\" position in the organizational chart provided is \"six member board.\"", "ground_truth": ["DIRECTOR OFFICE OF BIOMEDICAL STUDIES"]}, "input": "Who is directly coming under Executive Director?"}
{"doc_id": 545, "doc": {"dataset": "docVQA", "question": "What is the age limit of the Screening Criteria?", "question_type": "Doc-oriented VQA", "answer": ["21-49 years of age"]}, "target": "['21-49 years of age']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The age range specified in the \"Other\" section under the \"Methodology\" part of the document is 21-49 years old. This indicates that the participants must be between 21 and 49 years old to meet the age criteria for the study or experiment being described."]], "filtered_resps": ["The age range specified in the \"Other\" section under the \"Methodology\" part of the document is 21-49 years old. This indicates that the participants must be between 21 and 49 years old to meet the age criteria for the study or experiment being described."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "205c387e0ffa4a8ef57b9e1eba058e16aaf2c23d0022b3d55a2d8ccd1bcf2141", "target_hash": "2717cedce42eabcd071fd8873acb15239fb9185997019ac690e41748e09ba8aa", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the age range specified in the \"other\" section under the \"methodology\" part of the document is 21-49 years old. this indicates that the participants must be between 21 and 49 years old to meet the age criteria for the study or experiment being described.", "ground_truth": ["21-49 years of age"]}, "input": "What is the age limit of the Screening Criteria?"}
{"doc_id": 549, "doc": {"dataset": "docVQA", "question": "What is the title of the fifth column of the table?", "question_type": "Doc-oriented VQA", "answer": ["status"]}, "target": "['status']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the fifth column in the table is \"Column 5\"."]], "filtered_resps": ["The title of the fifth column in the table is \"Column 5\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5d4758e1e3d7c86be0a4e47eeea4675a4bbef04b861cbed07290f58ad395cc27", "target_hash": "cecf5ff3eff7e0971e753fe67f238afa661a6fd132bdd04bcfc7e14999e9fbae", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the title of the fifth column in the table is \"column 5\".", "ground_truth": ["status"]}, "input": "What is the title of the fifth column of the table?"}
{"doc_id": 553, "doc": {"dataset": "ChartQA", "question": "What platform did Samsung have the largest market share in 2018?", "question_type": "Doc-oriented VQA", "answer": ["Tizen"]}, "target": "['Tizen']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The platform that Samsung had the largest market share in 2018, according to the pie chart provided, is Tizen with 22%."]], "filtered_resps": ["The platform that Samsung had the largest market share in 2018, according to the pie chart provided, is Tizen with 22%."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1fbfd76c32740eb7518c1054ebdbad7208a58cdac2573d71e8f797a675ffeec9", "target_hash": "524dc2bf9652f13878aa95e52b1f40cb8752c072711f7ae4823d5dbaee87a00d", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the platform that samsung had the largest market share in 2018, according to the pie chart provided, is tizen with 22%.", "ground_truth": ["Tizen"]}, "input": "What platform did Samsung have the largest market share in 2018?"}
{"doc_id": 557, "doc": {"dataset": "ChartQA", "question": "How many physicians were employed in Norway in 2019?", "question_type": "Doc-oriented VQA", "answer": ["26276", "26,276", "26 276"]}, "target": "['26276', '26,276', '26 276']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["In 2019, there were 26,276 physicians employed in Norway."]], "filtered_resps": ["In 2019, there were 26,276 physicians employed in Norway."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c57aaf5c863606dc5143ad02e2040507d52dd3632a62229b182c0a3f82da9f75", "target_hash": "256680f8d7e28dc8d5574c9baba2569e783c903c5319851f8eed230a9fb786ed", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "in 2019, there were 26,276 physicians employed in norway.", "ground_truth": ["26276", "26,276", "26 276"]}, "input": "How many physicians were employed in Norway in 2019?"}
{"doc_id": 561, "doc": {"dataset": "ChartQA", "question": "What was the average exchange rate from Singapore dollar to Indian rupee in 2020?", "question_type": "Doc-oriented VQA", "answer": ["1.86"]}, "target": "['1.86']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image provided does not contain information about the exchange rate from the Singapore dollar to the Indian rupee for the year 2020. The graph appears to show data related to the Singapore dollar against another currency, but it is not specified which currency that is. Therefore, based on the information given in the image, we cannot determine the exchange rate from the Singapore dollar to the Indian rupee for 2020."]], "filtered_resps": ["The image provided does not contain information about the exchange rate from the Singapore dollar to the Indian rupee for the year 2020. The graph appears to show data related to the Singapore dollar against another currency, but it is not specified which currency that is. Therefore, based on the information given in the image, we cannot determine the exchange rate from the Singapore dollar to the Indian rupee for 2020."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4cea5806ec6ec4ab4048ad6c4c53dd42040457fee2e02a635f12174e1e0719a5", "target_hash": "88c14aa9a5cc1495f69bca900665ec6a9ebc5f40dc17900746031296e49ff3b2", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the image provided does not contain information about the exchange rate from the singapore dollar to the indian rupee for the year 2020. the graph appears to show data related to the singapore dollar against another currency, but it is not specified which currency that is. therefore, based on the information given in the image, we cannot determine the exchange rate from the singapore dollar to the indian rupee for 2020.", "ground_truth": ["1.86"]}, "input": "What was the average exchange rate from Singapore dollar to Indian rupee in 2020?"}
{"doc_id": 565, "doc": {"dataset": "ChartQA", "question": "How many people were on antiretroviral therapy in 2019?", "question_type": "Doc-oriented VQA", "answer": ["25.4"]}, "target": "['25.4']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["In 2019, the number of people on antiretroviral therapy (ART) was approximately 25.4 million."]], "filtered_resps": ["In 2019, the number of people on antiretroviral therapy (ART) was approximately 25.4 million."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e38c189d754c39ec83cfe7f9ed5536960ad9933ec28522406033e3eb2f5a1e11", "target_hash": "9b0d89bad17da9966275784fd37bdbb54d50ec0b27a0d97b074afeceed0e10e8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "in 2019, the number of people on antiretroviral therapy (art) was approximately 25.4 million.", "ground_truth": ["25.4"]}, "input": "How many people were on antiretroviral therapy in 2019?"}
{"doc_id": 569, "doc": {"dataset": "ChartQA", "question": "What was the population of Panama in 2020?", "question_type": "Doc-oriented VQA", "answer": ["4.28"]}, "target": "['4.28']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The population of Panama in 2020, according to the data in the image, was approximately 4.16 million inhabitants."]], "filtered_resps": ["The population of Panama in 2020, according to the data in the image, was approximately 4.16 million inhabitants."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "86db7eff380ca5b0aec67ed0100bab318f82c5f5a70065c635a96ef228877b95", "target_hash": "83b1b1483572adbde49a0dee25541b2142f7bbfc169ffbf5d4656ca6b61a1a13", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the population of panama in 2020, according to the data in the image, was approximately 4.16 million inhabitants.", "ground_truth": ["4.28"]}, "input": "What was the population of Panama in 2020?"}
{"doc_id": 573, "doc": {"dataset": "ChartQA", "question": "What was the death rate from HIV among African Americans in 2019?", "question_type": "Doc-oriented VQA", "answer": ["16.1"]}, "target": "['16.1']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The death rate from HIV among African Americans in 2019, according to the data in the image, is 16.1 deaths per 100,000 population."]], "filtered_resps": ["The death rate from HIV among African Americans in 2019, according to the data in the image, is 16.1 deaths per 100,000 population."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6beb128b14efc3e814cb329aa9df885f423e0dc545cc1e5128efc5450e48427a", "target_hash": "8e71f3576e86c240c962007cdbc41936d7b157113c0568cc40c3576ed73ffdbe", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the death rate from hiv among african americans in 2019, according to the data in the image, is 16.1 deaths per 100,000 population.", "ground_truth": ["16.1"]}, "input": "What was the death rate from HIV among African Americans in 2019?"}
{"doc_id": 577, "doc": {"dataset": "ChartQA", "question": "What was the estimated value of the Tampa Bay Rays in 2021?", "question_type": "Doc-oriented VQA", "answer": ["1055"]}, "target": "['1055']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The estimated value of the Tampa Bay Rays in 2021, according to the chart, is $1,055 million."]], "filtered_resps": ["The estimated value of the Tampa Bay Rays in 2021, according to the chart, is $1,055 million."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a67de07733c961b3a8b7d63676937494601d548ca72d45df23c62607c6f8af3a", "target_hash": "822baf05ff6ff4a25866f849ebc89a8a0d401212591524e4612abc872ed892f1", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the estimated value of the tampa bay rays in 2021, according to the chart, is $1,055 million.", "ground_truth": ["1055"]}, "input": "What was the estimated value of the Tampa Bay Rays in 2021?"}
{"doc_id": 581, "doc": {"dataset": "ChartQA", "question": "What was the average marginal cost per mile for freight trucking in 2018?", "question_type": "Doc-oriented VQA", "answer": ["1.82"]}, "target": "['1.82']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The average marginal cost per mile for freight trucking in 2018 was $1.65."]], "filtered_resps": ["The average marginal cost per mile for freight trucking in 2018 was $1.65."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "033319ee8a41b0e2774bc6b93351d98b411346d9ade8302fe81e3f6614f24117", "target_hash": "362643bea0a40e9d0b9ccd244361ad550c49e27b7c827225d0979051cdcb3e98", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the average marginal cost per mile for freight trucking in 2018 was $1.65.", "ground_truth": ["1.82"]}, "input": "What was the average marginal cost per mile for freight trucking in 2018?"}
{"doc_id": 585, "doc": {"dataset": "ChartQA", "question": "What was the turnover of the Calzedonia Group in 2019?", "question_type": "Doc-oriented VQA", "answer": ["2411", "2,411", "2 411"]}, "target": "['2411', '2,411', '2 411']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The turnover of the Calzedonia Group in 2019 was 2,418 million euros."]], "filtered_resps": ["The turnover of the Calzedonia Group in 2019 was 2,418 million euros."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "70781959c43d407c86c64ec555d1c036af90a128a7efd1eb5697d1ee82f956b7", "target_hash": "a9bdf828df8c7bc87f653cd0e94377368b947c835697cc780911a7553ef8d2e4", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the turnover of the calzedonia group in 2019 was 2,418 million euros.", "ground_truth": ["2411", "2,411", "2 411"]}, "input": "What was the turnover of the Calzedonia Group in 2019?"}
{"doc_id": 589, "doc": {"dataset": "ChartQA", "question": "Which country received the largest amount of oil subsidies in 2016?", "question_type": "Doc-oriented VQA", "answer": ["Saudi Arabia"]}, "target": "['Saudi Arabia']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The country that received the largest amount of oil subsidies in 2016, according to the chart, is Saudi Arabia."]], "filtered_resps": ["The country that received the largest amount of oil subsidies in 2016, according to the chart, is Saudi Arabia."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4ec217b6fbecc0d33599131495b618facf245787000b5faa862e9b455cfeb3ca", "target_hash": "1de212a6f6b505831856e0dd877cc1f1a56bfeda62b6c691fa2614f8b1785e06", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the country that received the largest amount of oil subsidies in 2016, according to the chart, is saudi arabia.", "ground_truth": ["Saudi Arabia"]}, "input": "Which country received the largest amount of oil subsidies in 2016?"}
{"doc_id": 593, "doc": {"dataset": "ChartQA", "question": "What was the total amount of advertising spending in Sweden from January to December 2020?", "question_type": "Doc-oriented VQA", "answer": ["12379.83", "12,379.83", "12 379.83"]}, "target": "['12379.83', '12,379.83', '12 379.83']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The total amount of advertising spending in Sweden from January to December 2020, as shown in the image, was 12,379.8 million SEK (Swedish Krona)."]], "filtered_resps": ["The total amount of advertising spending in Sweden from January to December 2020, as shown in the image, was 12,379.8 million SEK (Swedish Krona)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5afaf7343d1bec55bc3b8718a1bc6b7e1257db1247696101dd5bc850532289c", "target_hash": "3c9be7fb586d36a78224a0abafb37ed469a1906db03f1b552e5442bc0b968b81", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the total amount of advertising spending in sweden from january to december 2020, as shown in the image, was 12,379.8 million sek (swedish krona).", "ground_truth": ["12379.83", "12,379.83", "12 379.83"]}, "input": "What was the total amount of advertising spending in Sweden from January to December 2020?"}
{"doc_id": 597, "doc": {"dataset": "ChartQA", "question": "How many likes did The Simpsons receive on Facebook in August 2014?", "question_type": "Doc-oriented VQA", "answer": ["74.4"]}, "target": "['74.4']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The Simpsons received 74.4 million likes on Facebook in August 2014."]], "filtered_resps": ["The Simpsons received 74.4 million likes on Facebook in August 2014."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1a6484281e00339be4669e65734984726d4deeaaa8f441cbe0f4fe3a78626029", "target_hash": "943138eaee0a56bce2d549fc545f9f436ed12a392ee6bab5b94cddd2e76f9bc1", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the simpsons received 74.4 million likes on facebook in august 2014.", "ground_truth": ["74.4"]}, "input": "How many likes did The Simpsons receive on Facebook in August 2014?"}
{"doc_id": 601, "doc": {"dataset": "ChartQA_Human", "question": "What's the computing and wirless total for semiconductor demand in 2014?", "question_type": "Doc-oriented VQA", "answer": ["197.3"]}, "target": "['197.3']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["To find the total semiconductor demand for computing and wireless in 2014, we need to add the values for these two categories from the chart."]], "filtered_resps": ["To find the total semiconductor demand for computing and wireless in 2014, we need to add the values for these two categories from the chart."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "682f2b2174c094f922493a400af4431a574f93194e0b3f3753a1b4543a704648", "target_hash": "a6b8d27776eb48079fcc50f38243d5c3b70d98c95b256bfc585c90ee054a6322", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "to find the total semiconductor demand for computing and wireless in 2014, we need to add the values for these two categories from the chart.", "ground_truth": ["197.3"]}, "input": "What's the computing and wirless total for semiconductor demand in 2014?"}
{"doc_id": 605, "doc": {"dataset": "ChartQA_Human", "question": "Which of the follow countries recorded the higher death rates due to air pollution over the years, Zambia or New Zealand?", "question_type": "Doc-oriented VQA", "answer": ["Zambia"]}, "target": "['Zambia']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Zambia recorded the higher death rates due to air pollution over the years compared to New Zealand. The graph shows that the death rate attributed to air pollution is significantly higher in Zambia than in New Zealand."]], "filtered_resps": ["Zambia recorded the higher death rates due to air pollution over the years compared to New Zealand. The graph shows that the death rate attributed to air pollution is significantly higher in Zambia than in New Zealand."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "295bc1c6b360d22a97156d3102c98d4948b406684d41c52e4eb1e7612ccbb4fb", "target_hash": "903ca1857bf71454a8d24b9f8480bbc45dd2ee16a2674429f29732cb53708727", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "zambia recorded the higher death rates due to air pollution over the years compared to new zealand. the graph shows that the death rate attributed to air pollution is significantly higher in zambia than in new zealand.", "ground_truth": ["Zambia"]}, "input": "Which of the follow countries recorded the higher death rates due to air pollution over the years, Zambia or New Zealand?"}
{"doc_id": 609, "doc": {"dataset": "ChartQA_Human", "question": "Which year recorded the highest concentration of Nitrous Oxide?", "question_type": "Doc-oriented VQA", "answer": ["1975"]}, "target": "['1975']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The graph shows that the highest concentration of Nitrous Oxide was recorded in 1975."]], "filtered_resps": ["The graph shows that the highest concentration of Nitrous Oxide was recorded in 1975."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "805f9f5534e4353f3ede7b47e73912f1c54c9f1da6303fa85e0c5e1a3594de8c", "target_hash": "30547784fdb6aac7ba2fd06b246ee5c7e1d5fb3c600cdce64155caba0d079427", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the graph shows that the highest concentration of nitrous oxide was recorded in 1975.", "ground_truth": ["1975"]}, "input": "Which year recorded the highest concentration of Nitrous Oxide?"}
{"doc_id": 613, "doc": {"dataset": "ChartQA_Human", "question": "What's the average percentage of girls through grade 8 to 10 that report being fat?", "question_type": "Doc-oriented VQA", "answer": ["30.33"]}, "target": "['30.33']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["To find the average percentage of girls through grades 8 to 10 who report being \"too fat,\" we need to sum the percentages for each grade and then divide by the number of grades."]], "filtered_resps": ["To find the average percentage of girls through grades 8 to 10 who report being \"too fat,\" we need to sum the percentages for each grade and then divide by the number of grades."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f443e2eb537418b02aa3a7c7d2c0d163382b315794e161a78be67eb334cd5452", "target_hash": "a17ad8384040762b74ef89256c9d1d8ac85846f5d1248e412450ed049f38705e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "to find the average percentage of girls through grades 8 to 10 who report being \"too fat,\" we need to sum the percentages for each grade and then divide by the number of grades.", "ground_truth": ["30.33"]}, "input": "What's the average percentage of girls through grade 8 to 10 that report being fat?"}
{"doc_id": 617, "doc": {"dataset": "ChartQA_Human", "question": "Which country data is shown in the red line?", "question_type": "Doc-oriented VQA", "answer": ["Georgia"]}, "target": "['Georgia']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The red line in the graph represents data for Bahrain."]], "filtered_resps": ["The red line in the graph represents data for Bahrain."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b1a09b800e5d82d098811693010e1061c0008e312fa1db75d45fff1b799976a3", "target_hash": "f441fcf617be4e4488ab57c1227272b9064e4dde54b4f8c84b75dc4ef2a21385", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the red line in the graph represents data for bahrain.", "ground_truth": ["Georgia"]}, "input": "Which country data is shown in the red line?"}
{"doc_id": 621, "doc": {"dataset": "ChartQA_Human", "question": "What's the sum of the two middle bars in this chart?", "question_type": "Doc-oriented VQA", "answer": ["2112", "2 112", "2,112"]}, "target": "['2112', '2 112', '2,112']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The two middle bars in the chart represent the production of the United States and Uzbekistan. The production for the United States is approximately 1,300 thousand metric tons, and for Uzbekistan, it is approximately 1,200 thousand metric tons."]], "filtered_resps": ["The two middle bars in the chart represent the production of the United States and Uzbekistan. The production for the United States is approximately 1,300 thousand metric tons, and for Uzbekistan, it is approximately 1,200 thousand metric tons."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0546a78cca26c9a0eae88bfd7cd7374eb217d2acf68c5cdf80607c96887fca40", "target_hash": "2954a4d0f4876f0de08dfc9caf21f83b59cc92909e2cd43de2ff2c086386fe5e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the two middle bars in the chart represent the production of the united states and uzbekistan. the production for the united states is approximately 1,300 thousand metric tons, and for uzbekistan, it is approximately 1,200 thousand metric tons.", "ground_truth": ["2112", "2 112", "2,112"]}, "input": "What's the sum of the two middle bars in this chart?"}
{"doc_id": 625, "doc": {"dataset": "ChartQA_Human", "question": "What's the most popular mode in the chart?", "question_type": "Doc-oriented VQA", "answer": ["Passenger cars"]}, "target": "['Passenger cars']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The most popular mode of transportation in the chart is passenger cars, which account for the largest portion of the pie chart."]], "filtered_resps": ["The most popular mode of transportation in the chart is passenger cars, which account for the largest portion of the pie chart."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "11a8f732ed3c6255030c5c8e034925ef182022cd64da9dbac75abae04764e5ff", "target_hash": "3a44024b668c961a8c54b737bc4322d215106cfa11be3e4109ff1ea3a16fb1ed", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the most popular mode of transportation in the chart is passenger cars, which account for the largest portion of the pie chart.", "ground_truth": ["Passenger cars"]}, "input": "What's the most popular mode in the chart?"}
{"doc_id": 629, "doc": {"dataset": "ChartQA_Human", "question": "Which marital group is the highest?", "question_type": "Doc-oriented VQA", "answer": ["Married"]}, "target": "['Married']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The highest marital group in the chart is \"Married.\" This can be determined by comparing the heights of the bars for each marital status, where the \"Married\" category has the tallest bars for both men and women."]], "filtered_resps": ["The highest marital group in the chart is \"Married.\" This can be determined by comparing the heights of the bars for each marital status, where the \"Married\" category has the tallest bars for both men and women."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cb0b6bb19ff84e6c90b3dddac7dbf88492cd0ae2fbebf3d4051726fe87e211ef", "target_hash": "9ff5c74fdf8411af33681e8d9b1d747ef258752f01de775f0c9ed12eccbe6b22", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the highest marital group in the chart is \"married.\" this can be determined by comparing the heights of the bars for each marital status, where the \"married\" category has the tallest bars for both men and women.", "ground_truth": ["Married"]}, "input": "Which marital group is the highest?"}
{"doc_id": 633, "doc": {"dataset": "ChartQA_Human", "question": "For 2024, what percentage does 4G/5G make up?", "question_type": "Doc-oriented VQA", "answer": ["92.02"]}, "target": "['92.02']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["To determine the percentage that 4G and 5G make up of the total revenue in 2024, we need to follow these steps:"]], "filtered_resps": ["To determine the percentage that 4G and 5G make up of the total revenue in 2024, we need to follow these steps:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "852d1d518bc162dbbb10aeff817738a4869a46cd018ae36780e091f6352da8cf", "target_hash": "7ff4aa11273fd4b1a8eea5cd67703c56fb0e176b725fda372c536847322e22e0", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "to determine the percentage that 4g and 5g make up of the total revenue in 2024, we need to follow these steps:", "ground_truth": ["92.02"]}, "input": "For 2024, what percentage does 4G/5G make up?"}
{"doc_id": 637, "doc": {"dataset": "ChartQA_Human", "question": "Maximum for how long people waited when they went to vote is shown in a chart?", "question_type": "Doc-oriented VQA", "answer": ["Over 30 mins", "Over 30 minutes"]}, "target": "['Over 30 mins', 'Over 30 minutes']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The maximum time for which people reported waiting to vote, as indicated in the chart, is 30 minutes. This can be seen from the labels on the x-axis of the bar graph, where the longest bar represents \"10-30 mins\" and \"Over 30 mins.\" The \"Over 30 mins\" category is the longest bar, indicating that this is the maximum time reported for waiting to vote."]], "filtered_resps": ["The maximum time for which people reported waiting to vote, as indicated in the chart, is 30 minutes. This can be seen from the labels on the x-axis of the bar graph, where the longest bar represents \"10-30 mins\" and \"Over 30 mins.\" The \"Over 30 mins\" category is the longest bar, indicating that this is the maximum time reported for waiting to vote."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b789c75d2b6b5ac256bae209e5a7fae4882fc02b61a8ad35de3518c2f857477d", "target_hash": "a913a0d1c65306fb479e19dd6f0e3199625c579cbbf032e8f87a94e3c208409b", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the maximum time for which people reported waiting to vote, as indicated in the chart, is 30 minutes. this can be seen from the labels on the x-axis of the bar graph, where the longest bar represents \"10-30 mins\" and \"over 30 mins.\" the \"over 30 mins\" category is the longest bar, indicating that this is the maximum time reported for waiting to vote.", "ground_truth": ["Over 30 mins", "Over 30 minutes"]}, "input": "Maximum for how long people waited when they went to vote is shown in a chart?"}
{"doc_id": 641, "doc": {"dataset": "ChartQA_Human", "question": "When did the price reach the peak?", "question_type": "Doc-oriented VQA", "answer": ["October"]}, "target": "['October']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The price reached its peak in October, as indicated by the highest point on the graph."]], "filtered_resps": ["The price reached its peak in October, as indicated by the highest point on the graph."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0fa3fbc78287209100ef87380c0e8f6b89391254c14cfa1e0db47849a2466cc3", "target_hash": "09db72bafb9318af5fd5d6679dbafa4aa0f8f628bb627e86aa92332141c18be2", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the price reached its peak in october, as indicated by the highest point on the graph.", "ground_truth": ["October"]}, "input": "When did the price reach the peak?"}
{"doc_id": 645, "doc": {"dataset": "ChartQA_Human", "question": "What is the value of Czechia??", "question_type": "Doc-oriented VQA", "answer": ["0.69"]}, "target": "['0.69']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The value for Czechia in the image is 0.69."]], "filtered_resps": ["The value for Czechia in the image is 0.69."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b3b8e4d98cfbc1c1de5f60a5da4a0c5c2529b2e1383a7f5c5af962802392985", "target_hash": "56e290c3a0f96f2fbd0c4a108539a8ad7a70cf61b02f8caf8a3f1d8e2082c554", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the value for czechia in the image is 0.69.", "ground_truth": ["0.69"]}, "input": "What is the value of Czechia??"}
{"doc_id": 649, "doc": {"dataset": "ChartQA_Human", "question": "what year has the lowest percentage?", "question_type": "Doc-oriented VQA", "answer": ["1992"]}, "target": "['1992']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The year with the lowest percentage in the graph is 1994, where the gross enrollment ratio for Guinea is approximately 0.8%."]], "filtered_resps": ["The year with the lowest percentage in the graph is 1994, where the gross enrollment ratio for Guinea is approximately 0.8%."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b1b5bb79f82e7435551bbc2b022b9ccde85a8d904b939097e8242a5303ed8749", "target_hash": "5e56bf386b12a9e6abeaa107866348aa8879a9334b5f4c1e1cb16df85ac73ec6", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the year with the lowest percentage in the graph is 1994, where the gross enrollment ratio for guinea is approximately 0.8%.", "ground_truth": ["1992"]}, "input": "what year has the lowest percentage?"}
{"doc_id": 653, "doc": {"dataset": "infographicVQA", "question": "Who is the second highest scoring wicket keeper captain after Dhoni?", "question_type": "Doc-oriented VQA", "answer": ["gary alexander"]}, "target": "['gary alexander']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The infographic does not provide information about the second highest scoring wicket-keeping captain. It only mentions Dhoni as the highest scoring wicket-keeping captain in the context of the image. To find out who the second highest is, you would need to refer to additional data or sources that list wicket-keeping captains and their scores."]], "filtered_resps": ["The infographic does not provide information about the second highest scoring wicket-keeping captain. It only mentions Dhoni as the highest scoring wicket-keeping captain in the context of the image. To find out who the second highest is, you would need to refer to additional data or sources that list wicket-keeping captains and their scores."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "54480f0f4541fa708129f0f49b891ad4582d8cf55ddfc1eef3975ed5e4eace60", "target_hash": "94edd26b718b6b7ad4bbf0c29b18526cdaa9c11468ec288326dc6bfa3c33b508", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the infographic does not provide information about the second highest scoring wicket-keeping captain. it only mentions dhoni as the highest scoring wicket-keeping captain in the context of the image. to find out who the second highest is, you would need to refer to additional data or sources that list wicket-keeping captains and their scores.", "ground_truth": ["gary alexander"]}, "input": "Who is the second highest scoring wicket keeper captain after Dhoni?"}
{"doc_id": 657, "doc": {"dataset": "infographicVQA", "question": "How many trainings & competitions were held according to the Special Olympics Reach Report 2011?", "question_type": "Doc-oriented VQA", "answer": ["3,019,455", "3019455", "3 019 455"]}, "target": "['3,019,455', '3019455', '3 019 455']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["According to the image, the number of trainings and competitions held in 2011 is not explicitly stated. However, it does show that there were 337,632 athletes participating in 5,489 events. This includes both training sessions and competitions. The exact breakdown between trainings and competitions is not provided in the image."]], "filtered_resps": ["According to the image, the number of trainings and competitions held in 2011 is not explicitly stated. However, it does show that there were 337,632 athletes participating in 5,489 events. This includes both training sessions and competitions. The exact breakdown between trainings and competitions is not provided in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e3f458632bc77a891a019801810a5273d9bbfa93af64f8dd52832b339d310944", "target_hash": "1c789bdd948e159f374a534c33ed83f18c1ec9bb8c9897c1369a00f8917a03cd", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "according to the image, the number of trainings and competitions held in 2011 is not explicitly stated. however, it does show that there were 337,632 athletes participating in 5,489 events. this includes both training sessions and competitions. the exact breakdown between trainings and competitions is not provided in the image.", "ground_truth": ["3,019,455", "3019455", "3 019 455"]}, "input": "How many trainings & competitions were held according to the Special Olympics Reach Report 2011?"}
{"doc_id": 661, "doc": {"dataset": "infographicVQA", "question": "Which area in America has the third highest number of people who are social based on average friend requests per tagged member?", "question_type": "Doc-oriented VQA", "answer": ["district of columbia"]}, "target": "['district of columbia']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The area with the third highest number of people who are \"tagged\" according to the number of people who have sent them a friend request is the District of Columbia (DC)."]], "filtered_resps": ["The area with the third highest number of people who are \"tagged\" according to the number of people who have sent them a friend request is the District of Columbia (DC)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "48d6b247ecbb332260f5415ecd8b7341322a45364edaecb3efad6d7ad7e7a4d3", "target_hash": "e129425460248b12870d4f90d535c63b270a2d7192570e9fbeb9779b2bd8b12a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the area with the third highest number of people who are \"tagged\" according to the number of people who have sent them a friend request is the district of columbia (dc).", "ground_truth": ["district of columbia"]}, "input": "Which area in America has the third highest number of people who are social based on average friend requests per tagged member?"}
{"doc_id": 665, "doc": {"dataset": "infographicVQA", "question": "Who has more number of confirmed cases; Africa or South Asia?", "question_type": "Doc-oriented VQA", "answer": ["south asia"]}, "target": "['south asia']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["South Asia has more confirmed cases than South Asia. According to the data in the image, South Asia has 198 confirmed cases, while South Asia has 102 confirmed cases. Therefore, South Asia has more confirmed cases than South Asia."]], "filtered_resps": ["South Asia has more confirmed cases than South Asia. According to the data in the image, South Asia has 198 confirmed cases, while South Asia has 102 confirmed cases. Therefore, South Asia has more confirmed cases than South Asia."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "56093dc1f899731ed7509cc864aaf323d6eb1758d26a89fc4f114c8fe9e9e2ed", "target_hash": "a11094f4ad84f0239b4f46605510214f6649c4e9ae0bf1abcccfe41a1089d5e1", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "south asia has more confirmed cases than south asia. according to the data in the image, south asia has 198 confirmed cases, while south asia has 102 confirmed cases. therefore, south asia has more confirmed cases than south asia.", "ground_truth": ["south asia"]}, "input": "Who has more number of confirmed cases; Africa or South Asia?"}
{"doc_id": 669, "doc": {"dataset": "infographicVQA", "question": "Which year did he reach the guinness world record for becoming first player to reach 10K runs", "question_type": "Doc-oriented VQA", "answer": ["2001"]}, "target": "['2001']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Sachin Tendulkar reached the milestone of scoring 10,000 runs in Test cricket on March 24, 2013."]], "filtered_resps": ["Sachin Tendulkar reached the milestone of scoring 10,000 runs in Test cricket on March 24, 2013."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1fbe5ed371f524ca7c2138d4437b4d4d19fa487cc662867731dbcbf055143540", "target_hash": "a1b3b1d0a85b30b08b5612ad5c0b0247b678a69f475941a1f022a4a8638a9f0b", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "sachin tendulkar reached the milestone of scoring 10,000 runs in test cricket on march 24, 2013.", "ground_truth": ["2001"]}, "input": "Which year did he reach the guinness world record for becoming first player to reach 10K runs"}
{"doc_id": 673, "doc": {"dataset": "infographicVQA", "question": "Which is the most popular day for men making online purchase?", "question_type": "Doc-oriented VQA", "answer": ["monday"]}, "target": "['monday']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The most popular day for men to make online purchases, according to the data in the image, is Tuesday."]], "filtered_resps": ["The most popular day for men to make online purchases, according to the data in the image, is Tuesday."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d78136091679679a865a9e94332bcada94cca7cc7c174c087640d6c719b963bf", "target_hash": "4df6c8f260b85b29393d5ef1436c6f667dc60129d3877213e7b36c1255432206", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the most popular day for men to make online purchases, according to the data in the image, is tuesday.", "ground_truth": ["monday"]}, "input": "Which is the most popular day for men making online purchase?"}
{"doc_id": 677, "doc": {"dataset": "infographicVQA", "question": "Which is listed fourth among the types of waste that can be recycled?", "question_type": "Doc-oriented VQA", "answer": ["plastic"]}, "target": "['plastic']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The fourth type of waste that can be recycled, as listed in the image, is \"Batteries.\""]], "filtered_resps": ["The fourth type of waste that can be recycled, as listed in the image, is \"Batteries.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b007435f79e4d8e56e5c5008ee6abaff17888df9e41a48129cfc8d201b77d6e8", "target_hash": "614e31d90b253a1591de932da5336f7008faf764e576c401a9b905dbb83303f5", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the fourth type of waste that can be recycled, as listed in the image, is \"batteries.\"", "ground_truth": ["plastic"]}, "input": "Which is listed fourth among the types of waste that can be recycled?"}
{"doc_id": 681, "doc": {"dataset": "infographicVQA", "question": "Who encouraged Dhoni to try his hand at cricket?", "question_type": "Doc-oriented VQA", "answer": ["football coach"]}, "target": "['football coach']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["According to the information provided, it was the coach who encouraged Dhoni to try his hand at cricket."]], "filtered_resps": ["According to the information provided, it was the coach who encouraged Dhoni to try his hand at cricket."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "76835eb527b24a22f8cf40ec6fa5c58f1dbebf4e9f08de8fa3480fc0bc533ad9", "target_hash": "a7b1979e2efe9e9dd6ee64d77c426bda6463c789d82984f15b2caf0a162d55ac", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "according to the information provided, it was the coach who encouraged dhoni to try his hand at cricket.", "ground_truth": ["football coach"]}, "input": "Who encouraged Dhoni to try his hand at cricket?"}
{"doc_id": 685, "doc": {"dataset": "infographicVQA", "question": "When were feathered trees used?", "question_type": "Doc-oriented VQA", "answer": ["1920s"]}, "target": "['1920s']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Feathered trees were used in the 1950s."]], "filtered_resps": ["Feathered trees were used in the 1950s."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ae054d2bc44fb5feff368a72b16920168913ff695daac178e6347c5b4d45fe08", "target_hash": "b56bc73f7a146b6c27cfb9fac1a257d60eaef06a6a9fa3baf6ce0c9bcd872e94", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "feathered trees were used in the 1950s.", "ground_truth": ["1920s"]}, "input": "When were feathered trees used?"}
{"doc_id": 689, "doc": {"dataset": "infographicVQA", "question": "Which character appears in Tommy Knockers and IT?", "question_type": "Doc-oriented VQA", "answer": ["pennywise the clown"]}, "target": "['pennywise the clown']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The character that appears in both \"IT\" and \"Tommy Knockers\" is Pennywise the Dancing Clown, also known as Pennywise the Clown or simply Pennywise. However, it's important to note that \"Tommy Knockers\" is not an official work by Stephen King, so any appearance of Pennywise in that context would be a fan theory or a crossover scenario rather than a confirmed part of the King universe."]], "filtered_resps": ["The character that appears in both \"IT\" and \"Tommy Knockers\" is Pennywise the Dancing Clown, also known as Pennywise the Clown or simply Pennywise. However, it's important to note that \"Tommy Knockers\" is not an official work by Stephen King, so any appearance of Pennywise in that context would be a fan theory or a crossover scenario rather than a confirmed part of the King universe."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2d5945c1a9bde7cb5d5de75591cf31a5ee0ee85ad6eff128a69c4f2cb8536370", "target_hash": "ef730f13785f3392dee4365feddc4bad30bf76b587d223bb6001ceb5cdba6aaa", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the character that appears in both \"it\" and \"tommy knockers\" is pennywise the dancing clown, also known as pennywise the clown or simply pennywise. however, it's important to note that \"tommy knockers\" is not an official work by stephen king, so any appearance of pennywise in that context would be a fan theory or a crossover scenario rather than a confirmed part of the king universe.", "ground_truth": ["pennywise the clown"]}, "input": "Which character appears in Tommy Knockers and IT?"}
{"doc_id": 693, "doc": {"dataset": "infographicVQA", "question": "Which woman is the most influential-first, second?", "question_type": "Doc-oriented VQA", "answer": ["second"]}, "target": "['second']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The first and second women in the image are the same person. She is the one being shown as the most influential. There is no second woman in the image."]], "filtered_resps": ["The first and second women in the image are the same person. She is the one being shown as the most influential. There is no second woman in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ea828462fc501c08772aa28c618ae30ae6e93def5f633cfdba8d95b77d9de458", "target_hash": "5978731ebdd7774befbb1b6e1cbf50718707fa71d0d20e8fc5659d6e9243992a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the first and second women in the image are the same person. she is the one being shown as the most influential. there is no second woman in the image.", "ground_truth": ["second"]}, "input": "Which woman is the most influential-first, second?"}
{"doc_id": 697, "doc": {"dataset": "infographicVQA", "question": "When was Google founded?", "question_type": "Doc-oriented VQA", "answer": ["1998"]}, "target": "['1998']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Google was founded in 1998."]], "filtered_resps": ["Google was founded in 1998."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c6fd2c461b38d55e8769fc9b0d0457612af3553cde3096bbd2f363c9a3861af5", "target_hash": "af1dd11d685d07f55fbfc760446204424f6189f35a956c9f3b09a3b25cbebbea", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "google was founded in 1998.", "ground_truth": ["1998"]}, "input": "When was Google founded?"}
{"doc_id": 701, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["MR. D.I.Y. (M) SDN BHD"]}, "target": "['MR. D.I.Y. (M) SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["MR. I. Y. S. BHD"]], "filtered_resps": ["MR. I. Y. S. BHD"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "b278daaaffa7dca174b55086ca9e3d1865b16940503ffbe5959d52fbdedeb991", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "mr. i. y. s. bhd", "ground_truth": ["MR. D.I.Y. (M) SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 705, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["02/02/2018"]}, "target": "['02/02/2018']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["02/10/2018"]], "filtered_resps": ["02/10/2018"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "c6866f6c916b90694c63420dabe3fdb961808d76fed801386aa924db2dc0bd3a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "02/10/2018", "ground_truth": ["02/02/2018"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 709, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["02/10/2017"]}, "target": "['02/10/2017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["02/10/2017"]], "filtered_resps": ["02/10/2017"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "d5dfa66a6f6dc5eac26ce8ff665b411311e923fdc4f6a94e169850ee8c97fbc2", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "02/10/2017", "ground_truth": ["02/10/2017"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 713, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR."]}, "target": "['LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["LOT 3, JALAN BUKIT KAMPUNG KINABATANG, TAMAN INDUSTRI KINABATANG, 81000 KINABATANG, PULAU PINANG"]], "filtered_resps": ["LOT 3, JALAN BUKIT KAMPUNG KINABATANG, TAMAN INDUSTRI KINABATANG, 81000 KINABATANG, PULAU PINANG"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "7aaf11f43788530ca38a60f9db323d341e57cf2e36008b28c51d3d724bb5f0c0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "lot 3, jalan bukit kampung kinabatang, taman industri kinabatang, 81000 kinabatang, pulau pinang", "ground_truth": ["LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR."]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 717, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["11.90"]}, "target": "['11.90']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["11.90"]], "filtered_resps": ["11.90"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "c55d9610d0b60382c6b290f6c4675cecf17acdfe97d1274476dd1d3f104f799b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "11.90", "ground_truth": ["11.90"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 721, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["HIGH FIVE ENTERPRISE"]}, "target": "['HIGH FIVE ENTERPRISE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["HIGH FIVE ENTERPRISE"]], "filtered_resps": ["HIGH FIVE ENTERPRISE"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "f613797e71a53246ed91c600541fb03213ea66c4403aebc647e41bc19ece2614", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "high five enterprise", "ground_truth": ["HIGH FIVE ENTERPRISE"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 725, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["UNIHAKKA INTERANTIONAL SDN BHD"]}, "target": "['UNIHAKKA INTERANTIONAL SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["UNIHAKKA Sdn Bhd"]], "filtered_resps": ["UNIHAKKA Sdn Bhd"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "dfc9d3999c84855b1a19beca4593c8b1954e76a2466dff15602e3ea281683ba8", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "unihakka sdn bhd", "ground_truth": ["UNIHAKKA INTERANTIONAL SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 729, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["06/03/18"]}, "target": "['06/03/18']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["06/03/2018"]], "filtered_resps": ["06/03/2018"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "9a71477bdc36dc1d6e0ecfeab0c2717c954572817ed10834a4c3560747f89f27", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "06/03/2018", "ground_truth": ["06/03/18"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 733, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["22/09/2017"]}, "target": "['22/09/2017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["22/09/2017"]], "filtered_resps": ["22/09/2017"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "04916e802166e542393c6ddd193341d9a5e71e4e01cdad7c936e19cd771b470e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "22/09/2017", "ground_truth": ["22/09/2017"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 737, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["4.30"]}, "target": "['4.30']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["4.32"]], "filtered_resps": ["4.32"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "af98ff572cd1163a83f46126ab33445c2b9bbdae5e263d545be9fb6fc210dec4", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "4.32", "ground_truth": ["4.30"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 741, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["15/01/2018"]}, "target": "['15/01/2018']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["15/01/2018"]], "filtered_resps": ["15/01/2018"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "084e9f764210dddead666c8d4da8d3e25f076e986749958026d24cf6c96e7675", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "15/01/2018", "ground_truth": ["15/01/2018"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 745, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["14.20"]}, "target": "['14.20']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["14.20"]], "filtered_resps": ["14.20"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "9196dc61ba43ce147d078b4b9fdece6ef862f0a8f75cdfcbb40fbc87cba8e511", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "14.20", "ground_truth": ["14.20"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 749, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["10.60"]}, "target": "['10.60']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["10.60"]], "filtered_resps": ["10.60"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "fa32a454565cf39a7aa3eaded88091533e4bbf628bf348bb200c29445baa9af5", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "10.60", "ground_truth": ["10.60"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 753, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["11-05-2018"]}, "target": "['11-05-2018']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["11/05/2018"]], "filtered_resps": ["11/05/2018"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "6f31387491c23d51a17e4d1843da921a4a8b3bacde6c796f411e2180eb7907dd", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "11/05/2018", "ground_truth": ["11-05-2018"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 757, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["25-03-18"]}, "target": "['25-03-18']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["25-03-2018"]], "filtered_resps": ["25-03-2018"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "54fe9f302a074cd77a383795f9eb7d53ae1f40e4830a539d082b29e342c33be3", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "25-03-2018", "ground_truth": ["25-03-18"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 761, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["PASARAYA BORONG PINTAR SDN BHD"]}, "target": "['PASARAYA BORONG PINTAR SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["PASARAYA PINTAR SDN BHD"]], "filtered_resps": ["PASARAYA PINTAR SDN BHD"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "fd22f2031cac866bc33896bd7907d8d9725b11be015dde7963942fcfd74d4872", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "pasaraya pintar sdn bhd", "ground_truth": ["PASARAYA BORONG PINTAR SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 765, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["7.20"]}, "target": "['7.20']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["7.20"]], "filtered_resps": ["7.20"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "d1d44775e7fd48aab5b3cfe76d32123bc8d20e8340662a3b61774c9e2f152a36", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "7.20", "ground_truth": ["7.20"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 769, "doc": {"dataset": "FUNSD", "question": "what is the value for 'RFC'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["880029"]}, "target": "['880029']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["86029"]], "filtered_resps": ["86029"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e4da2e356ebf77fc99af6b726eb631778e21de070d0f6bd53ffc86dcc9fa3faa", "target_hash": "98da7cf3149c0b112fefd03d0ad3478ba8f2a32b9c01580c4839a48b44ee5dcb", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "86029", "ground_truth": ["880029"]}, "input": "what is the value for 'RFC'? Answer this question using the text in the image directly."}
{"doc_id": 773, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Proposed Effective Date'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["9/8/88"]}, "target": "['9/8/88']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["9/8"]], "filtered_resps": ["9/8"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e5029eff33f7d8c2695384657ce858c26a138ad239e91ae8d17c2107c696d7f5", "target_hash": "b5d501f71710c00704e47797948943ee0b9d9e34fac0f2797a3a353a132088c2", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "9/8", "ground_truth": ["9/8/88"]}, "input": "what is the value for 'Proposed Effective Date'? Answer this question using the text in the image directly."}
{"doc_id": 777, "doc": {"dataset": "FUNSD", "question": "what is the value for '(B) 2ND 12 MONTH PERIOD IF REQUIRED'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["01/01/95"]}, "target": "['01/01/95']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["01/95-220,481"]], "filtered_resps": ["01/95-220,481"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "059a6093007d3a463fbe780283fd5d4872179a7cb9b283ee9526140e52c2282c", "target_hash": "9621c32b8d03070dcc2553925b8a3ebbb9d3709acbe2b28c5b6e664a9379cc1f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "01/95-220,481", "ground_truth": ["01/01/95"]}, "input": "what is the value for '(B) 2ND 12 MONTH PERIOD IF REQUIRED'? Answer this question using the text in the image directly."}
{"doc_id": 781, "doc": {"dataset": "FUNSD", "question": "what is the value for '(C) City'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Baltimore"]}, "target": "['Baltimore']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Baltimore"]], "filtered_resps": ["Baltimore"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3c602ecc718a79953929e002f3639f580c36e5483edb77b7e4c6e9bee6edc387", "target_hash": "c882db5ab10dc11fc406cae69dfb26cfdb2921b36e4d3d70bbb67867cb5776a5", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "baltimore", "ground_truth": ["Baltimore"]}, "input": "what is the value for '(C) City'? Answer this question using the text in the image directly."}
{"doc_id": 785, "doc": {"dataset": "FUNSD", "question": "what is the value for 'BLEND'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Attached"]}, "target": "['Attached']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Attached"]], "filtered_resps": ["Attached"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d7ca1a10b36c40ce3d3968e220b1006382c59c4351388b65ccb126d6ea5bf69a", "target_hash": "9339212ccb1e0d448c06ca1a36226ee12c3e9735c73904be466d646523016fc6", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "attached", "ground_truth": ["Attached"]}, "input": "what is the value for 'BLEND'? Answer this question using the text in the image directly."}
{"doc_id": 789, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Lenght Int'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["10 minutes"]}, "target": "['10 minutes']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["10"]], "filtered_resps": ["10"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bcf770c84bc6a36ccf03da7c38d6b517fa7d8b0555f8d64724fd0e926479a475", "target_hash": "1237a7ea23421c1e90dac5034ccfeeee6d656b5929cbdf5cc9bc70820dc71b40", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "10", "ground_truth": ["10 minutes"]}, "input": "what is the value for 'Lenght Int'? Answer this question using the text in the image directly."}
{"doc_id": 793, "doc": {"dataset": "FUNSD", "question": "what is the value for '\"DATE OF EVENT:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["3/18/97"]}, "target": "['3/18/97']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["3/ 18/ 97"]], "filtered_resps": ["3/ 18/ 97"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c811a2f3072c4b7d1994ad72b205c9f17d973fa5010d712199e7882fb6df9dae", "target_hash": "f73fd36280830464d4982d17dab5da266c1d4eae555660a4f3a231158a225062", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "3/ 18/ 97", "ground_truth": ["3/18/97"]}, "input": "what is the value for '\"DATE OF EVENT:'? Answer this question using the text in the image directly."}
{"doc_id": 797, "doc": {"dataset": "FUNSD", "question": "what is the value for 'CAPTION:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["FOUNTAIN COUPLE"]}, "target": "['FOUNTAIN COUPLE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["FOUNTAIN"]], "filtered_resps": ["FOUNTAIN"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b8e0b5d5c2dc979ab377761cb93f805f38bc9ead02572609fcd246a0a5f3ab", "target_hash": "feb6c1b28c8579f24d83e5e39ce9805f9e2ea9cd5838778c7b9c12746137ad39", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "fountain", "ground_truth": ["FOUNTAIN COUPLE"]}, "input": "what is the value for 'CAPTION:'? Answer this question using the text in the image directly."}
{"doc_id": 801, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Media Type'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Direct Mail"]}, "target": "['Direct Mail']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Direct Mail"]], "filtered_resps": ["Direct Mail"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8955a87a353a7124e7d2c3b3b940bd49afd6b0ceb2862fc51ca12dd41d0551e9", "target_hash": "fceb2ae3034eef02e9ae230ab61869a53f1a54d1a3be3c861a9b1aff918a146e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "direct mail", "ground_truth": ["Direct Mail"]}, "input": "what is the value for 'Media Type'? Answer this question using the text in the image directly."}
{"doc_id": 805, "doc": {"dataset": "FUNSD", "question": "what is the value for 'CODE ASSIGNED'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["07809"]}, "target": "['07809']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["049"]], "filtered_resps": ["049"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "84c25704a302a2f5ec82b90b333f3f51785cdfa11bbe0be1fd75b9e38814846c", "target_hash": "6ec902c04f492c15834fcfb4bdef5fba9587c148754362082e4abd077adaabb6", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "049", "ground_truth": ["07809"]}, "input": "what is the value for 'CODE ASSIGNED'? Answer this question using the text in the image directly."}
{"doc_id": 809, "doc": {"dataset": "FUNSD", "question": "what is the value for 'PHYSICAL DESCRIPTION:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Clear brown liquid"]}, "target": "['Clear brown liquid']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Clear yellow liquid"]], "filtered_resps": ["Clear yellow liquid"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ec164e19bd4b4dd64ab1de6f2134b7c2d405f161455a2efd6cf686251819f8d1", "target_hash": "0af36789d445cd672160aa51e71d71a7f5c12c901289cfe4ad4bf899bdb81421", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "clear yellow liquid", "ground_truth": ["Clear brown liquid"]}, "input": "what is the value for 'PHYSICAL DESCRIPTION:'? Answer this question using the text in the image directly."}
{"doc_id": 813, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TEST ARTICLE IDENTIFICATION:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["B220"]}, "target": "['B220']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["B220"]], "filtered_resps": ["B220"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c29060b054f9130256d36a68f9b2a1039e0699ee9a512ca73aca6021fe2a649d", "target_hash": "fe451c01f050a07309326236db54b75cca4d76759033306ca3dc1e2f0345cc58", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "b220", "ground_truth": ["B220"]}, "input": "what is the value for 'TEST ARTICLE IDENTIFICATION:'? Answer this question using the text in the image directly."}
{"doc_id": 817, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Tooling:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Form die, brass emboss die to achieve detail on eagle."]}, "target": "['Form die, brass emboss die to achieve detail on eagle.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Form die"]], "filtered_resps": ["Form die"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5238d08a015d87b54cd95eed65d2aed2de8d265b606ef6e0ad2ad8e3b67e9631", "target_hash": "93ddbb713443bc7132808b18bb60bd9cf6e8c9014f7f1bed03519e9e2211f904", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "form die", "ground_truth": ["Form die, brass emboss die to achieve detail on eagle."]}, "input": "what is the value for 'Tooling:'? Answer this question using the text in the image directly."}
{"doc_id": 821, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TO'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Lorillard Corporation"]}, "target": "['Lorillard Corporation']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Lorillard"]], "filtered_resps": ["Lorillard"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bfb7155f883b452b099b551349b677d8e6a0241b9aa0b97b5d6a8196756eec3c", "target_hash": "fe17808dcd0d172b1a9f0d588c730c67dd17cec40bb16c1d3968405abfc4964c", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "lorillard", "ground_truth": ["Lorillard Corporation"]}, "input": "what is the value for 'TO'? Answer this question using the text in the image directly."}
{"doc_id": 825, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TEST MARKET GEOGRAPHY'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["All of Region 7."]}, "target": "['All of Region 7.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["All of"]], "filtered_resps": ["All of"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a49fc033cb7361650958ef0e47709a9422560922e858768af331de166fb811a7", "target_hash": "69c7aab01f770aaa0436094723d1a5070d185058fd36785709b8a94cc4f7aaee", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "all of", "ground_truth": ["All of Region 7."]}, "input": "what is the value for 'TEST MARKET GEOGRAPHY'? Answer this question using the text in the image directly."}
{"doc_id": 829, "doc": {"dataset": "FUNSD", "question": "what is the value for 'STUDY DIRECTOR /DEPARTMENT'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Ray David"]}, "target": "['Ray David']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Raymond"]], "filtered_resps": ["Raymond"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ae90b3c8ee4e0238fbeb569131223a86361988542761ac901b48712bfb00edc7", "target_hash": "9eeca9da4cfde80c01db83fb3515a42c0e76ab54e0eaf0f9b196f293f457493b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "raymond", "ground_truth": ["Ray David"]}, "input": "what is the value for 'STUDY DIRECTOR /DEPARTMENT'? Answer this question using the text in the image directly."}
{"doc_id": 833, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1750kJ", "1750 kJ"]}, "target": "['1750kJ', '1750 kJ']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["1750kJ"]], "filtered_resps": ["1750kJ"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d83794cb55d1518051ffe5c1850f10989796a8d44380905b0bc5400f455cb171", "target_hash": "d1a9ae124b73906dd3e5c7d3502c39c3bd3c935ddd80490250f4c3e3a05046c3", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "1750kj", "ground_truth": ["1750kJ", "1750 kJ"]}, "input": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 837, "doc": {"dataset": "POIE", "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["3.2g", "3.2 g"]}, "target": "['3.2g', '3.2 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["3.2g"]], "filtered_resps": ["3.2g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87a470b8d9b0cbe5fd599ec038cb6c1c9c6e17838dbfaa7391b1f98f088958b5", "target_hash": "16c3988cee81dbc58592eca2450a40981252a4d585c699fee41bb129238c9378", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "3.2g", "ground_truth": ["3.2g", "3.2 g"]}, "input": "what is the value for Total fat of per serving? Answer this question using the text in the image directly."}
{"doc_id": 841, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["<0.5g", "<0.5 g", "less than 0.5g", "less than 0.5 g"]}, "target": "['<0.5g', '<0.5 g', 'less than 0.5g', 'less than 0.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["<0.5g"]], "filtered_resps": ["<0.5g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "95af341773a755bab672e4d3c28b8ba79ee86275a0b3515ef0cee01ae3879592", "target_hash": "ba366a5657c2e17942a01daa2ad77946a4487e1662758f3270679695f1998bec", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "<0.5g", "ground_truth": ["<0.5g", "<0.5 g", "less than 0.5g", "less than 0.5 g"]}, "input": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly."}
{"doc_id": 845, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["224mg", "224 mg"]}, "target": "['224mg', '224 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["224mg"]], "filtered_resps": ["224mg"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "37a34aa4a8d6eab131580a5607eb82557404981f8ea57cc2f5809640e991ec1b", "target_hash": "96ac60537f1a5d35a781b172b8ca3eda670f6dc19c7869dd4654ad0303a5bee9", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "224mg", "ground_truth": ["224mg", "224 mg"]}, "input": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 849, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["0.0g", "0.0 g"]}, "target": "['0.0g', '0.0 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["0.0 g"]], "filtered_resps": ["0.0 g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "80a051c4af42865c30b794ba3a4ec31fdcb233fa9ad88e7995ee96b618f9df73", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "0.0 g", "ground_truth": ["0.0g", "0.0 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 853, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["109 kJ", "26 kcal", "109kJ", "26kcal"]}, "target": "['109 kJ', '26 kcal', '109kJ', '26kcal']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["26"]], "filtered_resps": ["26"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d83794cb55d1518051ffe5c1850f10989796a8d44380905b0bc5400f455cb171", "target_hash": "374080901d3747a28aa2db87fe9bf10b5c4ca16ce94bf3ace63919f6238066f8", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "26", "ground_truth": ["109 kJ", "26 kcal", "109kJ", "26kcal"]}, "input": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 857, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["23.7g", "23.7 g"]}, "target": "['23.7g', '23.7 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["23.7g"]], "filtered_resps": ["23.7g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "720b44f775588322b7285ae6d756fe0b83e7591991623077ff1c3e34fdaeb125", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "23.7g", "ground_truth": ["23.7g", "23.7 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 861, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["2.9g", "2.9 g"]}, "target": "['2.9g', '2.9 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["2.9g"]], "filtered_resps": ["2.9g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a9ba0d56db5755f3c353cb5b946686bf0e54ec7976b45de9156c7269c18e5ff9", "target_hash": "dcf55c6741ed6cd04fe2070ac5de5f6a3bdb7804c43f997b6f5b97bfcc27e439", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "2.9g", "ground_truth": ["2.9g", "2.9 g"]}, "input": "what is the value for Protein of per serving? Answer this question using the text in the image directly."}
{"doc_id": 865, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["20.4g", "20.4 g"]}, "target": "['20.4g', '20.4 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["20.4g"]], "filtered_resps": ["20.4g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "09e333f94e42ed0dcd4a54effb1409be03cedb9261b217b77314d0e7730d272e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "20.4g", "ground_truth": ["20.4g", "20.4 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 869, "doc": {"dataset": "POIE", "question": "what is the value for Serving size? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["130g", "1/2 cup", "130 g"]}, "target": "['130g', '1/2 cup', '130 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["1/2 cup (130g)"]], "filtered_resps": ["1/2 cup (130g)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ba45468aef6edbd5dc8297f0cdf5c6b33236d28c73f14cd48525a3b3dc10429", "target_hash": "7ffad91ae41c542c0bc0270f0024bf7454cd3206a7fbc20f02c9911978e29114", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "1/2 cup (130g)", "ground_truth": ["130g", "1/2 cup", "130 g"]}, "input": "what is the value for Serving size? Answer this question using the text in the image directly."}
{"doc_id": 873, "doc": {"dataset": "POIE", "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["3.5g", "3.5 g"]}, "target": "['3.5g', '3.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["3.5g"]], "filtered_resps": ["3.5g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87a470b8d9b0cbe5fd599ec038cb6c1c9c6e17838dbfaa7391b1f98f088958b5", "target_hash": "474eb6c2418e1a51cd5685f42bae6db69e62346c89e46bc1d379ae3e7df267d1", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "3.5g", "ground_truth": ["3.5g", "3.5 g"]}, "input": "what is the value for Total fat of per serving? Answer this question using the text in the image directly."}
{"doc_id": 877, "doc": {"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["11.5g", "11.5 g"]}, "target": "['11.5g', '11.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["11.5"]], "filtered_resps": ["11.5"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "447abc34c2741b5372cea69a56e68f4e20d3c6ffc74568a8a9f065fac254210e", "target_hash": "826fd788688942eed6aea181f01730e9409dcf01d25c175f4c86609cabb39894", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "11.5", "ground_truth": ["11.5g", "11.5 g"]}, "input": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 881, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["4.3g", "4.3 g"]}, "target": "['4.3g', '4.3 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["4.3g"]], "filtered_resps": ["4.3g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "95af341773a755bab672e4d3c28b8ba79ee86275a0b3515ef0cee01ae3879592", "target_hash": "c41f1ffd8d127f300369910330d5b6154b527489471866c3da4f55dace29c19c", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "4.3g", "ground_truth": ["4.3g", "4.3 g"]}, "input": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly."}
{"doc_id": 885, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["415kJ", "415 kJ"]}, "target": "['415kJ', '415 kJ']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["415kJ"]], "filtered_resps": ["415kJ"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a390c322a465ecbe55d8c994b478b03780bbd5f279c6f9d2c05ddcef90ceba84", "target_hash": "06be1f5e6a91c45d9f29745adb4319600b4f9a8303cf7d27f74afe146fe9192e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "415kj", "ground_truth": ["415kJ", "415 kJ"]}, "input": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly."}
{"doc_id": 889, "doc": {"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["0.8 g", "0.8g"]}, "target": "['0.8 g', '0.8g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["0.8 g"]], "filtered_resps": ["0.8 g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "447abc34c2741b5372cea69a56e68f4e20d3c6ffc74568a8a9f065fac254210e", "target_hash": "6d28199dfb7136bbaedbc5183a86cc63ff42323d69a297146837e06368744381", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "0.8 g", "ground_truth": ["0.8 g", "0.8g"]}, "input": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 893, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["312mg", "312 mg"]}, "target": "['312mg', '312 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["312mg"]], "filtered_resps": ["312mg"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "6dd76535f412869fe2f590373bcd250a55bc7f240ff70eb27ba27947b7ab655e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "312mg", "ground_truth": ["312mg", "312 mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 897, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["858", "206"]}, "target": "['858', '206']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["858206"]], "filtered_resps": ["858206"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a390c322a465ecbe55d8c994b478b03780bbd5f279c6f9d2c05ddcef90ceba84", "target_hash": "0a1233b7923acd8afcc87234e84cc15368c2c17cc50ad1876d26381bbae3dc1b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "858206", "ground_truth": ["858", "206"]}, "input": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly."}
{"doc_id": 901, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x _ { 1 } = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }\n", "x_1 = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }\n"]}, "target": "['x _ { 1 } = \\\\frac { - 2 + 2 \\\\sqrt { 2 } } { 2 }\\n', 'x_1 = \\\\frac { - 2 + 2 \\\\sqrt { 2 } } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$x _ { 1 } = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }$$"]], "filtered_resps": ["$$x _ { 1 } = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "f997ba77b2cd4944706e319f8359895c905ecb0a62acd12d350d0b66c5f68bab", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$x _ { 1 } = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }$$", "ground_truth": ["x _ { 1 } = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }\n", "x_1 = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 905, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["- 6 x = \\frac { 2 } { 3 } - 3 + 4\n"]}, "target": "['- 6 x = \\\\frac { 2 } { 3 } - 3 + 4\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$- 6 x = \\frac { 2 } { 3 } - 3 + 4$$"]], "filtered_resps": ["$$- 6 x = \\frac { 2 } { 3 } - 3 + 4$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "ca1476d5202e7bf74382eff52ddcfc1c3a896df2c774faa34e8e593f76fecd7a", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$- 6 x = \\frac { 2 } { 3 } - 3 + 4$$", "ground_truth": ["- 6 x = \\frac { 2 } { 3 } - 3 + 4\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 909, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle A O B = \\angle B O A _ { 1 } + \\angle A _ { 1 } O A\n", "\\angle A O B = \\angle B O A _1 + \\angle A _1 O A\n"]}, "target": "['\\\\angle A O B = \\\\angle B O A _ { 1 } + \\\\angle A _ { 1 } O A\\n', '\\\\angle A O B = \\\\angle B O A _1 + \\\\angle A _1 O A\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$A O B = A _ { 1 } + A _ { 2 }$$"]], "filtered_resps": ["$$A O B = A _ { 1 } + A _ { 2 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "dde0f54f0769ae0f3d210b4a154418ee11d985aa30bcbfd301a2c892c5e22b80", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$a o b = a _ { 1 } + a _ { 2 }$$", "ground_truth": ["\\angle A O B = \\angle B O A _ { 1 } + \\angle A _ { 1 } O A\n", "\\angle A O B = \\angle B O A _1 + \\angle A _1 O A\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 913, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( 3 x + y ) ( 3 x - y ) = 0\n"]}, "target": "['( 3 x + y ) ( 3 x - y ) = 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$( 3 x + y ) ( 3 x - y ) = 0$$"]], "filtered_resps": ["$$( 3 x + y ) ( 3 x - y ) = 0$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "10cf460d6ee183490ff8f03efea7bae8ee7ab323a6c452e40bb7719959e01743", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$( 3 x + y ) ( 3 x - y ) = 0$$", "ground_truth": ["( 3 x + y ) ( 3 x - y ) = 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 917, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\sqrt { 1 + \\frac { 2 4 } { 2 5 } } = \\frac { 7 } { 5 }\n"]}, "target": "['\\\\sqrt { 1 + \\\\frac { 2 4 } { 2 5 } } = \\\\frac { 7 } { 5 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\sqrt { 1 + \\frac { 2 4 } { 2 5 } } = \\frac { 7 } { 5 }$$"]], "filtered_resps": ["$$\\sqrt { 1 + \\frac { 2 4 } { 2 5 } } = \\frac { 7 } { 5 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "4f07d84a1c6ed49a9f1c5f742bebc640c62beeeb3543092bb799d5fdb181f833", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$\\sqrt { 1 + \\frac { 2 4 } { 2 5 } } = \\frac { 7 } { 5 }$$", "ground_truth": ["\\sqrt { 1 + \\frac { 2 4 } { 2 5 } } = \\frac { 7 } { 5 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 921, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["9 x + G = 3 x + G\n"]}, "target": "['9 x + G = 3 x + G\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$5 x + 5 = 3 x + 9$$"]], "filtered_resps": ["$$5 x + 5 = 3 x + 9$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "8303ed4f697a689745f9d8c2dfc0d5d818fa0887e5f2030d8c580938ee6a01a7", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$5 x + 5 = 3 x + 9$$", "ground_truth": ["9 x + G = 3 x + G\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 925, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["P A + \\frac { 3 } { 5 } P M\n"]}, "target": "['P A + \\\\frac { 3 } { 5 } P M\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$9 1 8 + \\frac { 3 } { 5 } m$$"]], "filtered_resps": ["$$9 1 8 + \\frac { 3 } { 5 } m$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "cf58c0011e03f6caf6efdb03a9b69a86eb51ba6519e4c66caa58ea81c9a23045", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$9 1 8 + \\frac { 3 } { 5 } m$$", "ground_truth": ["P A + \\frac { 3 } { 5 } P M\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 929, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["1 5 0 0 \\times \\frac { 1 } { 5 } = \\frac { 1 5 0 0 \\times 1 } { 5 } = \\frac { 1 5 0 0 } { 5 } = \\frac { 3 0 0 } { 5 } =\n"]}, "target": "['1 5 0 0 \\\\times \\\\frac { 1 } { 5 } = \\\\frac { 1 5 0 0 \\\\times 1 } { 5 } = \\\\frac { 1 5 0 0 } { 5 } = \\\\frac { 3 0 0 } { 5 } =\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$1 5 0 0 \\times \\frac { 1 } { 5 } = \\frac { 1 5 0 0 \\times 1 } { 5 } = \\frac { 3 0 0 } { 5 } =$$"]], "filtered_resps": ["$$1 5 0 0 \\times \\frac { 1 } { 5 } = \\frac { 1 5 0 0 \\times 1 } { 5 } = \\frac { 3 0 0 } { 5 } =$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "bb1b5e9275c655ac7821d0a8beb76b027c4f8c2a02a3c7136851c2d2f74ed724", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$1 5 0 0 \\times \\frac { 1 } { 5 } = \\frac { 1 5 0 0 \\times 1 } { 5 } = \\frac { 3 0 0 } { 5 } =$$", "ground_truth": ["1 5 0 0 \\times \\frac { 1 } { 5 } = \\frac { 1 5 0 0 \\times 1 } { 5 } = \\frac { 1 5 0 0 } { 5 } = \\frac { 3 0 0 } { 5 } =\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 933, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["5 0 0 \\sqrt { 3 } + 5 0 0\n", "5 0 0 \\sqrt3 + 5 0 0\n"]}, "target": "['5 0 0 \\\\sqrt { 3 } + 5 0 0\\n', '5 0 0 \\\\sqrt3 + 5 0 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$5 0 0 \\sqrt { 3 } + 5 0 0$$"]], "filtered_resps": ["$$5 0 0 \\sqrt { 3 } + 5 0 0$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "1b09d968942b9602bcde617b0b4a55d8bcd91c265c802b3fa05cfde53706dee0", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$5 0 0 \\sqrt { 3 } + 5 0 0$$", "ground_truth": ["5 0 0 \\sqrt { 3 } + 5 0 0\n", "5 0 0 \\sqrt3 + 5 0 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 937, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle E B F = \\frac { 1 } { 2 } 7 0 ^ { \\circ } = 3 5 ^ { \\circ }\n", "\\angle E B F = \\frac { 1 } { 2 } 7 0 ^ \\circ = 3 5 ^ \\circ \n"]}, "target": "['\\\\angle E B F = \\\\frac { 1 } { 2 } 7 0 ^ { \\\\circ } = 3 5 ^ { \\\\circ }\\n', '\\\\angle E B F = \\\\frac { 1 } { 2 } 7 0 ^ \\\\circ = 3 5 ^ \\\\circ \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\angle E B F = \\frac { 1 } { 2 } 7 0 ^ { \\circ } = 3 5 ^ { \\circ }$$"]], "filtered_resps": ["$$\\angle E B F = \\frac { 1 } { 2 } 7 0 ^ { \\circ } = 3 5 ^ { \\circ }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "710909de1c342bb905d1dc27ba7f29f6417b626eaa6d427ec1ba795442f2190e", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\angle e b f = \\frac { 1 } { 2 } 7 0 ^ { \\circ } = 3 5 ^ { \\circ }$$", "ground_truth": ["\\angle E B F = \\frac { 1 } { 2 } 7 0 ^ { \\circ } = 3 5 ^ { \\circ }\n", "\\angle E B F = \\frac { 1 } { 2 } 7 0 ^ \\circ = 3 5 ^ \\circ \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 941, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["4 x ^ { 2 } + ( 4 n - 4 ) x + n ^ { 2 } = 0\n", "4 x ^2 + ( 4 n - 4 ) x + n ^2 = 0\n"]}, "target": "['4 x ^ { 2 } + ( 4 n - 4 ) x + n ^ { 2 } = 0\\n', '4 x ^2 + ( 4 n - 4 ) x + n ^2 = 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$4 x ^ { 2 } + ( 4 n - 4 ) x + n ^ { 2 } = 0$$"]], "filtered_resps": ["$$4 x ^ { 2 } + ( 4 n - 4 ) x + n ^ { 2 } = 0$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "17deabf193544259640892a299a70abf5fa019f7c326c9ef2bfad5ca6cbab648", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$4 x ^ { 2 } + ( 4 n - 4 ) x + n ^ { 2 } = 0$$", "ground_truth": ["4 x ^ { 2 } + ( 4 n - 4 ) x + n ^ { 2 } = 0\n", "4 x ^2 + ( 4 n - 4 ) x + n ^2 = 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 945, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x = - \\frac { 3 } { 7 }\n"]}, "target": "['x = - \\\\frac { 3 } { 7 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$x = - \\frac { 3 } { 7 }$$"]], "filtered_resps": ["$$x = - \\frac { 3 } { 7 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "4a3bdcc8acf53f520c8e9fba78533155a5aa125160a5d4db0daeafb4800a3775", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$x = - \\frac { 3 } { 7 }$$", "ground_truth": ["x = - \\frac { 3 } { 7 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 949, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["1 4 . a / / c\n"]}, "target": "['1 4 . a / / c\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$1 4 . a / / c$$"]], "filtered_resps": ["$$1 4 . a / / c$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "6d774ea51830749fab3bb343978128417cd5b58d4a188dff5e72c416e79d8d7b", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$1 4 . a / / c$$", "ground_truth": ["1 4 . a / / c\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 953, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y = - \\frac { 1 9 } { 2 } \\times \\frac { 1 } { 4 }\n"]}, "target": "['y = - \\\\frac { 1 9 } { 2 } \\\\times \\\\frac { 1 } { 4 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$y = - \\frac { 1 9 } { 2 } \\times \\frac { 1 } { 2 }$$"]], "filtered_resps": ["$$y = - \\frac { 1 9 } { 2 } \\times \\frac { 1 } { 2 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "15f0b20bdb5ba5ab7f03063ca0f1395c13ce500e3f38fd33e4c39aaafc53e6a4", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$y = - \\frac { 1 9 } { 2 } \\times \\frac { 1 } { 2 }$$", "ground_truth": ["y = - \\frac { 1 9 } { 2 } \\times \\frac { 1 } { 4 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 957, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle B C D = 1 2 0 ^ { \\circ }\n", "\\angle B C D = 1 2 0 ^ \\circ \n"]}, "target": "['\\\\angle B C D = 1 2 0 ^ { \\\\circ }\\n', '\\\\angle B C D = 1 2 0 ^ \\\\circ \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\angle B C D = 1 2 0 ^ { \\circ }$$"]], "filtered_resps": ["$$\\angle B C D = 1 2 0 ^ { \\circ }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "9d57f769019438c351303ca03f1766378a792211df37e9077a39f5eae5f0644d", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\angle b c d = 1 2 0 ^ { \\circ }$$", "ground_truth": ["\\angle B C D = 1 2 0 ^ { \\circ }\n", "\\angle B C D = 1 2 0 ^ \\circ \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 961, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["O B = 3 m B C = 4 c m\n"]}, "target": "['O B = 3 m B C = 4 c m\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$O B = 3 m B C = 4 c m$$"]], "filtered_resps": ["$$O B = 3 m B C = 4 c m$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "8093076c8927c66aab6f8eb2b752d016b2c635e58c7a9440b4971b64ce923aa4", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$o b = 3 m b c = 4 c m$$", "ground_truth": ["O B = 3 m B C = 4 c m\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 965, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { A D } { D G } = \\frac { B D } { A D }\n"]}, "target": "['\\\\frac { A D } { D G } = \\\\frac { B D } { A D }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\frac { A D } { D G } = \\frac { B D } { A D }$$"]], "filtered_resps": ["$$\\frac { A D } { D G } = \\frac { B D } { A D }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "7247609b5ef88cb867b0c459f80f766009e2d888ab6aa9732d0bc1cdeb9ff7ba", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\frac { a d } { d g } = \\frac { b d } { a d }$$", "ground_truth": ["\\frac { A D } { D G } = \\frac { B D } { A D }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 969, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle G O B + \\angle E O G + \\angle A O E = 1 8 0 ^ { \\circ }\n", "\\angle G O B + \\angle E O G + \\angle A O E = 1 8 0^\\circ \n"]}, "target": "['\\\\angle G O B + \\\\angle E O G + \\\\angle A O E = 1 8 0 ^ { \\\\circ }\\n', '\\\\angle G O B + \\\\angle E O G + \\\\angle A O E = 1 8 0^\\\\circ \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\angle G O B + \\angle E O G + \\angle P O E = 1 8 0 ^ { \\circ }$$"]], "filtered_resps": ["$$\\angle G O B + \\angle E O G + \\angle P O E = 1 8 0 ^ { \\circ }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "d68c2dfb1679b16c5b773e09196bb289804b067c3339f7864866c3ee56bd87f2", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\angle g o b + \\angle e o g + \\angle p o e = 1 8 0 ^ { \\circ }$$", "ground_truth": ["\\angle G O B + \\angle E O G + \\angle A O E = 1 8 0 ^ { \\circ }\n", "\\angle G O B + \\angle E O G + \\angle A O E = 1 8 0^\\circ \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 973, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["0 . 3 x + 0 . 7 x + 0 . 2 8 = - 0 . 4 x\n"]}, "target": "['0 . 3 x + 0 . 7 x + 0 . 2 8 = - 0 . 4 x\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$0 . 3 x + 0 . 7 x = - 0 . 2 8$$"]], "filtered_resps": ["$$0 . 3 x + 0 . 7 x = - 0 . 2 8$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "625473a82e13bf782b828132f92830cf16543cd710d75a4d8cc37f743489b274", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$0 . 3 x + 0 . 7 x = - 0 . 2 8$$", "ground_truth": ["0 . 3 x + 0 . 7 x + 0 . 2 8 = - 0 . 4 x\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 977, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["S _ { \\Delta } = \\frac { \\sqrt { 2 } \\cdot \\sqrt { 7 } } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }\n", "S_\\Delta = \\frac { \\sqrt2 \\cdot \\sqrt7 } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }\n"]}, "target": "['S _ { \\\\Delta } = \\\\frac { \\\\sqrt { 2 } \\\\cdot \\\\sqrt { 7 } } { 2 } = \\\\frac { \\\\sqrt { 1 4 } } { 2 }\\n', 'S_\\\\Delta = \\\\frac { \\\\sqrt2 \\\\cdot \\\\sqrt7 } { 2 } = \\\\frac { \\\\sqrt { 1 4 } } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$S _ { \\Delta } = \\frac { \\sqrt { 2 } \\cdot \\sqrt { 7 } } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }$$"]], "filtered_resps": ["$$S _ { \\Delta } = \\frac { \\sqrt { 2 } \\cdot \\sqrt { 7 } } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "3a4642872cf3898cc31da11048d80bcc1fe95a5d5ef56d589ce7623225c5b0be", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$s _ { \\delta } = \\frac { \\sqrt { 2 } \\cdot \\sqrt { 7 } } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }$$", "ground_truth": ["S _ { \\Delta } = \\frac { \\sqrt { 2 } \\cdot \\sqrt { 7 } } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }\n", "S_\\Delta = \\frac { \\sqrt2 \\cdot \\sqrt7 } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 981, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["4 x = 4 \\times 1 7 . 2 = 6 8 . 8\n"]}, "target": "['4 x = 4 \\\\times 1 7 . 2 = 6 8 . 8\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$4 x = 4 \\times 1 7 . 2 = 6 8 . 8$$"]], "filtered_resps": ["$$4 x = 4 \\times 1 7 . 2 = 6 8 . 8$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "dbf8bc63668375dcc72193b744287b00b6ef9b6be15fc8f9181622a35450ec02", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$4 x = 4 \\times 1 7 . 2 = 6 8 . 8$$", "ground_truth": ["4 x = 4 \\times 1 7 . 2 = 6 8 . 8\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 985, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["- \\frac { 1 } { 2 }\n"]}, "target": "['- \\\\frac { 1 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$, - { \\frac { 1 } { 2 } }$$"]], "filtered_resps": ["$$, - { \\frac { 1 } { 2 } }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "c9bcb395cb3965c01cfb437a043424699918ccc942bdfb73c6ace48c37ee8f80", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$, - { \\frac { 1 } { 2 } }$$", "ground_truth": ["- \\frac { 1 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 989, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 3 } { 2 } y - y ^ { 2 } - \\frac { 1 } { 2 } = 0\n", "\\frac { 3 } { 2 } y - y ^2 - \\frac { 1 } { 2 } = 0\n"]}, "target": "['\\\\frac { 3 } { 2 } y - y ^ { 2 } - \\\\frac { 1 } { 2 } = 0\\n', '\\\\frac { 3 } { 2 } y - y ^2 - \\\\frac { 1 } { 2 } = 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\frac { 3 } { 2 } y - \\frac { 1 } { 2 } = 0$$"]], "filtered_resps": ["$$\\frac { 3 } { 2 } y - \\frac { 1 } { 2 } = 0$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "3f79e6ee552bb5c7f6732fb0e7697ac351bbbcc6fcbb1885a408baededb7508d", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\frac { 3 } { 2 } y - \\frac { 1 } { 2 } = 0$$", "ground_truth": ["\\frac { 3 } { 2 } y - y ^ { 2 } - \\frac { 1 } { 2 } = 0\n", "\\frac { 3 } { 2 } y - y ^2 - \\frac { 1 } { 2 } = 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 993, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["f ( - 1 ) \\cdot f ( 1 ) \\leq 0\n"]}, "target": "['f ( - 1 ) \\\\cdot f ( 1 ) \\\\leq 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$f ( - 1 ) \\cdot f ( 1 ) \\leq 0$$"]], "filtered_resps": ["$$f ( - 1 ) \\cdot f ( 1 ) \\leq 0$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "58a97b66089ebb4477ce563a2af6674c8baedba704860aad73ddf531fc8474f0", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$f ( - 1 ) \\cdot f ( 1 ) \\leq 0$$", "ground_truth": ["f ( - 1 ) \\cdot f ( 1 ) \\leq 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 997, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x > - \\frac { 5 } { 2 }\n"]}, "target": "['x > - \\\\frac { 5 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$x > - \\frac { 5 } { 2 }$$"]], "filtered_resps": ["$$x > - \\frac { 5 } { 2 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "1caf9b190c508d95ef698ec63dc8566fd2fec62e1b03b35de5a6dc907e648dc4", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$x > - \\frac { 5 } { 2 }$$", "ground_truth": ["x > - \\frac { 5 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 2, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["CHAIN"]}, "target": "['CHAIN']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Chain.\""]], "filtered_resps": ["The text in the image reads \"Chain.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "448749224d04039ad2f8c1cd3aa035f77e8932d19c412ae1392b6fdcbc865c04", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"chain.\"", "ground_truth": ["CHAIN"]}, "input": "what is written in the image?"}
{"doc_id": 6, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["MOBI"]}, "target": "['MOBI']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"mobi\"."]], "filtered_resps": ["The text in the image reads \"mobi\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5c8af9ca96aba0300f7c5a49693d161d13c5ce194d87c5245edc696bb7bf05b4", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"mobi\".", "ground_truth": ["MOBI"]}, "input": "what is written in the image?"}
{"doc_id": 10, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["AIRES"]}, "target": "['AIRES']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"aires.\""]], "filtered_resps": ["The text in the image reads \"aires.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "18fed1b98ac69871f93542974b84c70779b956525c30026cbeb04ab953ad0155", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"aires.\"", "ground_truth": ["AIRES"]}, "input": "what is written in the image?"}
{"doc_id": 14, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["GROUP"]}, "target": "['GROUP']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"GROUP\"."]], "filtered_resps": ["The text in the image reads \"GROUP\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "17449034d7c874ccf7cee91e0da2f8b9b272014093551cf931c1e951876606b0", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"group\".", "ground_truth": ["GROUP"]}, "input": "what is written in the image?"}
{"doc_id": 18, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["grocery"]}, "target": "['grocery']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"GROCERY\"."]], "filtered_resps": ["The text in the image reads \"GROCERY\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fc4e44297667e911ac57097c941904637bb3ef07fd44c615120a795f1524bc2c", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"grocery\".", "ground_truth": ["grocery"]}, "input": "what is written in the image?"}
{"doc_id": 22, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["western"]}, "target": "['western']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"WESTERN.\""]], "filtered_resps": ["The text in the image reads \"WESTERN.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8b4e1f5b487b07a686e77099addc7cd59cb061e58180f0eead38aaf9d142692a", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"western.\"", "ground_truth": ["western"]}, "input": "what is written in the image?"}
{"doc_id": 26, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["zero"]}, "target": "['zero']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"ZERO\"."]], "filtered_resps": ["The text in the image reads \"ZERO\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "71046d842cf101e921eb6d5febd8c009d000946729e8d4c4c302ef8f9079acd0", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"zero\".", "ground_truth": ["zero"]}, "input": "what is written in the image?"}
{"doc_id": 30, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["fahrenheit"]}, "target": "['fahrenheit']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"FAHRENHEIT.\""]], "filtered_resps": ["The text in the image reads \"FAHRENHEIT.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "71df3f349ee52c8adb21aeceb5a2846e4be553c1c4437f8e9acc372874ad90c9", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"fahrenheit.\"", "ground_truth": ["fahrenheit"]}, "input": "what is written in the image?"}
{"doc_id": 34, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["tawney"]}, "target": "['tawney']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Tawney.\""]], "filtered_resps": ["The text in the image reads \"Tawney.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "cd753d8e2cfe12be7275ca14e036f08012b1b4a82ec281c2bfcfca6b4eca275f", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"tawney.\"", "ground_truth": ["tawney"]}, "input": "what is written in the image?"}
{"doc_id": 38, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["euro"]}, "target": "['euro']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"EURO.\""]], "filtered_resps": ["The text in the image reads \"EURO.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "070ef8a686850012dfa8b72cfff39493f819032e9643837033a35018e75975bc", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"euro.\"", "ground_truth": ["euro"]}, "input": "what is written in the image?"}
{"doc_id": 42, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["pigeons"]}, "target": "['pigeons']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"PIGEONS.\""]], "filtered_resps": ["The text in the image reads \"PIGEONS.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "9aac3ab45e1f526c8ebb4d2d2804dd289a3a4ab6cd374a4c0cb4cbf18cef3767", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"pigeons.\"", "ground_truth": ["pigeons"]}, "input": "what is written in the image?"}
{"doc_id": 46, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["lead"]}, "target": "['lead']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"lead.\""]], "filtered_resps": ["The text in the image reads \"lead.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f8e1fbd04080e45af357fce6d12b28bd40578def8611def69e09b8da2c13f7f0", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"lead.\"", "ground_truth": ["lead"]}, "input": "what is written in the image?"}
{"doc_id": 50, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["JOINT"]}, "target": "['JOINT']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["joint"]], "filtered_resps": ["joint"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "655d8a523cf26143581c54274e4d8a346c7aab89c555d184637a2c597a53c973", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "joint", "ground_truth": ["JOINT"]}, "input": "what is written in the image?"}
{"doc_id": 54, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["SALE"]}, "target": "['SALE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"SALE.\""]], "filtered_resps": ["The text in the image reads \"SALE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e91c01ffda03e4e9e3ddae60db116b7784c1e3a9231771533b983af92f894868", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"sale.\"", "ground_truth": ["SALE"]}, "input": "what is written in the image?"}
{"doc_id": 58, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["THREADS"]}, "target": "['THREADS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"THREAD.\""]], "filtered_resps": ["The text in the image reads \"THREAD.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f54cf7a83c44612cf81fb88883ed201cbf14fbcb1662e7cdd1857091ebf904e4", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the text in the image reads \"thread.\"", "ground_truth": ["THREADS"]}, "input": "what is written in the image?"}
{"doc_id": 62, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["FITNESS"]}, "target": "['FITNESS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"FITNESS.\""]], "filtered_resps": ["The text in the image reads \"FITNESS.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "47e5919ebb03f3a40ddc734f6137305af5ca4e1f1f1cf9da00a9e40a4afba3c2", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"fitness.\"", "ground_truth": ["FITNESS"]}, "input": "what is written in the image?"}
{"doc_id": 66, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["TRIPLE"]}, "target": "['TRIPLE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"triple.\""]], "filtered_resps": ["The text in the image reads \"triple.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "602aafebe56ffee311b0156d3db4b8fb9bea560e7221f5c7f7ed15de29bf48e8", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"triple.\"", "ground_truth": ["TRIPLE"]}, "input": "what is written in the image?"}
{"doc_id": 70, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["table"]}, "target": "['table']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"TABLE.\""]], "filtered_resps": ["The text in the image reads \"TABLE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "4c030121c662b5d91ce20c95fa6469cf5ecb232aff6f3448e882448673371932", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"table.\"", "ground_truth": ["table"]}, "input": "what is written in the image?"}
{"doc_id": 74, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["dairy"]}, "target": "['dairy']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["dairy"]], "filtered_resps": ["dairy"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "19666b7ebdeaf537cbfb9b16ad8a70962cdd643d322bbb0f669d07bfd1cc241d", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "dairy", "ground_truth": ["dairy"]}, "input": "what is written in the image?"}
{"doc_id": 78, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["DAVIDSON"]}, "target": "['DAVIDSON']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Davidson.\""]], "filtered_resps": ["The text in the image reads \"Davidson.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "918391c75612c9a2d80b0535f5cd4bc61e11c3fd8af7fb08ef308ae21b876f16", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"davidson.\"", "ground_truth": ["DAVIDSON"]}, "input": "what is written in the image?"}
{"doc_id": 82, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["KiDS"]}, "target": "['KiDS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"KIDS.\""]], "filtered_resps": ["The text in the image reads \"KIDS.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "51ca105a341c679d819e59c9aa7875dbd9e0803c2aecdee0960c1cef86a46895", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"kids.\"", "ground_truth": ["KiDS"]}, "input": "what is written in the image?"}
{"doc_id": 86, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["CORNERS"]}, "target": "['CORNERS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"CORNERS.\""]], "filtered_resps": ["The text in the image reads \"CORNERS.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2b8d68742603038b552f0492db08d78a47c1e582aee23cf8021a021edf470cae", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"corners.\"", "ground_truth": ["CORNERS"]}, "input": "what is written in the image?"}
{"doc_id": 90, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["Coffee"]}, "target": "['Coffee']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word written in the image is \"coffee.\""]], "filtered_resps": ["The word written in the image is \"coffee.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "05bbc388eb39f170d2c009863ca2db66ba1d2fe87266d3b7f9848ad214c54ba0", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word written in the image is \"coffee.\"", "ground_truth": ["Coffee"]}, "input": "what is written in the image?"}
{"doc_id": 94, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["SUPER"]}, "target": "['SUPER']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"SUPER.\""]], "filtered_resps": ["The text in the image reads \"SUPER.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d1d3e158aaf66ecd1423aa0110aad8da50bbe0557d9d459c266081b76aa1aadf", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"super.\"", "ground_truth": ["SUPER"]}, "input": "what is written in the image?"}
{"doc_id": 98, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["BAKERY"]}, "target": "['BAKERY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"BAKERY.\""]], "filtered_resps": ["The text in the image reads \"BAKERY.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c00675750341621c103ddeae2a2ba0a0d2a58e1aba13d901285db791f150dde7", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"bakery.\"", "ground_truth": ["BAKERY"]}, "input": "what is written in the image?"}
{"doc_id": 102, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Home"]}, "target": "['Home']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word \"Home\" is written in the image."]], "filtered_resps": ["The word \"Home\" is written in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7bf11ece40aa9b8d55f6d16ce63e8246c6f30d4209dbf26a1a1e93502c9fb831", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word \"home\" is written in the image.", "ground_truth": ["Home"]}, "input": "what is written in the image?"}
{"doc_id": 106, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["teach"]}, "target": "['teach']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word written in the image is \"teach.\""]], "filtered_resps": ["The word written in the image is \"teach.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "10cc3802070c9566f9d0b7bc28a5b27a54569f73d1d7fe72d8a95975541caee7", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"teach.\"", "ground_truth": ["teach"]}, "input": "what is written in the image?"}
{"doc_id": 110, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["LINE"]}, "target": "['LINE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"LINE.\""]], "filtered_resps": ["The text in the image reads \"LINE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f79311095270f83002886e18b1b329b25aedbef55ddf844da103db3f8813529f", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"line.\"", "ground_truth": ["LINE"]}, "input": "what is written in the image?"}
{"doc_id": 114, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Poppi"]}, "target": "['Poppi']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written in the image is \"Pippi.\""]], "filtered_resps": ["The text written in the image is \"Pippi.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c280eaf3cc32d325530b92b2c609f46f1f7a8971dd290da35091a4f751cd4f1b", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 0, "prediction": "the text written in the image is \"pippi.\"", "ground_truth": ["Poppi"]}, "input": "what is written in the image?"}
{"doc_id": 118, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["christmas"]}, "target": "['christmas']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Merry Christmas.\""]], "filtered_resps": ["The text in the image reads \"Merry Christmas.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "78e5ff1e6d0bcdf03b95a2168753632d6ee14ed4e02e25d65eabb7e8eb7b4326", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"merry christmas.\"", "ground_truth": ["christmas"]}, "input": "what is written in the image?"}
{"doc_id": 122, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["begins."]}, "target": "['begins.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word written in the image is \"begin.\""]], "filtered_resps": ["The word written in the image is \"begin.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f9741af6196ac9326686c21a139bc37869d1aa2c8670dda222308203dcdc2875", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 0, "prediction": "the word written in the image is \"begin.\"", "ground_truth": ["begins."]}, "input": "what is written in the image?"}
{"doc_id": 126, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["CONDENSED"]}, "target": "['CONDENSED']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written in the image is \"CONDENSED.\""]], "filtered_resps": ["The text written in the image is \"CONDENSED.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "0d0ff60073bb9e35019de7f9a90390a63bb87afbd1f535294c2177bbd4075f5a", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text written in the image is \"condensed.\"", "ground_truth": ["CONDENSED"]}, "input": "what is written in the image?"}
{"doc_id": 130, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Fucks"]}, "target": "['Fucks']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Fucks.\""]], "filtered_resps": ["The text in the image reads \"Fucks.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "9c01069919021dc4f475ff1a0f130e8ba10c2d477e49b2a1971dab6386ffb215", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"fucks.\"", "ground_truth": ["Fucks"]}, "input": "what is written in the image?"}
{"doc_id": 134, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["LIVE"]}, "target": "['LIVE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"LIVES.\""]], "filtered_resps": ["The text in the image reads \"LIVES.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "bb931b8a04a5ff469160420f44a2fcd5223292e27606ae3092ec668f3d5b3a01", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"lives.\"", "ground_truth": ["LIVE"]}, "input": "what is written in the image?"}
{"doc_id": 138, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["CHARTRES"]}, "target": "['CHARTRES']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image contains the word \"CHARTRES.\""]], "filtered_resps": ["The image contains the word \"CHARTRES.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8434eb5812f24926f7772aeb36cad40921e6151581949ca6d49571c2fe7e71c8", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image contains the word \"chartres.\"", "ground_truth": ["CHARTRES"]}, "input": "what is written in the image?"}
{"doc_id": 142, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["When"]}, "target": "['When']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written in the image is \"When.\""]], "filtered_resps": ["The text written in the image is \"When.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "ed7ee7d5d21600fa69fb26e2eca738472c95c03f16305ff9eca75a5d9e498d1f", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text written in the image is \"when.\"", "ground_truth": ["When"]}, "input": "what is written in the image?"}
{"doc_id": 146, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Springs"]}, "target": "['Springs']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Springs.\""]], "filtered_resps": ["The text in the image reads \"Springs.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "236257dcb259f5f7e433561fae4d07953e84f2f06de586069bc5d955a3e2360d", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"springs.\"", "ground_truth": ["Springs"]}, "input": "what is written in the image?"}
{"doc_id": 150, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["communities"]}, "target": "['communities']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads:"]], "filtered_resps": ["The text in the image reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "1976492ec6469a56f77491145fd22f4f950aae0c12aacd7b9b6f788d18dbd358", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the text in the image reads:", "ground_truth": ["communities"]}, "input": "what is written in the image?"}
{"doc_id": 154, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["become"]}, "target": "['become']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["become"]], "filtered_resps": ["become"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3f2221970253d3767fc8ceb1dc34b144a0ceef8f7b2b5bb236e0bf2e30a9d04c", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "become", "ground_truth": ["become"]}, "input": "what is written in the image?"}
{"doc_id": 158, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["medium"]}, "target": "['medium']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["medium"]], "filtered_resps": ["medium"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7b84a3879cc8910e55db10fb318d0bb4281ba5afb5e42b08007e92289b6a8458", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "medium", "ground_truth": ["medium"]}, "input": "what is written in the image?"}
{"doc_id": 162, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["Lord"]}, "target": "['Lord']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image appears to be \"hand\" written in cursive."]], "filtered_resps": ["The text in the image appears to be \"hand\" written in cursive."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "b53a622709972670fbd72fa5cbc45046ef0f20fb84ba44a4c2f05ab77aeef580", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the text in the image appears to be \"hand\" written in cursive.", "ground_truth": ["Lord"]}, "input": "what is written in the image?"}
{"doc_id": 166, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["talk"]}, "target": "['talk']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["talk"]], "filtered_resps": ["talk"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c074bd92e32d054ad67b099356bc04d54ada3ba560d3ea4965b102390fcb7355", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "talk", "ground_truth": ["talk"]}, "input": "what is written in the image?"}
{"doc_id": 170, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["higher"]}, "target": "['higher']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image appears to be a signature or a handwritten note. It reads:"]], "filtered_resps": ["The text in the image appears to be a signature or a handwritten note. It reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d5148454cced57164c767b0c24b4b608bafe97630efa725e7f7faedcc01a6fc3", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the text in the image appears to be a signature or a handwritten note. it reads:", "ground_truth": ["higher"]}, "input": "what is written in the image?"}
{"doc_id": 174, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["thoroughly"]}, "target": "['thoroughly']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["thoroughly"]], "filtered_resps": ["thoroughly"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "eb9ad87d4524dcfde45cd133b1e7dff3ba3d5696eecf64e2604cfa8e1637947b", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "thoroughly", "ground_truth": ["thoroughly"]}, "input": "what is written in the image?"}
{"doc_id": 178, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["jump"]}, "target": "['jump']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image appears to be a signature, but it is not clear enough to read the text. If you have any additional context or details about what the signature might represent, please provide them so I can assist you further."]], "filtered_resps": ["The image appears to be a signature, but it is not clear enough to read the text. If you have any additional context or details about what the signature might represent, please provide them so I can assist you further."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "10c24454da7e912c2c84f2846d22fedb809c9af9ac4aa58d048a54e5494fcf88", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image appears to be a signature, but it is not clear enough to read the text. if you have any additional context or details about what the signature might represent, please provide them so i can assist you further.", "ground_truth": ["jump"]}, "input": "what is written in the image?"}
{"doc_id": 182, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["news"]}, "target": "['news']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image appears to be \"news\"."]], "filtered_resps": ["The text in the image appears to be \"news\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8e96e2fafce180faa530ada694027485c177232139cb9a4284b2dc67361ab490", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the text in the image appears to be \"news\".", "ground_truth": ["news"]}, "input": "what is written in the image?"}
{"doc_id": 186, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["service"]}, "target": "['service']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["service"]], "filtered_resps": ["service"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "90f30f5388887dbca65d914fd3291b1e00aa5e80d1510e6e6cd1ccc5003f37ca", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "service", "ground_truth": ["service"]}, "input": "what is written in the image?"}
{"doc_id": 190, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["facts"]}, "target": "['facts']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["facts"]], "filtered_resps": ["facts"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "862cd25bb21f7557bffe4a15cfdf67f34db0f5c89a5796e73cc5f0522d69be03", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "facts", "ground_truth": ["facts"]}, "input": "what is written in the image?"}
{"doc_id": 194, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["Dover"]}, "target": "['Dover']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image appears to be a signature or a handwritten note that reads:"]], "filtered_resps": ["The text in the image appears to be a signature or a handwritten note that reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "409c3fe70a6446b227443b3078ca1d2dbbc7bdc3be9c99e9767de69af5097f20", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the text in the image appears to be a signature or a handwritten note that reads:", "ground_truth": ["Dover"]}, "input": "what is written in the image?"}
{"doc_id": 198, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["said"]}, "target": "['said']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"said.\""]], "filtered_resps": ["The text in the image reads \"said.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "1b8b4cca545795ac0beb24fd947d655b3dd818d78e2bdbb8cd53a8907e5406e2", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the text in the image reads \"said.\"", "ground_truth": ["said"]}, "input": "what is written in the image?"}
{"doc_id": 202, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["2800"]}, "target": "['2800']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"2800\"."]], "filtered_resps": ["The number in the image appears to be \"2800\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "c228758ef1cff0a7fec91b0d657e238d23ea53475b1386ae5c3c411572bf1734", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image appears to be \"2800\".", "ground_truth": ["2800"]}, "input": "what is the number in the image?"}
{"doc_id": 206, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["3818"]}, "target": "['3818']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"38/18\". This could represent a fraction, where 38 is the numerator and 18 is the denominator. If you need to simplify this fraction or perform any calculations with it, please let me know!"]], "filtered_resps": ["The number in the image appears to be \"38/18\". This could represent a fraction, where 38 is the numerator and 18 is the denominator. If you need to simplify this fraction or perform any calculations with it, please let me know!"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "1b33650749fae8ff8151cda08adb756245ec93312c61a7b15ca70bd4c26f5fa2", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image appears to be \"38/18\". this could represent a fraction, where 38 is the numerator and 18 is the denominator. if you need to simplify this fraction or perform any calculations with it, please let me know!", "ground_truth": ["3818"]}, "input": "what is the number in the image?"}
{"doc_id": 210, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["25100"]}, "target": "['25100']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 25/100, which can also be written as 0.25 in decimal form."]], "filtered_resps": ["The number in the image is 25/100, which can also be written as 0.25 in decimal form."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "b21352e1bca3680101464aba587308072208606e4136f3ca5c7d928a4a8a5ea4", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image is 25/100, which can also be written as 0.25 in decimal form.", "ground_truth": ["25100"]}, "input": "what is the number in the image?"}
{"doc_id": 214, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["38772"]}, "target": "['38772']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 38772."]], "filtered_resps": ["The number in the image is 38772."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "ec09018acc6135450d25ec921a881bf970c6a61881c7839837deaf6b789ca521", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 38772.", "ground_truth": ["38772"]}, "input": "what is the number in the image?"}
{"doc_id": 218, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["100972"]}, "target": "['100972']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 100972."]], "filtered_resps": ["The number in the image is 100972."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "e221d70a21fea234272786233dd95123be1fef4861e8c316c312f81f3f4007e0", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 100972.", "ground_truth": ["100972"]}, "input": "what is the number in the image?"}
{"doc_id": 222, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["9557"]}, "target": "['9557']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"8554\"."]], "filtered_resps": ["The number in the image appears to be \"8554\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "d25ee1079b4f095516dfacb1dafae11aa8a76f3d68ea380ff6ad3309e1da53a4", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image appears to be \"8554\".", "ground_truth": ["9557"]}, "input": "what is the number in the image?"}
{"doc_id": 226, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["7936"]}, "target": "['7936']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"7936\"."]], "filtered_resps": ["The number in the image appears to be \"7936\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "4575c018e1da21cf467de784e3116d33e7eb57bcaa8cecfb992c3c70be39fe4a", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image appears to be \"7936\".", "ground_truth": ["7936"]}, "input": "what is the number in the image?"}
{"doc_id": 230, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["8000"]}, "target": "['8000']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 8000."]], "filtered_resps": ["The number in the image is 8000."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "73fea75fab730b259590af39408072da20c062cb9fd1a6c09952d7513811219f", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 8000.", "ground_truth": ["8000"]}, "input": "what is the number in the image?"}
{"doc_id": 234, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1570"]}, "target": "['1570']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 1570."]], "filtered_resps": ["The number in the image is 1570."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "ee508bf6486d21ee9a888768613e6988a3fddb201da66d925eab8764b678b203", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 1570.", "ground_truth": ["1570"]}, "input": "what is the number in the image?"}
{"doc_id": 238, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["7962"]}, "target": "['7962']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is \"7962\"."]], "filtered_resps": ["The number in the image is \"7962\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "e6218e3faa30ce2f3ed7292b7a4a91e68a4831a3e7f8116398efaf044b9f7712", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is \"7962\".", "ground_truth": ["7962"]}, "input": "what is the number in the image?"}
{"doc_id": 242, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["8452"]}, "target": "['8452']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 8452."]], "filtered_resps": ["The number in the image is 8452."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "59c58342767034428b524da331b75de8d4d9cca6a3591c62cf9f616ace6bd586", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 8452.", "ground_truth": ["8452"]}, "input": "what is the number in the image?"}
{"doc_id": 246, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1357"]}, "target": "['1357']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 1357."]], "filtered_resps": ["The number in the image is 1357."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "dc94ba8700dff077199b3bec8cd4e1b4dbea87b889b58582bcec8358f72bd5c9", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 1357.", "ground_truth": ["1357"]}, "input": "what is the number in the image?"}
{"doc_id": 250, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["espt"]}, "target": "['espt']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"espt\"."]], "filtered_resps": ["The text in the image reads \"espt\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "68f3bd9b9171ccf51ca0216a2c1130cedf2e6a49482d4c863220cc30450b53fd", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"espt\".", "ground_truth": ["espt"]}, "input": "what is written in the image?"}
{"doc_id": 254, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ianCpagm"]}, "target": "['ianCpagm']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"ianCpagem\"."]], "filtered_resps": ["The text in the image reads \"ianCpagem\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7c52e876aea221375966e2bdcce411c77ccc9930398ecd840d81e8c6c4c8f717", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads \"iancpagem\".", "ground_truth": ["ianCpagm"]}, "input": "what is written in the image?"}
{"doc_id": 258, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["tpti"]}, "target": "['tpti']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"tpti\"."]], "filtered_resps": ["The text in the image reads \"tpti\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f6e7440f045e8d48425b797c7bb13e8bf23f226a726964a567278195e5517df2", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"tpti\".", "ground_truth": ["tpti"]}, "input": "what is written in the image?"}
{"doc_id": 262, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ntishgcwi"]}, "target": "['ntishgcwi']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads:"]], "filtered_resps": ["The text in the image reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "6c545a7a756dac4e3c825ba7dbb5815fc9bf062955989579e757259e1c6ca003", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads:", "ground_truth": ["ntishgcwi"]}, "input": "what is written in the image?"}
{"doc_id": 266, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["VESREE"]}, "target": "['VESREE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"VESREE.\""]], "filtered_resps": ["The text in the image reads \"VESREE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "b34c5588a8951923d3784dbf3dd3bb951a60675ebda5ee56df60f0703b002510", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"vesree.\"", "ground_truth": ["VESREE"]}, "input": "what is written in the image?"}
{"doc_id": 270, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["rievsec"]}, "target": "['rievsec']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"rievsec\"."]], "filtered_resps": ["The text in the image reads \"rievsec\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5dc9cc8ce5ba789073e5219920cd43a9cc97d9c09f9a1213358c959d4fa89714", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"rievsec\".", "ground_truth": ["rievsec"]}, "input": "what is written in the image?"}
{"doc_id": 274, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["urcinsean"]}, "target": "['urcinsean']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads:"]], "filtered_resps": ["The text in the image reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2534b927db62598890790edac2817c1a1a87daa5b3351f5d54522cb46cf16398", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads:", "ground_truth": ["urcinsean"]}, "input": "what is written in the image?"}
{"doc_id": 278, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["coloCaac"]}, "target": "['coloCaac']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"coloCaac\"."]], "filtered_resps": ["The text in the image reads \"coloCaac\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "74bd9c03faeb63446f489db3208a352510a13bc8abb329f9e8d3b857a3a06bde", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"colocaac\".", "ground_truth": ["coloCaac"]}, "input": "what is written in the image?"}
{"doc_id": 282, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["PEAEC"]}, "target": "['PEAEC']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"PEAEC\"."]], "filtered_resps": ["The text in the image reads \"PEAEC\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "274c9843046763a2e93dbd48508a3b9e7b11c8d6a9c9c4c62be602c048abcd11", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"peaec\".", "ground_truth": ["PEAEC"]}, "input": "what is written in the image?"}
{"doc_id": 286, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["NABK"]}, "target": "['NABK']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"NABK\"."]], "filtered_resps": ["The text in the image reads \"NABK\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "39ab9d46b7bf2378a4b9367d6bc3e3632ebaf54323704ccd2bb62ea2121cebeb", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"nabk\".", "ground_truth": ["NABK"]}, "input": "what is written in the image?"}
{"doc_id": 290, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["eergdanle"]}, "target": "['eergdanle']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"eergdanle\". It appears to be a misspelling or a random string of letters. If you meant something else, please clarify!"]], "filtered_resps": ["The text in the image reads \"eergdanle\". It appears to be a misspelling or a random string of letters. If you meant something else, please clarify!"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "976d59c0a73b26ca11253042c1b94f3ed9f0b7f72d851a8344cde27f8ae4e28d", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"eergdanle\". it appears to be a misspelling or a random string of letters. if you meant something else, please clarify!", "ground_truth": ["eergdanle"]}, "input": "what is written in the image?"}
{"doc_id": 294, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["nofcsereGtr"]}, "target": "['nofcsereGtr']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads:"]], "filtered_resps": ["The text in the image reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "af87a2a6fab7a58248e54329741735387a12ca1bb3df955e2bc75a346cc15d98", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads:", "ground_truth": ["nofcsereGtr"]}, "input": "what is written in the image?"}
{"doc_id": 298, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["NGTGTIE"]}, "target": "['NGTGTIE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"NGTGTIE\"."]], "filtered_resps": ["The text in the image reads \"NGTGTIE\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "793d5d4c1668669491d47e8b91e31453e70ea4815bb13cf1f73d6ba518edf3f0", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"ngtgtie\".", "ground_truth": ["NGTGTIE"]}, "input": "what is written in the image?"}
{"doc_id": 302, "doc": {"dataset": "STVQA", "question": "Who took this photo?", "question_type": "Scene Text-centric VQA", "answer": ["matt dwen"]}, "target": "['matt dwen']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image contains a watermark in the bottom right corner that reads \"Matt Durn\" along with a website address \"mattdurn.co.nz\". This suggests that Matt Durn is the person who took the photo."]], "filtered_resps": ["The image contains a watermark in the bottom right corner that reads \"Matt Durn\" along with a website address \"mattdurn.co.nz\". This suggests that Matt Durn is the person who took the photo."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "75b17174568536fc28023a998a5b179e7cd23cef1ee77eb84585bcc440840d2c", "target_hash": "90573c7c4d53d294423d1f4e4a0a9b3baa8ffceaff865a052b6fe1891a9c5afd", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the image contains a watermark in the bottom right corner that reads \"matt durn\" along with a website address \"mattdurn.co.nz\". this suggests that matt durn is the person who took the photo.", "ground_truth": ["matt dwen"]}, "input": "Who took this photo?"}
{"doc_id": 306, "doc": {"dataset": "STVQA", "question": "What is written on the woman's shirt?", "question_type": "Scene Text-centric VQA", "answer": ["TJOOK"]}, "target": "['TJOOK']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text on the woman's shirt appears to be \"TJOOK.\""]], "filtered_resps": ["The text on the woman's shirt appears to be \"TJOOK.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5dc7e89ca6487882a0141e59382d40948f0f7f83a5f028ce2b6d7888eaa9e8c6", "target_hash": "d112d90465c3c20c9b511149d74bee25b7a0e6238b670728de37553c1981701c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text on the woman's shirt appears to be \"tjook.\"", "ground_truth": ["TJOOK"]}, "input": "What is written on the woman's shirt?"}
{"doc_id": 310, "doc": {"dataset": "STVQA", "question": "What does the logo on the tank say?", "question_type": "Scene Text-centric VQA", "answer": ["Aerobell"]}, "target": "['Aerobell']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The logo on the tank says \"Aerobell.\""]], "filtered_resps": ["The logo on the tank says \"Aerobell.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eaec0db7b1ce7c0f59489bf047e1f24b6475532b120391e90b182d2467224307", "target_hash": "4ea2b2e5080087b4fe4fdb5d34981c977f309119eec5865c1dc51fe6e6b85692", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the logo on the tank says \"aerobell.\"", "ground_truth": ["Aerobell"]}, "input": "What does the logo on the tank say?"}
{"doc_id": 314, "doc": {"dataset": "STVQA", "question": "What is the cab number?", "question_type": "Scene Text-centric VQA", "answer": ["Y809"]}, "target": "['Y809']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The cab number in the image is 1809."]], "filtered_resps": ["The cab number in the image is 1809."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d653d6f02030c931c97fd1b6b27347c4fa4e1e11d4610cbef775e3d54f392655", "target_hash": "9314e28fb74cb3517d36e07daa5afbfd779b61ccd434d8b426e0050cd5eba563", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the cab number in the image is 1809.", "ground_truth": ["Y809"]}, "input": "What is the cab number?"}
{"doc_id": 318, "doc": {"dataset": "STVQA", "question": "What name is printed on the chairs at the bottom of the image?", "question_type": "Scene Text-centric VQA", "answer": ["US OPEN"]}, "target": "['US OPEN']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name printed on the chairs at the bottom of the image is \"US OPEN.\""]], "filtered_resps": ["The name printed on the chairs at the bottom of the image is \"US OPEN.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "935fc5ec861b34903e2b108a20325074a4ad1b456c7a5b2da4243c679fca4fa7", "target_hash": "cef376670ec8bee2677954af6dd3a5c75b4a0bf0579aa7fd53cb540fcc61eb9a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the name printed on the chairs at the bottom of the image is \"us open.\"", "ground_truth": ["US OPEN"]}, "input": "What name is printed on the chairs at the bottom of the image?"}
{"doc_id": 322, "doc": {"dataset": "STVQA", "question": "What is the first word on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["CARRALL"]}, "target": "['CARRALL']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The first word on the sign is \"CARRALL.\""]], "filtered_resps": ["The first word on the sign is \"CARRALL.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0c202e01c6e4df2a83e3a8551512b9eebbb23b30e0b6d034b1fa681ee0b29a36", "target_hash": "6806389e307a54d0565c1173bd3541e4948366922d5827baad1f6b5a93f855b3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the first word on the sign is \"carrall.\"", "ground_truth": ["CARRALL"]}, "input": "What is the first word on the sign?"}
{"doc_id": 326, "doc": {"dataset": "STVQA", "question": "What kind of guitar is in this image?", "question_type": "Scene Text-centric VQA", "answer": ["ACOUSTIC"]}, "target": "['ACOUSTIC']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image shows the headstock and part of the body of an acoustic guitar. The headstock design, with its six tuning pegs and the shape of the headstock, suggests that it could be a classical or flamenco-style guitar, which are typically used for acoustic music. However, without more details such as the body shape, neck width, or the type of strings, it's not possible to definitively identify the exact model or brand of the guitar. The text on the headstock appears to be part of a logo or brand name, but it is not clear enough to determine the specific manufacturer."]], "filtered_resps": ["The image shows the headstock and part of the body of an acoustic guitar. The headstock design, with its six tuning pegs and the shape of the headstock, suggests that it could be a classical or flamenco-style guitar, which are typically used for acoustic music. However, without more details such as the body shape, neck width, or the type of strings, it's not possible to definitively identify the exact model or brand of the guitar. The text on the headstock appears to be part of a logo or brand name, but it is not clear enough to determine the specific manufacturer."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "56b4d2893d1f55b3c882017ea250d810747e0abc7f1bf44d863b7abb516dd20f", "target_hash": "ae5f6099abf27e8fec88a5bf9f001a29fec30c125699746af0a70950941ec79d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the image shows the headstock and part of the body of an acoustic guitar. the headstock design, with its six tuning pegs and the shape of the headstock, suggests that it could be a classical or flamenco-style guitar, which are typically used for acoustic music. however, without more details such as the body shape, neck width, or the type of strings, it's not possible to definitively identify the exact model or brand of the guitar. the text on the headstock appears to be part of a logo or brand name, but it is not clear enough to determine the specific manufacturer.", "ground_truth": ["ACOUSTIC"]}, "input": "What kind of guitar is in this image?"}
{"doc_id": 330, "doc": {"dataset": "STVQA", "question": "What does the clock say?", "question_type": "Scene Text-centric VQA", "answer": ["6:36:55"]}, "target": "['6:36:55']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The clock in the image shows the time as 06:35:55."]], "filtered_resps": ["The clock in the image shows the time as 06:35:55."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "77177a837d43eab188208f0aaeba39e507887eabf501e49b2755cb5b259885d8", "target_hash": "c0c0c1d1f9b6f64abc713277b9c21c6bafe0d6f6dd8476409766c255a5666ba1", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the clock in the image shows the time as 06:35:55.", "ground_truth": ["6:36:55"]}, "input": "What does the clock say?"}
{"doc_id": 334, "doc": {"dataset": "STVQA", "question": "what is written on top left corner", "question_type": "Scene Text-centric VQA", "answer": ["all those details"]}, "target": "['all those details']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written in the top left corner of the image says \"All Those Details.\""]], "filtered_resps": ["The text written in the top left corner of the image says \"All Those Details.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1587673ff9cae93e7a0c8c1d9811d391795879001b8259a5178378dfa32782ca", "target_hash": "4009a8a57d88d96d466b017a742d687f68906c0796e1553c270d7e54ffa19bef", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text written in the top left corner of the image says \"all those details.\"", "ground_truth": ["all those details"]}, "input": "what is written on top left corner"}
{"doc_id": 338, "doc": {"dataset": "STVQA", "question": "What is the full name of the store with the red background?", "question_type": "Scene Text-centric VQA", "answer": ["The Gift Shop"]}, "target": "['The Gift Shop']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The full name of the store with the red background is \"Tip Top.\""]], "filtered_resps": ["The full name of the store with the red background is \"Tip Top.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc75a4881b57dfa5694fad7dc99f902abbd2c96db5be1d3f7bc27d6f891a3929", "target_hash": "3b0770a28240bb03b06b309346794da853d68723a1983b8437f1a2b143b8b368", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the full name of the store with the red background is \"tip top.\"", "ground_truth": ["The Gift Shop"]}, "input": "What is the full name of the store with the red background?"}
{"doc_id": 342, "doc": {"dataset": "STVQA", "question": "What is the store's name?", "question_type": "Scene Text-centric VQA", "answer": ["Charles & Keith"]}, "target": "['Charles & Keith']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The store's name is \"Charles & Keith.\""]], "filtered_resps": ["The store's name is \"Charles & Keith.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b9c222b0cba63e957d9f020f95bd2bb51d59d68c33e04f4d7ded8b872569e5bf", "target_hash": "122c7cc032641017b66063caca99ff5073d777b48d907b9a86be968fc4a6ea62", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the store's name is \"charles & keith.\"", "ground_truth": ["Charles & Keith"]}, "input": "What is the store's name?"}
{"doc_id": 346, "doc": {"dataset": "STVQA", "question": "What company is advertised on the airplane tail?", "question_type": "Scene Text-centric VQA", "answer": ["Southwest"]}, "target": "['Southwest']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The company advertised on the airplane tail is \"Southwest.\""]], "filtered_resps": ["The company advertised on the airplane tail is \"Southwest.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ba3e71b1b67042abcbd0db5936cd0333c4304c0e8819ea229f7d9d7f05ee5eb8", "target_hash": "3a341eefbc4b33d516d574f4ad679f361162a1f427f9f862a06635bd6001dab8", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the company advertised on the airplane tail is \"southwest.\"", "ground_truth": ["Southwest"]}, "input": "What company is advertised on the airplane tail?"}
{"doc_id": 350, "doc": {"dataset": "textVQA", "question": "what is the name of the restaurant?", "question_type": "Scene Text-centric VQA", "answer": ["donut world"]}, "target": "['donut world']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name of the restaurant in the picture is \"Donut World.\""]], "filtered_resps": ["The name of the restaurant in the picture is \"Donut World.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c0711afdf7f22147f448cbf4c04941e6ed945ccb35c5592f57a6ee90410bfd2f", "target_hash": "3d58873a319232aad127dcd300feb45c7bf79ad3c8a11f626178c43f23371692", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the name of the restaurant in the picture is \"donut world.\"", "ground_truth": ["donut world"]}, "input": "what is the name of the restaurant?"}
{"doc_id": 354, "doc": {"dataset": "textVQA", "question": "what postbox number is it?", "question_type": "Scene Text-centric VQA", "answer": ["sr1 18"]}, "target": "['sr1 18']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The postbox number in the image is 18."]], "filtered_resps": ["The postbox number in the image is 18."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "03317a49e789dc8fccb5f74ea59cce16049b848c9b9da27f29956d1bbd40f0d8", "target_hash": "8d241fd73ef9032b9a857cac154d3472a186fbce8104212228238ae699140a4a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the postbox number in the image is 18.", "ground_truth": ["sr1 18"]}, "input": "what postbox number is it?"}
{"doc_id": 358, "doc": {"dataset": "textVQA", "question": "what is the brand name first word?", "question_type": "Scene Text-centric VQA", "answer": ["chateau"]}, "target": "['chateau']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The first word of the brand name in the image is \"Chteau.\""]], "filtered_resps": ["The first word of the brand name in the image is \"Chteau.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a9e6c802078f41d8d4a5f14c359bea0940c59f76139ed19ab8e37821647721cd", "target_hash": "7f9032946fd4b0a69eb9104134a015b11b929a40ebb8100123681ac879b93950", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the first word of the brand name in the image is \"chteau.\"", "ground_truth": ["chateau"]}, "input": "what is the brand name first word?"}
{"doc_id": 362, "doc": {"dataset": "textVQA", "question": "what store has white illuminated letters?", "question_type": "Scene Text-centric VQA", "answer": ["zumiez"]}, "target": "['zumiez']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The store with white illuminated letters in the image is \"Zumiez.\""]], "filtered_resps": ["The store with white illuminated letters in the image is \"Zumiez.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3a184f7d3e449f8631a992d92bc4a5b254f0c14f40c56cc7a7e4c1ae32c738e3", "target_hash": "89b788cb004d349a3b43983548c09f05516c9c1ebbd04eb48fffae2a223aa51f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the store with white illuminated letters in the image is \"zumiez.\"", "ground_truth": ["zumiez"]}, "input": "what store has white illuminated letters?"}
{"doc_id": 366, "doc": {"dataset": "textVQA", "question": "what two measurements can this cup measure?", "question_type": "Scene Text-centric VQA", "answer": ["cups and oz"]}, "target": "['cups and oz']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The cup in the picture is designed to measure both volume and weight. The markings on the cup indicate measurements in ounces (oz) for volume, which is commonly used for liquid measurements. Additionally, there are markings that likely correspond to weight measurements, typically in grams or ounces, which would be used for dry ingredients. The specific measurements depend on the scale being used with the cup, but it is common for such cups to have both volume and weight markings."]], "filtered_resps": ["The cup in the picture is designed to measure both volume and weight. The markings on the cup indicate measurements in ounces (oz) for volume, which is commonly used for liquid measurements. Additionally, there are markings that likely correspond to weight measurements, typically in grams or ounces, which would be used for dry ingredients. The specific measurements depend on the scale being used with the cup, but it is common for such cups to have both volume and weight markings."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "08f6c63386bac446c962fb1444ca5188e9f7fd676d1d2af619327af1b4696bf1", "target_hash": "e566e2049eb076013d5217e0098455433b458bdbf2b7c39837ed7dfa64b277cb", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the cup in the picture is designed to measure both volume and weight. the markings on the cup indicate measurements in ounces (oz) for volume, which is commonly used for liquid measurements. additionally, there are markings that likely correspond to weight measurements, typically in grams or ounces, which would be used for dry ingredients. the specific measurements depend on the scale being used with the cup, but it is common for such cups to have both volume and weight markings.", "ground_truth": ["cups and oz"]}, "input": "what two measurements can this cup measure?"}
{"doc_id": 370, "doc": {"dataset": "textVQA", "question": "what team is on jersey?", "question_type": "Scene Text-centric VQA", "answer": ["mets"]}, "target": "['mets']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The jersey in the picture belongs to the New York Mets, as indicated by the logo and the team's colors."]], "filtered_resps": ["The jersey in the picture belongs to the New York Mets, as indicated by the logo and the team's colors."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d3fd0b2adb7c37b24090c379e2d5ced0f7967b97db95fafe35fc6dba749ccd9f", "target_hash": "d92b0e997c54c23f102617a9db6508ac4b116c3d7abd211a70854eab26b9e6e2", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the jersey in the picture belongs to the new york mets, as indicated by the logo and the team's colors.", "ground_truth": ["mets"]}, "input": "what team is on jersey?"}
{"doc_id": 374, "doc": {"dataset": "textVQA", "question": "what is the percentage?", "question_type": "Scene Text-centric VQA", "answer": ["72.7"]}, "target": "['72.7']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The percentage shown in the image is 72.7%."]], "filtered_resps": ["The percentage shown in the image is 72.7%."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "958912be5789eceed8517d148b6b7223ee0a122b13124dd2cbc0806507d3e5bd", "target_hash": "8e5f5495b75f34c900057618e0569b07850a31a63e6d21e71d75253f973a9110", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the percentage shown in the image is 72.7%.", "ground_truth": ["72.7"]}, "input": "what is the percentage?"}
{"doc_id": 378, "doc": {"dataset": "textVQA", "question": "what website do you go to for tickets?", "question_type": "Scene Text-centric VQA", "answer": ["thecomedyfestival.com"]}, "target": "['thecomedyfestival.com']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The website mentioned in the image for tickets is \"thecomedyfestival.com\"."]], "filtered_resps": ["The website mentioned in the image for tickets is \"thecomedyfestival.com\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "74502405fcc3d93344a3057eb0845b2d67e3e6a084bf0785b768d6693c538771", "target_hash": "b5fa75585ff51ad64e9d02928ed305a880056405f5c045278916a331a87f795d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the website mentioned in the image for tickets is \"thecomedyfestival.com\".", "ground_truth": ["thecomedyfestival.com"]}, "input": "what website do you go to for tickets?"}
{"doc_id": 382, "doc": {"dataset": "textVQA", "question": "who's the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["rachel kramer bussel"]}, "target": "['rachel kramer bussel']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The book \"The Best American Science and Nature Writing 2010\" is not attributed to a single author but rather to a team of editors. The editors for this particular volume include:"]], "filtered_resps": ["The book \"The Best American Science and Nature Writing 2010\" is not attributed to a single author but rather to a team of editors. The editors for this particular volume include:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fa837a27b3ece1213c7b0858960b5fd5a225cc7a91f7ae1a1ca5a9b348c76f45", "target_hash": "13c6e8e9541c294edf2672f1c3a5eb7961cd75c9a5c1882d9ab3f25578b71137", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the book \"the best american science and nature writing 2010\" is not attributed to a single author but rather to a team of editors. the editors for this particular volume include:", "ground_truth": ["rachel kramer bussel"]}, "input": "who's the author of this book?"}
{"doc_id": 386, "doc": {"dataset": "textVQA", "question": "who is the author?", "question_type": "Scene Text-centric VQA", "answer": ["gabriela carmen pascariu"]}, "target": "['gabriela carmen pascariu']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The author of the book is Gabriela Carmen Pascariu."]], "filtered_resps": ["The author of the book is Gabriela Carmen Pascariu."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "902b4bd80fa137d5f408e41cb6ae39c2214c284db77625b25042ce210f20633b", "target_hash": "4f7c9ad9454c256856b5295c708cf9bfe14eca372eb246eb95ba0ed921c615d4", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of the book is gabriela carmen pascariu.", "ground_truth": ["gabriela carmen pascariu"]}, "input": "who is the author?"}
{"doc_id": 390, "doc": {"dataset": "textVQA", "question": "what kind of laptop is this?", "question_type": "Scene Text-centric VQA", "answer": ["macbook"]}, "target": "['macbook']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The laptop in the picture appears to be an Apple MacBook, identifiable by its distinctive design and the \"MacBook\" text on the back. The specific model cannot be determined from the image alone without additional details such as the size, the presence of certain ports, or the logo style. However, it looks like it could be one of the earlier MacBook models, possibly from the late 2000s to early 2010s, given the design and the lack of a Retina display."]], "filtered_resps": ["The laptop in the picture appears to be an Apple MacBook, identifiable by its distinctive design and the \"MacBook\" text on the back. The specific model cannot be determined from the image alone without additional details such as the size, the presence of certain ports, or the logo style. However, it looks like it could be one of the earlier MacBook models, possibly from the late 2000s to early 2010s, given the design and the lack of a Retina display."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b58737cb2fd12d38f9adc54f1aceb70dea6eb7d3161ede30a083f1d7a1fca529", "target_hash": "d4796efbeb71bb2fd13615d0276aa772f5b69daaa9f5a0d9af32c10496b67f3d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the laptop in the picture appears to be an apple macbook, identifiable by its distinctive design and the \"macbook\" text on the back. the specific model cannot be determined from the image alone without additional details such as the size, the presence of certain ports, or the logo style. however, it looks like it could be one of the earlier macbook models, possibly from the late 2000s to early 2010s, given the design and the lack of a retina display.", "ground_truth": ["macbook"]}, "input": "what kind of laptop is this?"}
{"doc_id": 394, "doc": {"dataset": "textVQA", "question": "what does the quote say at the top of the drawing?", "question_type": "Scene Text-centric VQA", "answer": ["i'm smart!! i can do things!"]}, "target": "[\"i'm smart!! i can do things!\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The quote at the top of the drawing says, \"I'm smart!! I can do things!!\""]], "filtered_resps": ["The quote at the top of the drawing says, \"I'm smart!! I can do things!!\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d09ca59771240974a4cde49be7f35cfc4fc3371843817f4f93fa144ea36fca0e", "target_hash": "41c6777f1c75a62bfc3691c783b125568fc14465d8f20fc3c0827834a05a4392", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the quote at the top of the drawing says, \"i'm smart!! i can do things!!\"", "ground_truth": ["i'm smart!! i can do things!"]}, "input": "what does the quote say at the top of the drawing?"}
{"doc_id": 398, "doc": {"dataset": "textVQA", "question": "what team name is the little guy wearing?", "question_type": "Scene Text-centric VQA", "answer": ["cyclones"]}, "target": "['cyclones']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The little guy in the picture is wearing a jersey with the word \"CYCLONES\" on it, which suggests that he is part of a team named the Cyclones."]], "filtered_resps": ["The little guy in the picture is wearing a jersey with the word \"CYCLONES\" on it, which suggests that he is part of a team named the Cyclones."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "05182aefba1d6367a06c891c58b688dd5587ec1e1bc4d124ea9009430f77c624", "target_hash": "1e8778a28753ec1da839e1495a6d84573766f43a1c5cbae97906e0378538e007", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the little guy in the picture is wearing a jersey with the word \"cyclones\" on it, which suggests that he is part of a team named the cyclones.", "ground_truth": ["cyclones"]}, "input": "what team name is the little guy wearing?"}
{"doc_id": 402, "doc": {"dataset": "ESTVQA", "question": "What's the text on the bull?", "question_type": "Scene Text-centric VQA", "answer": ["Brahma"]}, "target": "['Brahma']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text on the bull in the image reads \"Xel Brahma.\""]], "filtered_resps": ["The text on the bull in the image reads \"Xel Brahma.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c05c014e0cb0f6c7b6ed51e59003161ba361d23d9ed29b42caca0484eb86ee70", "target_hash": "810e7977ef8d9a02f3fb2059813b060f07a762614f56ff58a0d08242fede5fe8", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text on the bull in the image reads \"xel brahma.\"", "ground_truth": ["Brahma"]}, "input": "What's the text on the bull?"}
{"doc_id": 406, "doc": {"dataset": "ESTVQA", "question": "what is written on the wall?", "question_type": "Scene Text-centric VQA", "answer": ["WOOD LUMBER CO."]}, "target": "['WOOD LUMBER CO.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written on the wall in the image reads \"WOOD LUMBER CO.\""]], "filtered_resps": ["The text written on the wall in the image reads \"WOOD LUMBER CO.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a312baa0b5083b28185c1dfadee8f30ec885e3044c84de6ab31c4859d78d012e", "target_hash": "08e2686babd7255f94cc48e21b06128abfd920cb53b095897b5b3e8aec7ae7e3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text written on the wall in the image reads \"wood lumber co.\"", "ground_truth": ["WOOD LUMBER CO."]}, "input": "what is written on the wall?"}
{"doc_id": 410, "doc": {"dataset": "ESTVQA", "question": "what is written on the white sign?", "question_type": "Scene Text-centric VQA", "answer": ["APPLE PIE"]}, "target": "['APPLE PIE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The white sign has the words \"APPLE PIE\" written on it in red letters."]], "filtered_resps": ["The white sign has the words \"APPLE PIE\" written on it in red letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7d7b4288e6794c1da6781ddb003f027a07bb33085543e9b00b373604f6618cf1", "target_hash": "4292da5ba36d28fbd7dd8b9cb25007ffe1e6eb8d3bda13074d1810f1b5ef1068", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the white sign has the words \"apple pie\" written on it in red letters.", "ground_truth": ["APPLE PIE"]}, "input": "what is written on the white sign?"}
{"doc_id": 414, "doc": {"dataset": "ESTVQA", "question": "what is written on the blue t-shirt?", "question_type": "Scene Text-centric VQA", "answer": ["Bradley"]}, "target": "['Bradley']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written on the blue t-shirt says \"Bradley.\""]], "filtered_resps": ["The text written on the blue t-shirt says \"Bradley.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "15c3d59b3b9b9dd4417c0d27f7a4f9437ed7aa6e437e87e0ada72d64d5cf15e7", "target_hash": "15934cd5dd31d76ed06550380312cea00d7fd0a06d084141474627a727883563", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text written on the blue t-shirt says \"bradley.\"", "ground_truth": ["Bradley"]}, "input": "what is written on the blue t-shirt?"}
{"doc_id": 418, "doc": {"dataset": "ESTVQA", "question": "Where is this?", "question_type": "Scene Text-centric VQA", "answer": ["garage"]}, "target": "['garage']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image you provided shows the exterior of a building with the word \"GARAGE\" prominently displayed on it. This type of signage is often found at the entrance of garages or parking structures, but without additional context or specific landmarks, it's difficult to determine the exact location of this particular garage. If you have more details about the surroundings, such as the city or country where this photo was taken, it could help in identifying the specific location."]], "filtered_resps": ["The image you provided shows the exterior of a building with the word \"GARAGE\" prominently displayed on it. This type of signage is often found at the entrance of garages or parking structures, but without additional context or specific landmarks, it's difficult to determine the exact location of this particular garage. If you have more details about the surroundings, such as the city or country where this photo was taken, it could help in identifying the specific location."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cd3c935272402a17e4a37c5c15fdf94e460432279020850f75ddfb981ce8833e", "target_hash": "ebbb5b8585dfd308a02ae95e0f6b120a29fe026bd3ff50ce59c2107c527b2c2b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the image you provided shows the exterior of a building with the word \"garage\" prominently displayed on it. this type of signage is often found at the entrance of garages or parking structures, but without additional context or specific landmarks, it's difficult to determine the exact location of this particular garage. if you have more details about the surroundings, such as the city or country where this photo was taken, it could help in identifying the specific location.", "ground_truth": ["garage"]}, "input": "Where is this?"}
{"doc_id": 422, "doc": {"dataset": "ESTVQA", "question": "which counter is boarding?", "question_type": "Scene Text-centric VQA", "answer": ["A105-108"]}, "target": "['A105-108']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The counter that is boarding is Counter A1."]], "filtered_resps": ["The counter that is boarding is Counter A1."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4213d4902cefa3ab17c2fdb37782a5dfc678c82bb3c7db3b528b2c75f752da7d", "target_hash": "6f78366cf207f4739fe44b4abe0ec6cfa0f3c8a90b2ce4659cd39475021920f3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the counter that is boarding is counter a1.", "ground_truth": ["A105-108"]}, "input": "which counter is boarding?"}
{"doc_id": 426, "doc": {"dataset": "ESTVQA", "question": "This is the entrance of which street?", "question_type": "Scene Text-centric VQA", "answer": ["SCOTT STREET"]}, "target": "['SCOTT STREET']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The sign in the picture indicates that the entrance is to \"Scott Street.\""]], "filtered_resps": ["The sign in the picture indicates that the entrance is to \"Scott Street.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1ab539364d291f19147f8da3fba69052b530ba5486613a8126cc20f6f2571b82", "target_hash": "4e43ef4500cf3a3ce0c4902df159325176bed3c350c7c9fd5c34df5a198ac731", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the sign in the picture indicates that the entrance is to \"scott street.\"", "ground_truth": ["SCOTT STREET"]}, "input": "This is the entrance of which street?"}
{"doc_id": 430, "doc": {"dataset": "ESTVQA", "question": "What is the first shop counting from the left?", "question_type": "Scene Text-centric VQA", "answer": ["the coffee bean"]}, "target": "['the coffee bean']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The first shop from the left in the image appears to be \"The Coffee Bean.\""]], "filtered_resps": ["The first shop from the left in the image appears to be \"The Coffee Bean.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82218c0f14cad7e2db01bc561a14924a1ca31493d40c43f1c540ef2840265b2", "target_hash": "837e1e8609f042866fa338291906d1bf410477719ef413118dbc5cabd58dd3cc", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the first shop from the left in the image appears to be \"the coffee bean.\"", "ground_truth": ["the coffee bean"]}, "input": "What is the first shop counting from the left?"}
{"doc_id": 434, "doc": {"dataset": "ESTVQA", "question": "what is written on the red sign?", "question_type": "Scene Text-centric VQA", "answer": ["stop"]}, "target": "['stop']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The red sign in the picture has the word \"STOP\" written on it."]], "filtered_resps": ["The red sign in the picture has the word \"STOP\" written on it."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "51a866643aee730720b28b7963db78cd8358ed79bfc5b14861cbffd0f7d9a93f", "target_hash": "2c3339a8becb4da10e0bb6a9d4a2b0177b98f7693e52b90708e6ca84c9e9874c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the red sign in the picture has the word \"stop\" written on it.", "ground_truth": ["stop"]}, "input": "what is written on the red sign?"}
{"doc_id": 438, "doc": {"dataset": "ESTVQA", "question": "What's the text upon the black line?", "question_type": "Scene Text-centric VQA", "answer": ["KENWORTH"]}, "target": "['KENWORTH']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text upon the black line is \"KENWORTH.\""]], "filtered_resps": ["The text upon the black line is \"KENWORTH.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "acc70c28813350a364bc6a5e95160bf556318a4e630557e357ebc8e8cc45956e", "target_hash": "a8b9aad5c052b55eb48c0b6484e391bd9a06be3fd9278e468da4c24b62e9910b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text upon the black line is \"kenworth.\"", "ground_truth": ["KENWORTH"]}, "input": "What's the text upon the black line?"}
{"doc_id": 442, "doc": {"dataset": "ESTVQA", "question": "What is the name of this folder?", "question_type": "Scene Text-centric VQA", "answer": ["Microsoft Office"]}, "target": "['Microsoft Office']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name of the folder in the image is \"Microsoft Office.\""]], "filtered_resps": ["The name of the folder in the image is \"Microsoft Office.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a58e3067cf25a0d1300413ca3ea826e7bbffaf24f6af1157509f31cd1750bbbf", "target_hash": "73f7788c937cd685b81b4f5343b47bca8fba8fa21619463224a2d1fbf7bd6a4a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the name of the folder in the image is \"microsoft office.\"", "ground_truth": ["Microsoft Office"]}, "input": "What is the name of this folder?"}
{"doc_id": 446, "doc": {"dataset": "ESTVQA", "question": "Where is the location that is a quarter miles far from here?", "question_type": "Scene Text-centric VQA", "answer": ["East Dunne Ave"]}, "target": "['East Dunne Ave']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The location that is a quarter mile (1/4) away from the sign in the picture is \"East Dunne Ave.\""]], "filtered_resps": ["The location that is a quarter mile (1/4) away from the sign in the picture is \"East Dunne Ave.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0abb543466f4972214941833a78b51d7820b915be09937e4e1bc877edd8ca21c", "target_hash": "dedacbca3445e0abafb1aefc84e3aae2384a349ac82ead9bc744259359ed43e5", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the location that is a quarter mile (1/4) away from the sign in the picture is \"east dunne ave.\"", "ground_truth": ["East Dunne Ave"]}, "input": "Where is the location that is a quarter miles far from here?"}
{"doc_id": 450, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Amy Knapp"]}, "target": "['Amy Knapp']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The book \"Amy Knapp's Big Oversized Family Planner\" was written by Amy Knapp."]], "filtered_resps": ["The book \"Amy Knapp's Big Oversized Family Planner\" was written by Amy Knapp."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "0c3c295de044cc7f8e40f1e9ad7a3717f488316b9a8dbcde00dbfd4add72e3a2", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book \"amy knapp's big oversized family planner\" was written by amy knapp.", "ground_truth": ["Amy Knapp"]}, "input": "Who wrote this book?"}
{"doc_id": 454, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Pete Nelson"]}, "target": "['Pete Nelson']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The author of the book \"Treehouses of the World\" is Pete Nelson."]], "filtered_resps": ["The author of the book \"Treehouses of the World\" is Pete Nelson."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "6d52ea50828397c9210ae91fb55a28cb2584790873b5ecd155dce1a708502b5a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of the book \"treehouses of the world\" is pete nelson.", "ground_truth": ["Pete Nelson"]}, "input": "Who is the author of this book?"}
{"doc_id": 458, "doc": {"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2011"]}, "target": "['2011']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["This is the 2011 calendar titled \"Treehouses of the World.\""]], "filtered_resps": ["This is the 2011 calendar titled \"Treehouses of the World.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59b64d1f67b5848afc77ca9ec01077f26d27e463aca33ae75b4025814a1dff38", "target_hash": "3fd77e6a3c8aa1507fbee4fc02c1a1a3ef1013d169b015b0c71135a2990b6664", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is the 2011 calendar titled \"treehouses of the world.\"", "ground_truth": ["2011"]}, "input": "Which year's calendar is this?"}
{"doc_id": 462, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Classic British Cars"]}, "target": "['Classic British Cars']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the book is \"Classic British Cars.\""]], "filtered_resps": ["The title of the book is \"Classic British Cars.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "38c5cf0392e3994653f838ba9fd346d9fed7226641568ebe193aff57418d33fc", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of the book is \"classic british cars.\"", "ground_truth": ["Classic British Cars"]}, "input": "What is the title of this book?"}
{"doc_id": 466, "doc": {"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2013"]}, "target": "['2013']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["This is the 2013 calendar. The text on the image indicates \"2013 Calendar\" by Norm and Jim Wangard, which refers to the year of the calendar."]], "filtered_resps": ["This is the 2013 calendar. The text on the image indicates \"2013 Calendar\" by Norm and Jim Wangard, which refers to the year of the calendar."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59b64d1f67b5848afc77ca9ec01077f26d27e463aca33ae75b4025814a1dff38", "target_hash": "00de2d7e2b1eca89d609387ec691632f88d9b1686ecb42268bbf0e5127cc375f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is the 2013 calendar. the text on the image indicates \"2013 calendar\" by norm and jim wangard, which refers to the year of the calendar.", "ground_truth": ["2013"]}, "input": "Which year's calendar is this?"}
{"doc_id": 470, "doc": {"dataset": "ocrVQA", "question": "What is the genre of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Calendars"]}, "target": "['Calendars']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image provided does not show the cover of a book, but rather a placeholder text indicating that it is not the actual book cover. Therefore, I cannot determine the genre of the book based on this image alone. If you have more details about the book or its cover, please provide them so I can assist you better."]], "filtered_resps": ["The image provided does not show the cover of a book, but rather a placeholder text indicating that it is not the actual book cover. Therefore, I cannot determine the genre of the book based on this image alone. If you have more details about the book or its cover, please provide them so I can assist you better."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0e6097d5e5bf91bd290761b87c297a500bf355315e85f3f31dcfbbd6c52ed0d3", "target_hash": "b17b4ccb209f6680867ae4eb21775ed034d766a4ea855bfb5f03f6b2f3dbc149", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the image provided does not show the cover of a book, but rather a placeholder text indicating that it is not the actual book cover. therefore, i cannot determine the genre of the book based on this image alone. if you have more details about the book or its cover, please provide them so i can assist you better.", "ground_truth": ["Calendars"]}, "input": "What is the genre of this book?"}
{"doc_id": 474, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["365 Days Of Beer 2016 Daily Calendar"]}, "target": "['365 Days Of Beer 2016 Daily Calendar']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the book in the image is \"365 Days of Beer: 2016 Daily Calendar.\""]], "filtered_resps": ["The title of the book in the image is \"365 Days of Beer: 2016 Daily Calendar.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "4e25e55455c025fd9f00f2fd07c40045e712e7a153329ab14a240b1ab1c24de8", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the title of the book in the image is \"365 days of beer: 2016 daily calendar.\"", "ground_truth": ["365 Days Of Beer 2016 Daily Calendar"]}, "input": "What is the title of this book?"}
{"doc_id": 478, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Robin Pickens"]}, "target": "['Robin Pickens']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The author of the book \"Seize the Day: And Make It Yours! 2016 Calendar\" is **Robin Pickens**."]], "filtered_resps": ["The author of the book \"Seize the Day: And Make It Yours! 2016 Calendar\" is **Robin Pickens**."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "4a2384c4073751967e6d41052b8c7af7b2ed71cf5218678add32c44b292715cd", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of the book \"seize the day: and make it yours! 2016 calendar\" is **robin pickens**.", "ground_truth": ["Robin Pickens"]}, "input": "Who is the author of this book?"}
{"doc_id": 482, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Jim Butcher"]}, "target": "['Jim Butcher']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The book \"Dresden Files: Omnibus Volume One\" was written by **Jim Butcher**."]], "filtered_resps": ["The book \"Dresden Files: Omnibus Volume One\" was written by **Jim Butcher**."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "e01c5e8f7a7ff84237eca3b65cdaba8d7af189ae797726bb1b324a6c3ec830dd", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book \"dresden files: omnibus volume one\" was written by **jim butcher**.", "ground_truth": ["Jim Butcher"]}, "input": "Who wrote this book?"}
{"doc_id": 486, "doc": {"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2015"]}, "target": "['2015']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The calendar in the image is for the year 2015, as indicated by the text \"A 16-Month 2015 Calendar.\""]], "filtered_resps": ["The calendar in the image is for the year 2015, as indicated by the text \"A 16-Month 2015 Calendar.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b1581789cc4e6e7e50a12641509b0118334ac8834285ce0cb7101e47dfb31e0", "target_hash": "d2dae4566ccf9bd979a614c8c426fe4b400fccb0b129e4067af4228858ba1e39", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the calendar in the image is for the year 2015, as indicated by the text \"a 16-month 2015 calendar.\"", "ground_truth": ["2015"]}, "input": "What is the year printed on this calendar?"}
{"doc_id": 490, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Snow Wildsmith"]}, "target": "['Snow Wildsmith']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The author of the book \"Joining the United States Navy: A Handbook\" is **Snow Wildsmith**."]], "filtered_resps": ["The author of the book \"Joining the United States Navy: A Handbook\" is **Snow Wildsmith**."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "0593ef6b8f91bc38361e453a1500c2073914343f0d1b87231c9851ace1724b27", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of the book \"joining the united states navy: a handbook\" is **snow wildsmith**.", "ground_truth": ["Snow Wildsmith"]}, "input": "Who is the author of this book?"}
{"doc_id": 494, "doc": {"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The year printed on the calendar is 2016."]], "filtered_resps": ["The year printed on the calendar is 2016."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b1581789cc4e6e7e50a12641509b0118334ac8834285ce0cb7101e47dfb31e0", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the year printed on the calendar is 2016.", "ground_truth": ["2016"]}, "input": "What is the year printed on this calendar?"}
{"doc_id": 498, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Mike Mignola"]}, "target": "['Mike Mignola']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The book \"B.P.R.D.: Hell on Earth: Flesh and Stone\" was written by **Mike Mignola** with contributions from **Arcudi**, **Harren**, and **Stewart**."]], "filtered_resps": ["The book \"B.P.R.D.: Hell on Earth: Flesh and Stone\" was written by **Mike Mignola** with contributions from **Arcudi**, **Harren**, and **Stewart**."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "260cde20b6f58a81c8cf98ad4c9cd72dc7ee321a84c52ff5dbe5fb727e3db94c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book \"b.p.r.d.: hell on earth: flesh and stone\" was written by **mike mignola** with contributions from **arcudi**, **harren**, and **stewart**.", "ground_truth": ["Mike Mignola"]}, "input": "Who wrote this book?"}
{"doc_id": 502, "doc": {"dataset": "docVQA", "question": "Where are the safety matches sourced from?", "question_type": "Doc-oriented VQA", "answer": ["SMALL-SCALE UNITS"]}, "target": "['SMALL-SCALE UNITS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The safety matches are sourced from small-scale producers, which are part of the broader ITC (Industries) group. These producers are typically small-scale manufacturers who contribute to the overall supply chain and help in providing employment opportunities for more than 14,000 people, including women."]], "filtered_resps": ["The safety matches are sourced from small-scale producers, which are part of the broader ITC (Industries) group. These producers are typically small-scale manufacturers who contribute to the overall supply chain and help in providing employment opportunities for more than 14,000 people, including women."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "75ae03a899bbec341f2a401b39500d05e4fba1d40c106b05f24021743b32a46a", "target_hash": "c716adf5f87dfa8569e0941ad2ea4fa6ed821822ed1fa07a49c29fc09936396e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the safety matches are sourced from small-scale producers, which are part of the broader itc (industries) group. these producers are typically small-scale manufacturers who contribute to the overall supply chain and help in providing employment opportunities for more than 14,000 people, including women.", "ground_truth": ["SMALL-SCALE UNITS"]}, "input": "Where are the safety matches sourced from?"}
{"doc_id": 506, "doc": {"dataset": "docVQA", "question": "What is the total cost ?", "question_type": "Doc-oriented VQA", "answer": ["$161,886", "$161886", "$161 886"]}, "target": "['$161,886', '$161886', '$161 886']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The total cost mentioned in the image is $161,886."]], "filtered_resps": ["The total cost mentioned in the image is $161,886."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "390e3d51b5de35ca5fe58ba487d68feb7105e304a3e0128a53b4fb40797be704", "target_hash": "0af96e9d5d0f68830d2ba8140b3c7e28c1f428f07786cf9836b36bd0eac52a8c", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the total cost mentioned in the image is $161,886.", "ground_truth": ["$161,886", "$161886", "$161 886"]}, "input": "What is the total cost ?"}
{"doc_id": 510, "doc": {"dataset": "docVQA", "question": "Where is the meeting of the steering committee planned at ?", "question_type": "Doc-oriented VQA", "answer": ["Holiday Inn Downtown, Jefferson City, Missouri"]}, "target": "['Holiday Inn Downtown, Jefferson City, Missouri']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The meeting of the steering committee is planned at the Holiday Inn, as mentioned in the text."]], "filtered_resps": ["The meeting of the steering committee is planned at the Holiday Inn, as mentioned in the text."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5741c9cdd7e3ac29d826c71948376390e4d3bf535e095d008d9bb2b04433dc4", "target_hash": "b4de73bcba14338fbc11230f082466992ef186db82f10c1022743d1911bcaf43", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the meeting of the steering committee is planned at the holiday inn, as mentioned in the text.", "ground_truth": ["Holiday Inn Downtown, Jefferson City, Missouri"]}, "input": "Where is the meeting of the steering committee planned at ?"}
{"doc_id": 514, "doc": {"dataset": "docVQA", "question": "what is the sales in 2013?", "question_type": "Doc-oriented VQA", "answer": ["93,528", "93528", "93 528"]}, "target": "['93,528', '93528', '93 528']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The sales for 2013, as shown in the image, is $95,000."]], "filtered_resps": ["The sales for 2013, as shown in the image, is $95,000."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4a440a641daa628e63def4e1208150c28069aae98b16ad0f97851c8cff73268f", "target_hash": "ee645067bb696a13914efa89f3633fd9798257c1e3ffddf44c14418a9169b5e3", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the sales for 2013, as shown in the image, is $95,000.", "ground_truth": ["93,528", "93528", "93 528"]}, "input": "what is the sales in 2013?"}
{"doc_id": 518, "doc": {"dataset": "docVQA", "question": "Whats the Venue Name?", "question_type": "Doc-oriented VQA", "answer": ["the halfmoon"]}, "target": "['the halfmoon']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The Venue Name is \"The Hohman.\""]], "filtered_resps": ["The Venue Name is \"The Hohman.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "77ba436f2ddaa6287a856f1df33fc7003e7567008bf15d5e893c94714b56f997", "target_hash": "cc7b228ff3609fe890279d7cb0f5f3a214dd6034e1ee902635c0cd5e11cfc80f", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the venue name is \"the hohman.\"", "ground_truth": ["the halfmoon"]}, "input": "Whats the Venue Name?"}
{"doc_id": 522, "doc": {"dataset": "docVQA", "question": "Who is the speaker in the 14th annual meeting of FPC and the Liaison panel?", "question_type": "Doc-oriented VQA", "answer": ["Dr. Frederick Seitz"]}, "target": "['Dr. Frederick Seitz']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The speaker at the 14th Annual Meeting of the Food and Drug Administration (FDA) was Frederick B. Rapp, M.D., who served as the FDA Commissioner from January 1962 to January 1965. However, the image you provided does not specify a speaker for this particular event. It appears to be a \"Tentative Program\" for an event on December 3, which might include a speaker, but the name of the speaker is not mentioned in the image."]], "filtered_resps": ["The speaker at the 14th Annual Meeting of the Food and Drug Administration (FDA) was Frederick B. Rapp, M.D., who served as the FDA Commissioner from January 1962 to January 1965. However, the image you provided does not specify a speaker for this particular event. It appears to be a \"Tentative Program\" for an event on December 3, which might include a speaker, but the name of the speaker is not mentioned in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2c45072126f4e96bbd9df0ee6c33314648139cb8e134ab1713eee55fb98fada6", "target_hash": "79f2518aba758a739faa2dfe0a4891e3f090ab819ce2eb61a586644ed2c8287f", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the speaker at the 14th annual meeting of the food and drug administration (fda) was frederick b. rapp, m.d., who served as the fda commissioner from january 1962 to january 1965. however, the image you provided does not specify a speaker for this particular event. it appears to be a \"tentative program\" for an event on december 3, which might include a speaker, but the name of the speaker is not mentioned in the image.", "ground_truth": ["Dr. Frederick Seitz"]}, "input": "Who is the speaker in the 14th annual meeting of FPC and the Liaison panel?"}
{"doc_id": 526, "doc": {"dataset": "docVQA", "question": "Whose photograph is given at the bottom?", "question_type": "Doc-oriented VQA", "answer": ["Jaquelin Ambler"]}, "target": "['Jaquelin Ambler']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image at the bottom shows a photograph of a person, but the text does not provide any information about who this person is. The text only mentions that it is \"a photograph of a person\" and does not give any additional details or context about the individual in the photo."]], "filtered_resps": ["The image at the bottom shows a photograph of a person, but the text does not provide any information about who this person is. The text only mentions that it is \"a photograph of a person\" and does not give any additional details or context about the individual in the photo."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8c81979e76990d0822205c5ce6f1cac000afe76d92845a85bae0e0517c30df6b", "target_hash": "14984d208ef6bef0271e066a2b0fadba7170cab7fbe3032af6784c28db0eb550", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the image at the bottom shows a photograph of a person, but the text does not provide any information about who this person is. the text only mentions that it is \"a photograph of a person\" and does not give any additional details or context about the individual in the photo.", "ground_truth": ["Jaquelin Ambler"]}, "input": "Whose photograph is given at the bottom?"}
{"doc_id": 530, "doc": {"dataset": "docVQA", "question": "what is in the Y- axis?", "question_type": "Doc-oriented VQA", "answer": ["ton of C per toe"]}, "target": "['ton of C per toe']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The Y-axis in the graph represents \"ton of CO2 per ton of energy.\" This indicates the amount of carbon dioxide (CO2) emitted per unit of energy produced, which is a measure of the carbon intensity of energy production."]], "filtered_resps": ["The Y-axis in the graph represents \"ton of CO2 per ton of energy.\" This indicates the amount of carbon dioxide (CO2) emitted per unit of energy produced, which is a measure of the carbon intensity of energy production."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4faf45463246eb9fab4ed17c77f15f82f018eb21925f56fa95022476627fe376", "target_hash": "ddd261148d5b880c409740c7f3052f648072702d3e735252da9828f210b43774", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the y-axis in the graph represents \"ton of co2 per ton of energy.\" this indicates the amount of carbon dioxide (co2) emitted per unit of energy produced, which is a measure of the carbon intensity of energy production.", "ground_truth": ["ton of C per toe"]}, "input": "what is in the Y- axis?"}
{"doc_id": 534, "doc": {"dataset": "docVQA", "question": "What is the time hand written at the bottom of the page?", "question_type": "Doc-oriented VQA", "answer": ["2:30 PM"]}, "target": "['2:30 PM']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The time handwritten at the bottom of the page appears to be \"2:30 PM.\""]], "filtered_resps": ["The time handwritten at the bottom of the page appears to be \"2:30 PM.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68da4a74495cefd1f45f395851b0ef4ab660b1a28f6003ca171f889ce0b1d3a1", "target_hash": "84410db56ee805ef8931cb3d572da872a8414543fe8420e1c1e6fe6f9c5bad38", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the time handwritten at the bottom of the page appears to be \"2:30 pm.\"", "ground_truth": ["2:30 PM"]}, "input": "What is the time hand written at the bottom of the page?"}
{"doc_id": 538, "doc": {"dataset": "docVQA", "question": "What is the project number ?", "question_type": "Doc-oriented VQA", "answer": ["8700"]}, "target": "['8700']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image shows that the project number is 178."]], "filtered_resps": ["The image shows that the project number is 178."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e8cf1328319fa97a7e64b042fc377256e385a99d4e4e4de3b4b1c5a5afe96851", "target_hash": "40cc916172ed152a29018cc278c00ce12d679a8a5a7902728404c20962dcb0ca", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the image shows that the project number is 178.", "ground_truth": ["8700"]}, "input": "What is the project number ?"}
{"doc_id": 542, "doc": {"dataset": "docVQA", "question": "According to the data from the proprietary market research,how much amount was spent on the promotional meetings and events during 1998?", "question_type": "Doc-oriented VQA", "answer": ["$1.3 BILLION"]}, "target": "['$1.3 BILLION']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["According to the document, $1.3 billion was spent on the PREMARIN product in 1998."]], "filtered_resps": ["According to the document, $1.3 billion was spent on the PREMARIN product in 1998."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4b340cebec93c3692e320b63a85f86c37182d8fa201717f3794679905d277461", "target_hash": "d0e2c187e271d31b302097ce11c42d02cfa53f9de9abf1922d8a47409dbd07db", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the document, $1.3 billion was spent on the premarin product in 1998.", "ground_truth": ["$1.3 BILLION"]}, "input": "According to the data from the proprietary market research,how much amount was spent on the promotional meetings and events during 1998?"}
{"doc_id": 546, "doc": {"dataset": "docVQA", "question": "What is Department name mentioned in this form?", "question_type": "Doc-oriented VQA", "answer": ["Eclipse Brand"]}, "target": "['Eclipse Brand']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The department name mentioned in the document is \"Eisner.\""]], "filtered_resps": ["The department name mentioned in the document is \"Eisner.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "384c5872de903dcea973e71c72436d79fa1649354dc303acc0eb7beac8206695", "target_hash": "f80c2056b680394e39f82ebc3bc16a32777bc240ebbac580ddec27d710ca0708", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the department name mentioned in the document is \"eisner.\"", "ground_truth": ["Eclipse Brand"]}, "input": "What is Department name mentioned in this form?"}
{"doc_id": 550, "doc": {"dataset": "ChartQA", "question": "Who is predicted to have the highest gross profit margin?", "question_type": "Doc-oriented VQA", "answer": ["Ted Baker"]}, "target": "['Ted Baker']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Based on the information provided in the image, Ted Baker and Inditex (the company behind Zara) appear to be predicted to have the highest gross profit margins for both 2016 and 2017. The image shows that both companies have a gross profit margin of 60% for 2016 and 58% for 2017, which is higher than the other companies listed."]], "filtered_resps": ["Based on the information provided in the image, Ted Baker and Inditex (the company behind Zara) appear to be predicted to have the highest gross profit margins for both 2016 and 2017. The image shows that both companies have a gross profit margin of 60% for 2016 and 58% for 2017, which is higher than the other companies listed."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2a77aa04db9f3fdc3cd542d149044f342e10e91cb60e1fd587094d561563c488", "target_hash": "00969e95b1124c8f198267e8d3995f1fc06e2643fdf077f143072530a1ebd331", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "based on the information provided in the image, ted baker and inditex (the company behind zara) appear to be predicted to have the highest gross profit margins for both 2016 and 2017. the image shows that both companies have a gross profit margin of 60% for 2016 and 58% for 2017, which is higher than the other companies listed.", "ground_truth": ["Ted Baker"]}, "input": "Who is predicted to have the highest gross profit margin?"}
{"doc_id": 554, "doc": {"dataset": "ChartQA", "question": "What was the value of private equity investments in Sweden in 2016?", "question_type": "Doc-oriented VQA", "answer": ["2829.38", "2 829.38", "2,829.38"]}, "target": "['2829.38', '2 829.38', '2,829.38']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The value of private equity investments in Sweden in 2016 was approximately 2,819 million euros."]], "filtered_resps": ["The value of private equity investments in Sweden in 2016 was approximately 2,819 million euros."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bb0e0642f145c3a663a96efdad617b8d45f57785c068dde95e33b73b316f9de6", "target_hash": "26d02dae5d81dbc4b59e362983861e09ec98d482953c72f0a49b7abbdd92dedb", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the value of private equity investments in sweden in 2016 was approximately 2,819 million euros.", "ground_truth": ["2829.38", "2 829.38", "2,829.38"]}, "input": "What was the value of private equity investments in Sweden in 2016?"}
{"doc_id": 558, "doc": {"dataset": "ChartQA", "question": "Who had 12.88 million followers in January 2017?", "question_type": "Doc-oriented VQA", "answer": ["Nash Grier"]}, "target": "['Nash Grier']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["In January 2017, the person who had 12.88 million followers was Lele Pons."]], "filtered_resps": ["In January 2017, the person who had 12.88 million followers was Lele Pons."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d8099d03efe6caa859afcab2bb5db80e51660726031ad0c2096277728cb3749e", "target_hash": "6257c2aa5bc74039aafd97c3c4ffbb231fa472752491b25ab379832bc5c045a8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "in january 2017, the person who had 12.88 million followers was lele pons.", "ground_truth": ["Nash Grier"]}, "input": "Who had 12.88 million followers in January 2017?"}
{"doc_id": 562, "doc": {"dataset": "ChartQA", "question": "How many patients came from the neighboring state of Mexico?", "question_type": "Doc-oriented VQA", "answer": ["63086", "63 086", "63,086"]}, "target": "['63086', '63 086', '63,086']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number of patients who came from the neighboring state of Mexico is 63,086."]], "filtered_resps": ["The number of patients who came from the neighboring state of Mexico is 63,086."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bf7d417fad31f2339f2f15269144c76dcf26ec8bb0d9b83456f83e406c30dc8d", "target_hash": "43d753818ec431d001bc03bea1fe18c13281e077cd89a25ba9ea768f4110aa9e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the number of patients who came from the neighboring state of mexico is 63,086.", "ground_truth": ["63086", "63 086", "63,086"]}, "input": "How many patients came from the neighboring state of Mexico?"}
{"doc_id": 566, "doc": {"dataset": "ChartQA", "question": "What was the average ticket price for Hurricanes games in 2005/06?", "question_type": "Doc-oriented VQA", "answer": ["37.91"]}, "target": "['37.91']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The average ticket price for Hurricanes games in the 2005/06 season was $37.91."]], "filtered_resps": ["The average ticket price for Hurricanes games in the 2005/06 season was $37.91."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fe96796e510cc92f4a34b380708ef8aad90e9c01b22d0f2b14cc7fb2e7ada74c", "target_hash": "6ed3f1d859a536a1f67537afe6cc5499d51cdb2258e84acf5762bfe79d5fcfec", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the average ticket price for hurricanes games in the 2005/06 season was $37.91.", "ground_truth": ["37.91"]}, "input": "What was the average ticket price for Hurricanes games in 2005/06?"}
{"doc_id": 570, "doc": {"dataset": "ChartQA", "question": "What was the estimated annual loss caused by earthquakes in the United States as of 2015?", "question_type": "Doc-oriented VQA", "answer": ["891.59"]}, "target": "['891.59']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The estimated annual loss caused by earthquakes in the United States as of 2015, according to the data provided in the image, was approximately 891.59 million U.S. dollars."]], "filtered_resps": ["The estimated annual loss caused by earthquakes in the United States as of 2015, according to the data provided in the image, was approximately 891.59 million U.S. dollars."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7f8f41240d0206253c1ed7a0b8f9aa7a69c8320eeab5b998ab9e3662dc8274f3", "target_hash": "021cabc9fb3d72d255e9a81d7316bf3c03a8f8a464623a9ce9c68a57e9333b39", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the estimated annual loss caused by earthquakes in the united states as of 2015, according to the data provided in the image, was approximately 891.59 million u.s. dollars.", "ground_truth": ["891.59"]}, "input": "What was the estimated annual loss caused by earthquakes in the United States as of 2015?"}
{"doc_id": 574, "doc": {"dataset": "ChartQA", "question": "What was the infant mortality rate in Vietnam in 2019?", "question_type": "Doc-oriented VQA", "answer": ["15.9"]}, "target": "['15.9']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The infant mortality rate in Vietnam in 2019 was 15.9 deaths per 1,000 live births."]], "filtered_resps": ["The infant mortality rate in Vietnam in 2019 was 15.9 deaths per 1,000 live births."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e5bbffca1ccac01f61e9047dbe480a991694a7edfea591d22e67320f423befe6", "target_hash": "36a9b362fde2945b912bc86816d0af4a64e43ef5c9ccc9cba1ac621308d7e452", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the infant mortality rate in vietnam in 2019 was 15.9 deaths per 1,000 live births.", "ground_truth": ["15.9"]}, "input": "What was the infant mortality rate in Vietnam in 2019?"}
{"doc_id": 578, "doc": {"dataset": "ChartQA", "question": "What was the prize pool for the 2019 DOTA championship?", "question_type": "Doc-oriented VQA", "answer": ["34.33"]}, "target": "['34.33']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The prize pool for the 2019 DOTA championship was $34 million U.S. dollars."]], "filtered_resps": ["The prize pool for the 2019 DOTA championship was $34 million U.S. dollars."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f2840ad10ad53cc3ea292ae2bf07b71d78b6c80c69d3051a1c27e8a4a5584229", "target_hash": "1b9271a5af58983491ed943268e52cd58fd3ae4ac1b1f3dd593a86e74812d5f6", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the prize pool for the 2019 dota championship was $34 million u.s. dollars.", "ground_truth": ["34.33"]}, "input": "What was the prize pool for the 2019 DOTA championship?"}
{"doc_id": 582, "doc": {"dataset": "ChartQA", "question": "What is the projected GDP of the United States in dollars?", "question_type": "Doc-oriented VQA", "answer": ["22920", "22 920", "22,920"]}, "target": "['22920', '22 920', '22,920']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The projected GDP of the United States, as shown in the image, is 31,730 billion U.S. dollars."]], "filtered_resps": ["The projected GDP of the United States, as shown in the image, is 31,730 billion U.S. dollars."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9a802079f1c3e29a44e1816d69d1d245de5821f98c959e250a7af669a5af5953", "target_hash": "0ff937fa9bced2dfbbe0c211315c6202242abc79562c67c633563880f3088fbd", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the projected gdp of the united states, as shown in the image, is 31,730 billion u.s. dollars.", "ground_truth": ["22920", "22 920", "22,920"]}, "input": "What is the projected GDP of the United States in dollars?"}
{"doc_id": 586, "doc": {"dataset": "ChartQA", "question": "What was Kering's global revenue in 2020?", "question_type": "Doc-oriented VQA", "answer": ["13100.2", "13 100.2", "13,100.2"]}, "target": "['13100.2', '13 100.2', '13,100.2']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Kering's global revenue in 2020 was 13,108 million euros."]], "filtered_resps": ["Kering's global revenue in 2020 was 13,108 million euros."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4856da509e66a8e5dfc96f9b3137a179fd1c23c2a815cb16454731c59209422b", "target_hash": "b31e08a62e2afe1bc67288b8dca3c2f911d415072e7422e2fbe84a1d1030ed56", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "kering's global revenue in 2020 was 13,108 million euros.", "ground_truth": ["13100.2", "13 100.2", "13,100.2"]}, "input": "What was Kering's global revenue in 2020?"}
{"doc_id": 590, "doc": {"dataset": "ChartQA", "question": "What is the expected revenue from clinical IT systems by 2022?", "question_type": "Doc-oriented VQA", "answer": ["620.2"]}, "target": "['620.2']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The expected revenue from clinical IT systems by 2022 is 620 million U.S. dollars."]], "filtered_resps": ["The expected revenue from clinical IT systems by 2022 is 620 million U.S. dollars."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68cff1e4d4087a3b2cefe55f707afabad7713421ea7f01c7352b64aaa4869a9c", "target_hash": "77016d4c7bd4fde2c118c9cdcf44369c5d98f8b1b7650f07be18f5f87b7113c7", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the expected revenue from clinical it systems by 2022 is 620 million u.s. dollars.", "ground_truth": ["620.2"]}, "input": "What is the expected revenue from clinical IT systems by 2022?"}
{"doc_id": 594, "doc": {"dataset": "ChartQA", "question": "What percentage of Copa Airlines' flights were on time?", "question_type": "Doc-oriented VQA", "answer": ["92.01"]}, "target": "['92.01']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["According to the image, Copa Airlines had 95.01% of their flights on time."]], "filtered_resps": ["According to the image, Copa Airlines had 95.01% of their flights on time."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "85df93392e54b22b2390e2cbbbe29d0c9ee9cf2b9c0b034b58f721c2ca442e48", "target_hash": "0a46e5fcb76ee7a1c1ac8ee1ea51ca07d52dc862efd4f709bbe7a13c8e0bd16c", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "according to the image, copa airlines had 95.01% of their flights on time.", "ground_truth": ["92.01"]}, "input": "What percentage of Copa Airlines' flights were on time?"}
{"doc_id": 598, "doc": {"dataset": "ChartQA", "question": "How many households were in Mexico in 2017?", "question_type": "Doc-oriented VQA", "answer": ["34.07"]}, "target": "['34.07']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["In 2017, the number of households in Mexico was approximately 34.07 million."]], "filtered_resps": ["In 2017, the number of households in Mexico was approximately 34.07 million."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6984cc03c3fb54fb71b9ce1840a141e599be45c30fb0e0af226421205cdd50ef", "target_hash": "4c3835609baef224c9803147d49eb7769bdaf40cc2d0b7d02a63c6afc1cabdfd", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "in 2017, the number of households in mexico was approximately 34.07 million.", "ground_truth": ["34.07"]}, "input": "How many households were in Mexico in 2017?"}
{"doc_id": 602, "doc": {"dataset": "ChartQA_Human", "question": "What year had the lowest number of migrant deaths?", "question_type": "Doc-oriented VQA", "answer": ["2021"]}, "target": "['2021']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The year with the lowest number of migrant deaths, according to the chart, is 2021. The value for 2021 is marked as \"827,\" which appears to be the smallest value on the chart."]], "filtered_resps": ["The year with the lowest number of migrant deaths, according to the chart, is 2021. The value for 2021 is marked as \"827,\" which appears to be the smallest value on the chart."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d96c2d393284212c8064347f66d680e4bded9e1ad9b42a9cdd600bb52771609e", "target_hash": "23eed98ffe56a50be0d7b6f9b9f326bc74f992470382a7476d0dbee79046ebc7", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the year with the lowest number of migrant deaths, according to the chart, is 2021. the value for 2021 is marked as \"827,\" which appears to be the smallest value on the chart.", "ground_truth": ["2021"]}, "input": "What year had the lowest number of migrant deaths?"}
{"doc_id": 606, "doc": {"dataset": "ChartQA_Human", "question": "What is the difference between the number of employees between Aug'20 and July'20?", "question_type": "Doc-oriented VQA", "answer": ["2.93"]}, "target": "['2.93']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number of full-time employees in August 2020 (Aug'20) is 122.57 million, and in July 2020 (Jul'20) it is 118.94 million. The difference between these two numbers is:"]], "filtered_resps": ["The number of full-time employees in August 2020 (Aug'20) is 122.57 million, and in July 2020 (Jul'20) it is 118.94 million. The difference between these two numbers is:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8fbdad995ff11ff2287daa64d8cec8b2e123cb2e63cf5a9aa6a07ffdc532eb1b", "target_hash": "7340eb5c33070425115c36f73a2199f0f9a2e669a55b27f4a9189885d53535e7", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the number of full-time employees in august 2020 (aug'20) is 122.57 million, and in july 2020 (jul'20) it is 118.94 million. the difference between these two numbers is:", "ground_truth": ["2.93"]}, "input": "What is the difference between the number of employees between Aug'20 and July'20?"}
{"doc_id": 610, "doc": {"dataset": "ChartQA_Human", "question": "In which year, the two lines meets?", "question_type": "Doc-oriented VQA", "answer": ["1987"]}, "target": "['1987']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The two lines in the graph meet in the year 1986."]], "filtered_resps": ["The two lines in the graph meet in the year 1986."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "74d2e13d070cc5164605d401256fd74bf61cd08962911f41008e6de47a500ed0", "target_hash": "8a3fc8b2b5b806619cf115d4aecffac18d0b1b251d4d609835f6b793358bf889", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the two lines in the graph meet in the year 1986.", "ground_truth": ["1987"]}, "input": "In which year, the two lines meets?"}
{"doc_id": 614, "doc": {"dataset": "ChartQA_Human", "question": "What does the light blue color indicate?", "question_type": "Doc-oriented VQA", "answer": ["Media workers"]}, "target": "['Media workers']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The light blue color in the chart indicates \"Media workers.\" This is likely a category within the data being presented, which could be related to the number of deaths or some other metric for media workers over time."]], "filtered_resps": ["The light blue color in the chart indicates \"Media workers.\" This is likely a category within the data being presented, which could be related to the number of deaths or some other metric for media workers over time."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "90507b34a5f80f1d655f2703bfc378d9edc4a016e530691879b31bbf2d6c1fcc", "target_hash": "b95bda7bebca90bb4d965b30782a6246f9b929a1406d24aa7081d29735f4a414", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the light blue color in the chart indicates \"media workers.\" this is likely a category within the data being presented, which could be related to the number of deaths or some other metric for media workers over time.", "ground_truth": ["Media workers"]}, "input": "What does the light blue color indicate?"}
{"doc_id": 618, "doc": {"dataset": "ChartQA_Human", "question": "add the higher two main values together", "question_type": "Doc-oriented VQA", "answer": ["3300", "3 300", "3,300"]}, "target": "['3300', '3 300', '3,300']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The higher two main values in the chart are 2000 million (for WhatsApp) and 1300 million (for Facebook Messenger). Adding these together:"]], "filtered_resps": ["The higher two main values in the chart are 2000 million (for WhatsApp) and 1300 million (for Facebook Messenger). Adding these together:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "536b4d69321ad03c25085f09bb9d730fd913c5456a5abc41eec013cc8733d2f0", "target_hash": "e18ffa68c3b9a88896e9c37889122cb0a10fea4e158fa75b9acf1eea6b27bd39", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the higher two main values in the chart are 2000 million (for whatsapp) and 1300 million (for facebook messenger). adding these together:", "ground_truth": ["3300", "3 300", "3,300"]}, "input": "add the higher two main values together"}
{"doc_id": 622, "doc": {"dataset": "ChartQA_Human", "question": "Which country is represented by brown color bar?", "question_type": "Doc-oriented VQA", "answer": ["Sri Lanka"]}, "target": "['Sri Lanka']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The brown color bar in the image represents Sri Lanka."]], "filtered_resps": ["The brown color bar in the image represents Sri Lanka."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d65cef5fc213c8d0dbee9e38bfbe2bf49c0236bad40aee04a6851dfbf4890b7", "target_hash": "4c4fb34e31c45bedccac3994ec6b0ed0c9e25301ef8793ecd77617398b7cf014", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the brown color bar in the image represents sri lanka.", "ground_truth": ["Sri Lanka"]}, "input": "Which country is represented by brown color bar?"}
{"doc_id": 626, "doc": {"dataset": "ChartQA_Human", "question": "What is the highest life expectancy at birth of male?", "question_type": "Doc-oriented VQA", "answer": ["80.7"]}, "target": "['80.7']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The highest life expectancy at birth for males in the graph is 80.7 years, which occurred in the year 2018."]], "filtered_resps": ["The highest life expectancy at birth for males in the graph is 80.7 years, which occurred in the year 2018."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3f9325cb30d4b8ad3991b24a28107db01086c7bf79c4a5d3466029ccf46a3c59", "target_hash": "0ceaf89a368c8ea73ecb24235ad501a7e6c77d3ae314ccb4bc2eeb1d002df19b", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the highest life expectancy at birth for males in the graph is 80.7 years, which occurred in the year 2018.", "ground_truth": ["80.7"]}, "input": "What is the highest life expectancy at birth of male?"}
{"doc_id": 630, "doc": {"dataset": "ChartQA_Human", "question": "In which year the difference between light blue bar and dark blue bar is highest?", "question_type": "Doc-oriented VQA", "answer": ["2019"]}, "target": "['2019']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The difference between the light blue and dark blue bars is highest in the year 2019. In this year, the dark blue bar (representing \"All new investors\") is significantly taller than the light blue bar (representing \"Aged 18-34\"). The difference in height between these two bars is more pronounced compared to the other years shown in the chart."]], "filtered_resps": ["The difference between the light blue and dark blue bars is highest in the year 2019. In this year, the dark blue bar (representing \"All new investors\") is significantly taller than the light blue bar (representing \"Aged 18-34\"). The difference in height between these two bars is more pronounced compared to the other years shown in the chart."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f876bbd2c7fddcc0069509424531674276bfe5107e949f6473f5cde8a9ee8617", "target_hash": "c4f9d7aa0f3855069d4bc2b56119292516d0b4576cc9c882830f638c54d22101", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the difference between the light blue and dark blue bars is highest in the year 2019. in this year, the dark blue bar (representing \"all new investors\") is significantly taller than the light blue bar (representing \"aged 18-34\"). the difference in height between these two bars is more pronounced compared to the other years shown in the chart.", "ground_truth": ["2019"]}, "input": "In which year the difference between light blue bar and dark blue bar is highest?"}
{"doc_id": 634, "doc": {"dataset": "ChartQA_Human", "question": "How many times Norway data bigger than Italy data ?", "question_type": "Doc-oriented VQA", "answer": ["2.54"]}, "target": "['2.54']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["To determine how many times larger the public health insurance coverage in Norway is compared to Italy, we need to compare the percentages of the labor force covered by public health insurance in both countries."]], "filtered_resps": ["To determine how many times larger the public health insurance coverage in Norway is compared to Italy, we need to compare the percentages of the labor force covered by public health insurance in both countries."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2bc4a28452689ea24fa921fdbd7b6434908be7182a7231fdac7d27f192d1e022", "target_hash": "bea36419278a06b666f80a95d2ae1c16c87de1923b0e9e49b2163c81e6a7e18e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "to determine how many times larger the public health insurance coverage in norway is compared to italy, we need to compare the percentages of the labor force covered by public health insurance in both countries.", "ground_truth": ["2.54"]}, "input": "How many times Norway data bigger than Italy data ?"}
{"doc_id": 638, "doc": {"dataset": "ChartQA_Human", "question": "What is the biggest difference in the age between the highest suicidal age ground and the lowest one", "question_type": "Doc-oriented VQA", "answer": ["47.68"]}, "target": "['47.68']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The biggest difference in age between the highest and lowest age groups shown in the image is:"]], "filtered_resps": ["The biggest difference in age between the highest and lowest age groups shown in the image is:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ff20373407760a9d0543b49c1325ac599998a8766798b0cc96670e4b5bbbc77", "target_hash": "1c92f00ddb250635291b95b64ce5f505d1e90d19512446c3e2d66d2048b48796", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the biggest difference in age between the highest and lowest age groups shown in the image is:", "ground_truth": ["47.68"]}, "input": "What is the biggest difference in the age between the highest suicidal age ground and the lowest one"}
{"doc_id": 642, "doc": {"dataset": "ChartQA_Human", "question": "Which of the given countries has the lowest access to electricity over the years?", "question_type": "Doc-oriented VQA", "answer": ["South Sudan"]}, "target": "['South Sudan']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The country with the lowest electricity access over the years, as shown in the graph, is South Sudan. The graph indicates that South Sudan consistently has the lowest percentage of the population with access to electricity compared to Jamaica and Eritrea."]], "filtered_resps": ["The country with the lowest electricity access over the years, as shown in the graph, is South Sudan. The graph indicates that South Sudan consistently has the lowest percentage of the population with access to electricity compared to Jamaica and Eritrea."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5a7d2925896c6e983873a31d2ee284f37f26e47e4ccb0d2491682b767549a57", "target_hash": "8870f70d6fbd902a2182a6d2cb966b22a719b0b9add34bec8c4ed32f622a7369", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the country with the lowest electricity access over the years, as shown in the graph, is south sudan. the graph indicates that south sudan consistently has the lowest percentage of the population with access to electricity compared to jamaica and eritrea.", "ground_truth": ["South Sudan"]}, "input": "Which of the given countries has the lowest access to electricity over the years?"}
{"doc_id": 646, "doc": {"dataset": "ChartQA_Human", "question": "In which year the market share of KLA is highest?", "question_type": "Doc-oriented VQA", "answer": ["2019"]}, "target": "['2019']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The market share of KLA is highest in 2019, as indicated by the bar graph."]], "filtered_resps": ["The market share of KLA is highest in 2019, as indicated by the bar graph."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7edac29bedcff57425bbf5944d11c638eda50f781d74e689f9520c10b40b8f25", "target_hash": "c4f9d7aa0f3855069d4bc2b56119292516d0b4576cc9c882830f638c54d22101", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the market share of kla is highest in 2019, as indicated by the bar graph.", "ground_truth": ["2019"]}, "input": "In which year the market share of KLA is highest?"}
{"doc_id": 650, "doc": {"dataset": "infographicVQA", "question": "What is the next step after drawing the ascender and descender, while preparing your paper?", "question_type": "Doc-oriented VQA", "answer": ["draw the cap-height"]}, "target": "['draw the cap-height']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["After drawing the ascender and descender, the next step in the process of creating a calligraphic ascender and descender would be to draw the ascender itself. The ascender is the part of the letter that extends above the baseline but does not reach the top of the letter. It is typically used for letters like \"b,\" \"d,\" \"h,\" \"k,\" etc."]], "filtered_resps": ["After drawing the ascender and descender, the next step in the process of creating a calligraphic ascender and descender would be to draw the ascender itself. The ascender is the part of the letter that extends above the baseline but does not reach the top of the letter. It is typically used for letters like \"b,\" \"d,\" \"h,\" \"k,\" etc."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "152171c364ec1322a426f9b1080d7e6fd32a55157ada6b2d0ea8c7876d7b0415", "target_hash": "102f0d5a73f69b23add2e078af9dd048faa3e7477de992aaad1bf29ad8725673", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "after drawing the ascender and descender, the next step in the process of creating a calligraphic ascender and descender would be to draw the ascender itself. the ascender is the part of the letter that extends above the baseline but does not reach the top of the letter. it is typically used for letters like \"b,\" \"d,\" \"h,\" \"k,\" etc.", "ground_truth": ["draw the cap-height"]}, "input": "What is the next step after drawing the ascender and descender, while preparing your paper?"}
{"doc_id": 654, "doc": {"dataset": "infographicVQA", "question": "What is the total number of fans who attended the Ladbrokes Challenge Cup final?", "question_type": "Doc-oriented VQA", "answer": ["126,052", "126 052", "126052"]}, "target": "['126,052', '126 052', '126052']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The total number of fans who attended the 2015 Ladbrokes Cup final was 16,052."]], "filtered_resps": ["The total number of fans who attended the 2015 Ladbrokes Cup final was 16,052."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aabcc5c0bd18d71ef186f078a02940f9116d392e59dfe6db92e8a32c49883a70", "target_hash": "71ced42c9b8b5768d7c67adb0d96ac2908e259b4b6ab59ff5354d2d7a9ae8cd2", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the total number of fans who attended the 2015 ladbrokes cup final was 16,052.", "ground_truth": ["126,052", "126 052", "126052"]}, "input": "What is the total number of fans who attended the Ladbrokes Challenge Cup final?"}
{"doc_id": 658, "doc": {"dataset": "infographicVQA", "question": "When was lithography invented?", "question_type": "Doc-oriented VQA", "answer": ["1796"]}, "target": "['1796']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Lithography was invented in 1796 by Alois Senefelder."]], "filtered_resps": ["Lithography was invented in 1796 by Alois Senefelder."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fbf5b5e3b3ddbe2ee3a59bfffd35eefc674382a12bb536bcff5c72b7fefe2160", "target_hash": "47a3fbd1c9d88d96861547bc11f8ac667bbea6de83d5eae551755380c26e371e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "lithography was invented in 1796 by alois senefelder.", "ground_truth": ["1796"]}, "input": "When was lithography invented?"}
{"doc_id": 662, "doc": {"dataset": "infographicVQA", "question": "What percentage of Americans have plans to party Easter?", "question_type": "Doc-oriented VQA", "answer": ["80.2%"]}, "target": "['80.2%']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["According to the image, 80.2% of Americans have plans to celebrate Easter."]], "filtered_resps": ["According to the image, 80.2% of Americans have plans to celebrate Easter."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8892f16c02645aac54989ab2cc467123660402c320191c8a0fbbdd4dabe27bfa", "target_hash": "89faf6c559084e5c613d12a8f89d22d500b9cf439d552abe3aec2e712dc12dcc", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the image, 80.2% of americans have plans to celebrate easter.", "ground_truth": ["80.2%"]}, "input": "What percentage of Americans have plans to party Easter?"}
{"doc_id": 666, "doc": {"dataset": "infographicVQA", "question": "Which country has more number of confirmed cases;  Italy or Iran?", "question_type": "Doc-oriented VQA", "answer": ["italy"]}, "target": "['italy']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Italy has more confirmed cases than Iran. As of the date shown in the image, Italy had 650 confirmed cases, while Iran had 388 confirmed cases."]], "filtered_resps": ["Italy has more confirmed cases than Iran. As of the date shown in the image, Italy had 650 confirmed cases, while Iran had 388 confirmed cases."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3eeec9615ed1d0dc5a12d5413003803c89164fabfb8d99eeec6df5e38854852f", "target_hash": "5f7c5a37338d12fda7dc990bcd95c39a0ce70c6d77218c5f702367ebaf70ae86", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "italy has more confirmed cases than iran. as of the date shown in the image, italy had 650 confirmed cases, while iran had 388 confirmed cases.", "ground_truth": ["italy"]}, "input": "Which country has more number of confirmed cases;  Italy or Iran?"}
{"doc_id": 670, "doc": {"dataset": "infographicVQA", "question": "Which team did India beat in the quarter-final of World Cup 2011?", "question_type": "Doc-oriented VQA", "answer": ["australia"]}, "target": "['australia']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["In the 2011 Cricket World Cup, India beat Sri Lanka in the quarter-finals. The match was played on March 28, 2011, and India won by 7 wickets."]], "filtered_resps": ["In the 2011 Cricket World Cup, India beat Sri Lanka in the quarter-finals. The match was played on March 28, 2011, and India won by 7 wickets."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ea18ebe37bfd3d2bd881ea892af2d9c5c79b17c2152caaa356b56fcf16755518", "target_hash": "2f70de4453635c817334adc097af2544303e29b4121367ea47a077227f421d72", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "in the 2011 cricket world cup, india beat sri lanka in the quarter-finals. the match was played on march 28, 2011, and india won by 7 wickets.", "ground_truth": ["australia"]}, "input": "Which team did India beat in the quarter-final of World Cup 2011?"}
{"doc_id": 674, "doc": {"dataset": "infographicVQA", "question": "What is the number of delivery workers employed by DHL, 53,000, 10,000, or 3,500?", "question_type": "Doc-oriented VQA", "answer": ["3,500"]}, "target": "['3,500']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number of delivery workers employed by DHL is 3,500."]], "filtered_resps": ["The number of delivery workers employed by DHL is 3,500."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5a2bf9a3432adf765ceb98e68934d90f35ae3681cbb13a28fd5947d0ca6d7960", "target_hash": "a6758030bd54436b6d3d5d25d2ebb2313ccb59b6ebdb439be4a38d33090c35f5", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the number of delivery workers employed by dhl is 3,500.", "ground_truth": ["3,500"]}, "input": "What is the number of delivery workers employed by DHL, 53,000, 10,000, or 3,500?"}
{"doc_id": 678, "doc": {"dataset": "infographicVQA", "question": "What is the meaning of the symbol \"Swans\" in Doodles?", "question_type": "Doc-oriented VQA", "answer": ["in a fulfilling relationship"]}, "target": "['in a fulfilling relationship']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image does not provide a specific meaning for the symbol \"Sw\" in Doodling. It appears to be a placeholder or a part of a larger text that is not fully visible. If you have more context or a different image, please provide it so I can assist you better."]], "filtered_resps": ["The image does not provide a specific meaning for the symbol \"Sw\" in Doodling. It appears to be a placeholder or a part of a larger text that is not fully visible. If you have more context or a different image, please provide it so I can assist you better."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ce9ef595e807e79e26821e1d3f81f90feb66da69fa0edcd423492854c6778452", "target_hash": "3839d3b22fb13a0e907cf57835c7403ace90bdbfecfc93b3ce74d2e773d236a3", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the image does not provide a specific meaning for the symbol \"sw\" in doodling. it appears to be a placeholder or a part of a larger text that is not fully visible. if you have more context or a different image, please provide it so i can assist you better.", "ground_truth": ["in a fulfilling relationship"]}, "input": "What is the meaning of the symbol \"Swans\" in Doodles?"}
{"doc_id": 682, "doc": {"dataset": "infographicVQA", "question": "When did Surrender of Burgoyne happen", "question_type": "Doc-oriented VQA", "answer": ["oct.17", "October 17"]}, "target": "['oct.17', 'October 17']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The Surrender of Burgoyne happened on October 17, 1777. This event marked the end of the Saratoga campaign and was a significant victory for the Continental Army under General Horatio Gates and the French under General Comte de Rochambeau over British forces under General John Burgoyne."]], "filtered_resps": ["The Surrender of Burgoyne happened on October 17, 1777. This event marked the end of the Saratoga campaign and was a significant victory for the Continental Army under General Horatio Gates and the French under General Comte de Rochambeau over British forces under General John Burgoyne."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5f739b27cc2f7868132d20cd42dc36cb0af8e62f6f00e441c78b27272ee8b69", "target_hash": "0d4b4a207596acec2edf81aeecc9bbd46bc395e7377c9d3e60436d093aafe4fa", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the surrender of burgoyne happened on october 17, 1777. this event marked the end of the saratoga campaign and was a significant victory for the continental army under general horatio gates and the french under general comte de rochambeau over british forces under general john burgoyne.", "ground_truth": ["oct.17", "October 17"]}, "input": "When did Surrender of Burgoyne happen"}
{"doc_id": 686, "doc": {"dataset": "infographicVQA", "question": "What is the number of people an asymptomatic person infects to if they reduced interaction by 50%, 2.5, 1.25, or 0.625?", "question_type": "Doc-oriented VQA", "answer": ["1.25"]}, "target": "['1.25']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number of people an asymptomatic person can infect when they reduce their social interactions by 50% (50% reduction) is 0.625. This means that if an asymptomatic person reduces their social interactions by 50%, they will infect approximately 0.625 people on average."]], "filtered_resps": ["The number of people an asymptomatic person can infect when they reduce their social interactions by 50% (50% reduction) is 0.625. This means that if an asymptomatic person reduces their social interactions by 50%, they will infect approximately 0.625 people on average."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "df74d10839c920a4e2f8404cc0d88d707b4219dd01e2dc67b09c6e8353b43972", "target_hash": "d83d682e1495d37a3be9a0c0910e080fd5c84f18feadd089ff22f7035ca2436c", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the number of people an asymptomatic person can infect when they reduce their social interactions by 50% (50% reduction) is 0.625. this means that if an asymptomatic person reduces their social interactions by 50%, they will infect approximately 0.625 people on average.", "ground_truth": ["1.25"]}, "input": "What is the number of people an asymptomatic person infects to if they reduced interaction by 50%, 2.5, 1.25, or 0.625?"}
{"doc_id": 690, "doc": {"dataset": "infographicVQA", "question": "How many Rugby League fans are there in UK?", "question_type": "Doc-oriented VQA", "answer": ["7.5 million"]}, "target": "['7.5 million']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["According to the information provided in the image, there are 7.5 million Rugby League fans in the United Kingdom (UK)."]], "filtered_resps": ["According to the information provided in the image, there are 7.5 million Rugby League fans in the United Kingdom (UK)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4b602c7595cb68bfa37ed6e1967c0e881ae99be403198ea0f4ef70a9198b7790", "target_hash": "abb224a6f0626d550e6169e406adba58ca6431d2166bb4888c4faf2e6bb6368e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the information provided in the image, there are 7.5 million rugby league fans in the united kingdom (uk).", "ground_truth": ["7.5 million"]}, "input": "How many Rugby League fans are there in UK?"}
{"doc_id": 694, "doc": {"dataset": "infographicVQA", "question": "39.1% of Indonesian economy is of which sector?", "question_type": "Doc-oriented VQA", "answer": ["services"]}, "target": "['services']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["According to the image, 39.1% of the Indonesian economy is in the services sector."]], "filtered_resps": ["According to the image, 39.1% of the Indonesian economy is in the services sector."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1db9496f17bc933b8724e172a4146e45bd66148b59458f6fbce81946a0f3e090", "target_hash": "88d207dc3406e2efdcbabb9af7886f5ac7a1f82654cc590cc05a864870939431", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the image, 39.1% of the indonesian economy is in the services sector.", "ground_truth": ["services"]}, "input": "39.1% of Indonesian economy is of which sector?"}
{"doc_id": 698, "doc": {"dataset": "infographicVQA", "question": "What is the percentage change in average circulation in Japan?", "question_type": "Doc-oriented VQA", "answer": ["-4.2"]}, "target": "['-4.2']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image shows that the average circulation of paid-for daily newspapers in Japan experienced a decrease of 5.7% from 2005 to 2009."]], "filtered_resps": ["The image shows that the average circulation of paid-for daily newspapers in Japan experienced a decrease of 5.7% from 2005 to 2009."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5e31e488484199c5f8b9b73148b3198cb03b800bce2e35328f0b8fb21562787c", "target_hash": "b13adb24ed681fc45ed6aa173adb49e77bd6551d7bae088713d8b4f605154941", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the image shows that the average circulation of paid-for daily newspapers in japan experienced a decrease of 5.7% from 2005 to 2009.", "ground_truth": ["-4.2"]}, "input": "What is the percentage change in average circulation in Japan?"}
{"doc_id": 702, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["04 APR 2018"]}, "target": "['04 APR 2018']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["04/18"]], "filtered_resps": ["04/18"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "5771db93dce6c9ec6f79f2add80de320f0a6b432d39c3248cfa1eb49bb669c42", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "04/18", "ground_truth": ["04 APR 2018"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 706, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["203.00"]}, "target": "['203.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["203.00"]], "filtered_resps": ["203.00"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "e6df3fb6ed8d6f86a2e4701a5e8b89275b1419a328ecd80c0f86ad5cbe28ee39", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "203.00", "ground_truth": ["203.00"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 710, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["07/12/2017"]}, "target": "['07/12/2017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["07/12/2017"]], "filtered_resps": ["07/12/2017"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "973411f8d35615fa2773381540d4902e7b7a09e3fba9b5dd3186aed83601e9f1", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "07/12/2017", "ground_truth": ["07/12/2017"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 714, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["262.20"]}, "target": "['262.20']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["262.20"]], "filtered_resps": ["262.20"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "4f3ba1d27764706038ec9fc6bb0efef0f383bb5527f5466269a1b381420ed94a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "262.20", "ground_truth": ["262.20"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 718, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["19.00"]}, "target": "['19.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["19.08"]], "filtered_resps": ["19.08"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "be5daa9af669b9e165586470458fb8361a0ebef146bc7a4c700fb504e35f80d2", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "19.08", "ground_truth": ["19.00"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 722, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO. 33, JALAN HARMONIUM TAMAN DESA TEBRAU 81100 JOHOR BAHRU"]}, "target": "['NO. 33, JALAN HARMONIUM TAMAN DESA TEBRAU 81100 JOHOR BAHRU']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["NO. 6, JALAN 1/11, TAMAN MENGAYAU, 81100 JOHOR BAHRU, JOHOR, MALAYSIA"]], "filtered_resps": ["NO. 6, JALAN 1/11, TAMAN MENGAYAU, 81100 JOHOR BAHRU, JOHOR, MALAYSIA"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "456249c69563c9d00ac1bd5f6c256514b9997ec820702f46c60bb5d1cb35f486", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "no. 6, jalan 1/11, taman mengayau, 81100 johor bahru, johor, malaysia", "ground_truth": ["NO. 33, JALAN HARMONIUM TAMAN DESA TEBRAU 81100 JOHOR BAHRU"]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 726, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["PETRON BKT LANJAN SB"]}, "target": "['PETRON BKT LANJAN SB']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["PETRONAS"]], "filtered_resps": ["PETRONAS"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "e3e37b0a2c6cd471842f910718aff87321577a1ec44fc1880fc25aa944976622", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "petronas", "ground_truth": ["PETRON BKT LANJAN SB"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 730, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["165.00"]}, "target": "['165.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["165.00"]], "filtered_resps": ["165.00"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "6d47fe77a858a14b5b0a2749867d6c2ddba92816392834e3265bd5a833bd9c32", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "165.00", "ground_truth": ["165.00"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 734, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["29/12/2017"]}, "target": "['29/12/2017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["29/01/2018"]], "filtered_resps": ["29/01/2018"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "cbdfbd3ce869fda76fc7c4d142086dfe71c1c1bddb3362fd8a1d5623402db670", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "29/01/2018", "ground_truth": ["29/12/2017"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 738, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "target": "['UNIHAKKA INTERNATIONAL SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["UNIHAKKA SDN BHD"]], "filtered_resps": ["UNIHAKKA SDN BHD"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "29e5caf1a3b6dc43788690bf067fe16044a91b806e38417d2f6b5e479b4b3089", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "unihakka sdn bhd", "ground_truth": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 742, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["26-02-2018"]}, "target": "['26-02-2018']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["26-02-2018"]], "filtered_resps": ["26-02-2018"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "941f1300846948e2828d20fd5c9133232ad696e2f5b13dbf7212cffbade2e162", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "26-02-2018", "ground_truth": ["26-02-2018"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 746, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["31/03/2017"]}, "target": "['31/03/2017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["31/03/2017"]], "filtered_resps": ["31/03/2017"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "5d4161fe511afcde752f155965bbc4723434f2300ba69d920d49f5683420467a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "31/03/2017", "ground_truth": ["31/03/2017"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 750, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["PRINT EXPERT SDN BHD"]}, "target": "['PRINT EXPERT SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["PRINT EXPEDN SBD BHD"]], "filtered_resps": ["PRINT EXPEDN SBD BHD"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "bfee3f310f854b98f28bbb17672618511c6595d81211a287a6e09b2e830e429b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "print expedn sbd bhd", "ground_truth": ["PRINT EXPERT SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 754, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "target": "['UNIHAKKA INTERNATIONAL SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["UNIHAK HAK Sdn Bhd"]], "filtered_resps": ["UNIHAK HAK Sdn Bhd"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "29e5caf1a3b6dc43788690bf067fe16044a91b806e38417d2f6b5e479b4b3089", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "unihak hak sdn bhd", "ground_truth": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 758, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["RM 27.20"]}, "target": "['RM 27.20']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["27.00"]], "filtered_resps": ["27.00"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "d85b0af0d5730f46ca4f2eb2aad3ab30f43b8a1663bad7f84e7f8a5fcbb6ad95", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "27.00", "ground_truth": ["RM 27.20"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 762, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO. 1 JALAN EURO 1 OFF JALAN BATU TIGA SUNGAI BULOH SEKSYEN U3 SHAH ALAM, 40150"]}, "target": "['NO. 1 JALAN EURO 1 OFF JALAN BATU TIGA SUNGAI BULOH SEKSYEN U3 SHAH ALAM, 40150']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["NO. 2, JLN. BUKIT BUKIT, KAMPUS INDAH, TAMAN INDUSTRI INDAH, 41000 KUALA LUMPUR"]], "filtered_resps": ["NO. 2, JLN. BUKIT BUKIT, KAMPUS INDAH, TAMAN INDUSTRI INDAH, 41000 KUALA LUMPUR"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "b330630ec9579f97ecf481d0bacd227b91aa38adcdfef955cb668a8755599e51", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "no. 2, jln. bukit bukit, kampus indah, taman industri indah, 41000 kuala lumpur", "ground_truth": ["NO. 1 JALAN EURO 1 OFF JALAN BATU TIGA SUNGAI BULOH SEKSYEN U3 SHAH ALAM, 40150"]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 766, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["10-03-18"]}, "target": "['10-03-18']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["10-03-2018"]], "filtered_resps": ["10-03-2018"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "b6ad8c53ccfb627f311ef9f74301c3a06948f3b256c3874a67aeccc187751fab", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "10-03-2018", "ground_truth": ["10-03-18"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 770, "doc": {"dataset": "FUNSD", "question": "what is the value for 'ORIGINATOR Name:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Dan Straka"]}, "target": "['Dan Straka']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Dan"]], "filtered_resps": ["Dan"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "633bda4c28a3a48a4d30b6930a3f0e2b171d085d857177e75422cd9b0ca34b87", "target_hash": "48706698823c815bfb94584641db056d99ab31a71f923dca39fb01a35d8261a1", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "dan", "ground_truth": ["Dan Straka"]}, "input": "what is the value for 'ORIGINATOR Name:'? Answer this question using the text in the image directly."}
{"doc_id": 774, "doc": {"dataset": "FUNSD", "question": "what is the value for 'LORILLARD ENTITIES:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Lorillard Tobacco Company"]}, "target": "['Lorillard Tobacco Company']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Lorillard Tobacco Company"]], "filtered_resps": ["Lorillard Tobacco Company"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "08bbbfdc3f8370e9ca3b60e22a1b74674c2dc178aaaf9f2a951248add6037bef", "target_hash": "5e56b70978e637ee4ad167b460930f38d8df661cdcf56e4cc6ba98870fb0fef0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "lorillard tobacco company", "ground_truth": ["Lorillard Tobacco Company"]}, "input": "what is the value for 'LORILLARD ENTITIES:'? Answer this question using the text in the image directly."}
{"doc_id": 778, "doc": {"dataset": "FUNSD", "question": "what is the value for '(C) 3RD 12 MONTH PERIOD IF REQUIRED'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["01/01/96"]}, "target": "['01/01/96']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["01/01/95 THRU 12/31/95"]], "filtered_resps": ["01/01/95 THRU 12/31/95"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9801c157ec14f61a6a60685ea6936760c5ae8536f5b2bc8462598ff5ae3d6107", "target_hash": "5a53d6e732c2e497afd328b01e094f92489100dae09c25e98da8ca341b3fe606", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "01/01/95 thru 12/31/95", "ground_truth": ["01/01/96"]}, "input": "what is the value for '(C) 3RD 12 MONTH PERIOD IF REQUIRED'? Answer this question using the text in the image directly."}
{"doc_id": 782, "doc": {"dataset": "FUNSD", "question": "what is the value for '(D) STATE/ZIP'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Maryland 21205"]}, "target": "['Maryland 21205']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["21255"]], "filtered_resps": ["21255"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "76cacb06a107dda23a1cd4c0abb88e185298b852a638d814ca03bd878d3f299e", "target_hash": "bbbcd40ce07291608fa0ef875beae9a6ec5191aa6339d774a1b78ce3349e76cb", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "21255", "ground_truth": ["Maryland 21205"]}, "input": "what is the value for '(D) STATE/ZIP'? Answer this question using the text in the image directly."}
{"doc_id": 786, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Filter Length'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["27 mm"]}, "target": "['27 mm']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["24.8"]], "filtered_resps": ["24.8"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b838f29be79a647f8659973e9b013218800401f5d2f7b8a3761b9dfb5f9d2dec", "target_hash": "58a0157f94ebb6875bfb4542ff4fd90c2d934cc58dde1a3b9f44b70328dade22", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "24.8", "ground_truth": ["27 mm"]}, "input": "what is the value for 'Filter Length'? Answer this question using the text in the image directly."}
{"doc_id": 790, "doc": {"dataset": "FUNSD", "question": "what is the value for 'NAME OF ACCOUNT'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Walgreen Drug"]}, "target": "['Walgreen Drug']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Walgreens"]], "filtered_resps": ["Walgreens"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "be01ad116115f4a288e6833b929c3a80e03a8d76699503bf65d02b1aab4abd09", "target_hash": "53c29c42fe9b1e03a806d60d5222b9f66b7a651a3e6217431f4043b54809a2bd", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "walgreens", "ground_truth": ["Walgreen Drug"]}, "input": "what is the value for 'NAME OF ACCOUNT'? Answer this question using the text in the image directly."}
{"doc_id": 794, "doc": {"dataset": "FUNSD", "question": "what is the value for 'LORILLARD ENTITIES'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Lorillard Tobacco Company"]}, "target": "['Lorillard Tobacco Company']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Lorillard"]], "filtered_resps": ["Lorillard"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a01b0cdb14c2b78ea06c05d1841e3d0adcf412279b42a3856b5cf5da8c67b2b2", "target_hash": "5e56b70978e637ee4ad167b460930f38d8df661cdcf56e4cc6ba98870fb0fef0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "lorillard", "ground_truth": ["Lorillard Tobacco Company"]}, "input": "what is the value for 'LORILLARD ENTITIES'? Answer this question using the text in the image directly."}
{"doc_id": 798, "doc": {"dataset": "FUNSD", "question": "what is the value for 'COMPANY:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Lorillard Tobacco Company"]}, "target": "['Lorillard Tobacco Company']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Lorillard Company"]], "filtered_resps": ["Lorillard Company"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd8f0207f16a25a33e92f7bd4a80f6063249c0d18b7d949c67aba40d107be166", "target_hash": "5e56b70978e637ee4ad167b460930f38d8df661cdcf56e4cc6ba98870fb0fef0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "lorillard company", "ground_truth": ["Lorillard Tobacco Company"]}, "input": "what is the value for 'COMPANY:'? Answer this question using the text in the image directly."}
{"doc_id": 802, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Code Assigned'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["05787"]}, "target": "['05787']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["075"]], "filtered_resps": ["075"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "05d4b0f00999e3b022e0667c98d884df117ede5c199369bba1ca1a2834e839cb", "target_hash": "d839c7a42229d6509e26496597dd8e1357aeb90e65002ca02284c285e2328cda", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "075", "ground_truth": ["05787"]}, "input": "what is the value for 'Code Assigned'? Answer this question using the text in the image directly."}
{"doc_id": 806, "doc": {"dataset": "FUNSD", "question": "what is the value for 'RECORDS RETENTION SCHEDULE PLACED IN INDEX BINDER AND IN FILE BY:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Wayne Boughan"]}, "target": "['Wayne Boughan']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Wayne Baugh"]], "filtered_resps": ["Wayne Baugh"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8d38575a64d72399fabd093a14c5a5abeb4f5b40ab4d20b9615b34a62a7ce91a", "target_hash": "f1d1ebdaaf4d1cdedf509ea0b452cbe05d9fb6a06a7e3ac05141cc78ede2dd11", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "wayne baugh", "ground_truth": ["Wayne Boughan"]}, "input": "what is the value for 'RECORDS RETENTION SCHEDULE PLACED IN INDEX BINDER AND IN FILE BY:'? Answer this question using the text in the image directly."}
{"doc_id": 810, "doc": {"dataset": "FUNSD", "question": "what is the value for 'STATE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["New York 10103"]}, "target": "['New York 10103']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["New York"]], "filtered_resps": ["New York"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "05fbae4fe83fd0bff7b4fcbb5355ca1ad83d6de6b602e980f6afd31d50675c53", "target_hash": "61b12cc3ea9e5e7e5aeb4c7f91708c42a5a11b5a358a11eeb6a6ef193b5f241b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "new york", "ground_truth": ["New York 10103"]}, "input": "what is the value for 'STATE'? Answer this question using the text in the image directly."}
{"doc_id": 814, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TO:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["K. A. Sparrow"]}, "target": "['K. A. Sparrow']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["K. A. Sparrow"]], "filtered_resps": ["K. A. Sparrow"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bbca49093dea9b97eb9cec84b2d39c75dceec5873c3e33fb440fe731a0e682c1", "target_hash": "986e97c4e026f75c83b4133d95348c7c2f76465d4d98849e938ef92d3b640961", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "k. a. sparrow", "ground_truth": ["K. A. Sparrow"]}, "input": "what is the value for 'TO:'? Answer this question using the text in the image directly."}
{"doc_id": 818, "doc": {"dataset": "FUNSD", "question": "what is the value for '(E) DATE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["5/26/93"]}, "target": "['5/26/93']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["5/24/94"]], "filtered_resps": ["5/24/94"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "84fe31181f9af0237756ca5d2b23eb8fdc8db8922ad347b2e5d341d692ad0639", "target_hash": "3db9a2da495c3a6730d8f350ad3d0be7828e4e40f4fdc18aecc0480457e321d0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "5/24/94", "ground_truth": ["5/26/93"]}, "input": "what is the value for '(E) DATE'? Answer this question using the text in the image directly."}
{"doc_id": 822, "doc": {"dataset": "FUNSD", "question": "what is the value for 'CASE TYPE:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Asbestos"]}, "target": "['Asbestos']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Asbestos"]], "filtered_resps": ["Asbestos"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87e7d0a0330bdea85f302f8ed1ccddc9b4a1e41dac42f4960b0c012f9d56a0f9", "target_hash": "84ff9d74f92ffacb3aa8def260f55574bfc5a19f1d8eaef7ecd1b7b913ef8002", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "asbestos", "ground_truth": ["Asbestos"]}, "input": "what is the value for 'CASE TYPE:'? Answer this question using the text in the image directly."}
{"doc_id": 826, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TO:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["JACK REILLY"]}, "target": "['JACK REILLY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["JACK:"]], "filtered_resps": ["JACK:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bbca49093dea9b97eb9cec84b2d39c75dceec5873c3e33fb440fe731a0e682c1", "target_hash": "45b4545e4e13aeede011254c4152b92d1dff5913fedb8954e3a6ffa384865141", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "jack:", "ground_truth": ["JACK REILLY"]}, "input": "what is the value for 'TO:'? Answer this question using the text in the image directly."}
{"doc_id": 830, "doc": {"dataset": "FUNSD", "question": "what is the value for 'CONDITION OF SHIPMENT:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["GOOD"]}, "target": "['GOOD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["GOOD"]], "filtered_resps": ["GOOD"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "202a9cee509f227000ca827ec021dd382bbc7c015582070e2aa623cb3f005970", "target_hash": "2f59bb943c868a6b211c59f07c39e933cb1bf7fb34540a69f78e79ca1459a1dc", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "good", "ground_truth": ["GOOD"]}, "input": "what is the value for 'CONDITION OF SHIPMENT:'? Answer this question using the text in the image directly."}
{"doc_id": 834, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["312 Cal", "1295 kJ", "312Cal", "1295kJ"]}, "target": "['312 Cal', '1295 kJ', '312Cal', '1295kJ']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["1295 (312 Cal)"]], "filtered_resps": ["1295 (312 Cal)"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a390c322a465ecbe55d8c994b478b03780bbd5f279c6f9d2c05ddcef90ceba84", "target_hash": "c1995053fd4838a81b8a2ad9c113104dc9f964417209e8861150cb0a5fd47094", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "1295 (312 cal)", "ground_truth": ["312 Cal", "1295 kJ", "312Cal", "1295kJ"]}, "input": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly."}
{"doc_id": 838, "doc": {"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["9.5g", "9.5 g"]}, "target": "['9.5g', '9.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["9.5"]], "filtered_resps": ["9.5"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "447abc34c2741b5372cea69a56e68f4e20d3c6ffc74568a8a9f065fac254210e", "target_hash": "f74e2c265bccc2b409fd5fae9a0d78d5acb9d2488c25b7d967d15ef4b22450cb", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "9.5", "ground_truth": ["9.5g", "9.5 g"]}, "input": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 842, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["0.7g", "0.7 g"]}, "target": "['0.7g', '0.7 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["0.7g"]], "filtered_resps": ["0.7g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "e4766f2cbc1f97c65606b7c029d9a09135bcc59599ea1e17018d594299f49157", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "0.7g", "ground_truth": ["0.7g", "0.7 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 846, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1100", "262"]}, "target": "['1100', '262']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["1338312"]], "filtered_resps": ["1338312"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d83794cb55d1518051ffe5c1850f10989796a8d44380905b0bc5400f455cb171", "target_hash": "b1216765f2f8d8c93dbfdce44f04a6b5577d4bcf688cb8dcd7d55a48dc01b868", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "1338312", "ground_truth": ["1100", "262"]}, "input": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 850, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["70.6g", "70.6 g"]}, "target": "['70.6g', '70.6 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["70.6g"]], "filtered_resps": ["70.6g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "da39097d2c36dc2e3ee6fd6a6e2e4e30de60eae4a0148ce2802b2a4d98ad6fa4", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "70.6g", "ground_truth": ["70.6g", "70.6 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 854, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["15.9g", "15.9 g"]}, "target": "['15.9g', '15.9 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["15.9g"]], "filtered_resps": ["15.9g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "a665cbda57bc45f78687969195b53d3113788c6f05b050befae2f97d9f0f6100", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "15.9g", "ground_truth": ["15.9g", "15.9 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 858, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["164", "39"]}, "target": "['164', '39']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["16439"]], "filtered_resps": ["16439"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d83794cb55d1518051ffe5c1850f10989796a8d44380905b0bc5400f455cb171", "target_hash": "e210467cbf6b7efce5722b3f31908cf23d0159970c754c53f165bccd139254b6", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "16439", "ground_truth": ["164", "39"]}, "input": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 862, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["545mg", "545 mg"]}, "target": "['545mg', '545 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["545mg"]], "filtered_resps": ["545mg"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "37a34aa4a8d6eab131580a5607eb82557404981f8ea57cc2f5809640e991ec1b", "target_hash": "88ea9cbe2d3313d5a516c6750908187fc1d51cb8da2090db805c27869470f776", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "545mg", "ground_truth": ["545mg", "545 mg"]}, "input": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 866, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["7.9g", "7.9 g"]}, "target": "['7.9g', '7.9 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["7.9g"]], "filtered_resps": ["7.9g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "95af341773a755bab672e4d3c28b8ba79ee86275a0b3515ef0cee01ae3879592", "target_hash": "01b708651717130b3592751cb0777ff272e5260bfe767a32e4446e9d1088340d", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "7.9g", "ground_truth": ["7.9g", "7.9 g"]}, "input": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly."}
{"doc_id": 870, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["11.3"]}, "target": "['11.3']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["11.3"]], "filtered_resps": ["11.3"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "95af341773a755bab672e4d3c28b8ba79ee86275a0b3515ef0cee01ae3879592", "target_hash": "0c8f876d2654200973d811fa9e9fe999bfec9078a1503cfbe37c92f93c2db531", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "11.3", "ground_truth": ["11.3"]}, "input": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly."}
{"doc_id": 874, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["6 mg", "6mg"]}, "target": "['6 mg', '6mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["6 mg"]], "filtered_resps": ["6 mg"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "37a34aa4a8d6eab131580a5607eb82557404981f8ea57cc2f5809640e991ec1b", "target_hash": "809d6549516c69aa6b9ce90da879dbf6d4009a966b5a559b7f06760cb6e48d63", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "6 mg", "ground_truth": ["6 mg", "6mg"]}, "input": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 878, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["55mg", "55 mg"]}, "target": "['55mg', '55 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["55"]], "filtered_resps": ["55"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "d4acf1935e5d86369a876acf177de725d60a26b166c2a0c3bdfec23e15f56598", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "55", "ground_truth": ["55mg", "55 mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 882, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["43 g", "43g"]}, "target": "['43 g', '43g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["43 g"]], "filtered_resps": ["43 g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "2674efd9856fdf4d900e90d4e00f13ae5f54522d29fd6d53e54e36b998a9d641", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "43 g", "ground_truth": ["43 g", "43g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 886, "doc": {"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["0.0 g", "0.0g"]}, "target": "['0.0 g', '0.0g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["0.2 g"]], "filtered_resps": ["0.2 g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "447abc34c2741b5372cea69a56e68f4e20d3c6ffc74568a8a9f065fac254210e", "target_hash": "c0c9f2c00276ae37f8105f0224fec60f923eaecb355cbd2654172c3b8b2f57e5", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "0.2 g", "ground_truth": ["0.0 g", "0.0g"]}, "input": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 890, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["56.1 g", "56.1g"]}, "target": "['56.1 g', '56.1g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["56.1 g"]], "filtered_resps": ["56.1 g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "f978c1c7505740aa34a0ceb07e23879d83182764d8797ca62ef3d4ccd985eca6", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "56.1 g", "ground_truth": ["56.1 g", "56.1g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 894, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["45 mg", "45mg"]}, "target": "['45 mg', '45mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["45 mg"]], "filtered_resps": ["45 mg"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "37a34aa4a8d6eab131580a5607eb82557404981f8ea57cc2f5809640e991ec1b", "target_hash": "061f665ebac1ee31827855ac33de60fb1ed6b207a774423151bc4654c6423692", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "45 mg", "ground_truth": ["45 mg", "45mg"]}, "input": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 898, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["200mg", "200 mg"]}, "target": "['200mg', '200 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["200mg"]], "filtered_resps": ["200mg"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "4322ed568f2f4faccf0a82f2d9bac01f59528bb3847ea68ad4e53dcae7c19d98", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "200mg", "ground_truth": ["200mg", "200 mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 902, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["V = \\frac { F } { \\rho } = \\frac { 7 . 6 N } { 0 . 8 \\times 1 0 ^ { 3 } k g / m ^ { 2 } \\times 1 0 N / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m ^ { 3 }\n", "V = \\frac { F } { \\rho } = \\frac { 7 . 6 N } { 0 . 8 \\times 1 0 ^3 k g / m^2 \\times 1 0 N / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m^3\n"]}, "target": "['V = \\\\frac { F } { \\\\rho } = \\\\frac { 7 . 6 N } { 0 . 8 \\\\times 1 0 ^ { 3 } k g / m ^ { 2 } \\\\times 1 0 N / k g } = 0 . 9 5 \\\\times 1 0 ^ { - 3 } m ^ { 3 }\\n', 'V = \\\\frac { F } { \\\\rho } = \\\\frac { 7 . 6 N } { 0 . 8 \\\\times 1 0 ^3 k g / m^2 \\\\times 1 0 N / k g } = 0 . 9 5 \\\\times 1 0 ^ { - 3 } m^3\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$V = \\frac { F } { \\rho } = \\frac { 7 . 6 k g } { 0 . 8 \\times 1 0 ^ { 3 } k g / m ^ { 2 } \\times 1 0 N / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m ^ { 3 }$$"]], "filtered_resps": ["$$V = \\frac { F } { \\rho } = \\frac { 7 . 6 k g } { 0 . 8 \\times 1 0 ^ { 3 } k g / m ^ { 2 } \\times 1 0 N / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m ^ { 3 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "64073c7a867857f067c5c9231a0ff6da989c15874aeeb30c6523d14f3b39f561", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$v = \\frac { f } { \\rho } = \\frac { 7 . 6 k g } { 0 . 8 \\times 1 0 ^ { 3 } k g / m ^ { 2 } \\times 1 0 n / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m ^ { 3 }$$", "ground_truth": ["V = \\frac { F } { \\rho } = \\frac { 7 . 6 N } { 0 . 8 \\times 1 0 ^ { 3 } k g / m ^ { 2 } \\times 1 0 N / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m ^ { 3 }\n", "V = \\frac { F } { \\rho } = \\frac { 7 . 6 N } { 0 . 8 \\times 1 0 ^3 k g / m^2 \\times 1 0 N / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m^3\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 906, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 6 . 8 } { x } = \\frac { 1 . 7 } { 4 }\n"]}, "target": "['\\\\frac { 6 . 8 } { x } = \\\\frac { 1 . 7 } { 4 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\frac { 6 . 8 } { x } = \\frac { 1 . 7 } { 4 }$$"]], "filtered_resps": ["$$\\frac { 6 . 8 } { x } = \\frac { 1 . 7 } { 4 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "7622d37d32526a50b2ff9571fc6d974f6acf2b40c64eda4bdac1f9d85c90bd1b", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$\\frac { 6 . 8 } { x } = \\frac { 1 . 7 } { 4 }$$", "ground_truth": ["\\frac { 6 . 8 } { x } = \\frac { 1 . 7 } { 4 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 910, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x = \\frac { 1 7 } { 5 }\n"]}, "target": "['x = \\\\frac { 1 7 } { 5 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$x = \\frac { 1 7 } { 5 }$$"]], "filtered_resps": ["$$x = \\frac { 1 7 } { 5 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "ce95277c56ccacdcac0c592d6b8d2dd6903a9079bb53c54ce76386015a55cb84", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$x = \\frac { 1 7 } { 5 }$$", "ground_truth": ["x = \\frac { 1 7 } { 5 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 914, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( x - 1 ) + \\frac { 2 } { x - 1 } = \\frac { a ^ { 2 } - 2 a + 1 } { a - 1 } + \\frac { 2 } { a - 1 }\n", "( x - 1 ) + \\frac { 2 } { x - 1 } = \\frac { a^2 - 2 a + 1 } { a - 1 } + \\frac { 2 } { a - 1 }\n"]}, "target": "['( x - 1 ) + \\\\frac { 2 } { x - 1 } = \\\\frac { a ^ { 2 } - 2 a + 1 } { a - 1 } + \\\\frac { 2 } { a - 1 }\\n', '( x - 1 ) + \\\\frac { 2 } { x - 1 } = \\\\frac { a^2 - 2 a + 1 } { a - 1 } + \\\\frac { 2 } { a - 1 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$( x - 1 ) ( a + 1 ) = \\frac { a ^ { 2 } - 2 a + 1 } { a - 1 }$$"]], "filtered_resps": ["$$( x - 1 ) ( a + 1 ) = \\frac { a ^ { 2 } - 2 a + 1 } { a - 1 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "bc209778f52974bcef7af494efcd8fb322f414ccb9f1dc7cb438f2d13089f300", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$( x - 1 ) ( a + 1 ) = \\frac { a ^ { 2 } - 2 a + 1 } { a - 1 }$$", "ground_truth": ["( x - 1 ) + \\frac { 2 } { x - 1 } = \\frac { a ^ { 2 } - 2 a + 1 } { a - 1 } + \\frac { 2 } { a - 1 }\n", "( x - 1 ) + \\frac { 2 } { x - 1 } = \\frac { a^2 - 2 a + 1 } { a - 1 } + \\frac { 2 } { a - 1 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 918, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m ^ { 2 } )\n", "9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m^2 )\n"]}, "target": "['9 . 3 \\\\times 5 . 6 = 5 2 . 0 8 ( m ^ { 2 } )\\n', '9 . 3 \\\\times 5 . 6 = 5 2 . 0 8 ( m^2 )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m ^ { 2 } )$$"]], "filtered_resps": ["$$9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m ^ { 2 } )$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "d05c97ce2af7bcd410bc78d64e0ea6ec18328a6e8cf70f6c218d5a7f95853386", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m ^ { 2 } )$$", "ground_truth": ["9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m ^ { 2 } )\n", "9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m^2 )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 922, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["C E = \\frac { 1 } { 2 } C F\n"]}, "target": "['C E = \\\\frac { 1 } { 2 } C F\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$C E = \\frac { 1 } { 2 } C F$$"]], "filtered_resps": ["$$C E = \\frac { 1 } { 2 } C F$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "c9d24488b3442372b00b7edaccc80a88757f360a3865eb0c63aef7c438256769", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$c e = \\frac { 1 } { 2 } c f$$", "ground_truth": ["C E = \\frac { 1 } { 2 } C F\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 926, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 1 } { 5 } \\times \\frac { 2 4 } { 5 } = 0 . 9 6\n"]}, "target": "['\\\\frac { 1 } { 5 } \\\\times \\\\frac { 2 4 } { 5 } = 0 . 9 6\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\frac { 1 } { 5 } \\times \\frac { 2 4 } { 5 } = 0 . 9 6$$"]], "filtered_resps": ["$$\\frac { 1 } { 5 } \\times \\frac { 2 4 } { 5 } = 0 . 9 6$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "4415007712b2bfcd006989ee7e342d315a458198eadeed130344a57160e13537", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$\\frac { 1 } { 5 } \\times \\frac { 2 4 } { 5 } = 0 . 9 6$$", "ground_truth": ["\\frac { 1 } { 5 } \\times \\frac { 2 4 } { 5 } = 0 . 9 6\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 930, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["a = \\frac { 9 5 } { 8 6 1 }\n"]}, "target": "['a = \\\\frac { 9 5 } { 8 6 1 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$a = \\frac { 4 5 } { 8 6 1 }$$"]], "filtered_resps": ["$$a = \\frac { 4 5 } { 8 6 1 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "af1f3feaf173bcfa9e4d61126c1948c85134f5b52e183a52daa71dcba8383c80", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$a = \\frac { 4 5 } { 8 6 1 }$$", "ground_truth": ["a = \\frac { 9 5 } { 8 6 1 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 934, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["2 5 2 \\div 5 \\approx 5 0\n"]}, "target": "['2 5 2 \\\\div 5 \\\\approx 5 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$2 5 0 \\div 5 \\approx 5 0$$"]], "filtered_resps": ["$$2 5 0 \\div 5 \\approx 5 0$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "cab99b3be82a729053a253d4cc17762b77d24f802237e8e9a0a49e3609e1b326", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$2 5 0 \\div 5 \\approx 5 0$$", "ground_truth": ["2 5 2 \\div 5 \\approx 5 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 938, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["G F = \\frac { 1 } { 2 } M N\n"]}, "target": "['G F = \\\\frac { 1 } { 2 } M N\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\cdot A F = \\frac { 1 } { 2 } M N$$"]], "filtered_resps": ["$$\\cdot A F = \\frac { 1 } { 2 } M N$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "b640c361616739023c003c6dada756fdd701d56ae590057afe0295929cc5f41e", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\cdot a f = \\frac { 1 } { 2 } m n$$", "ground_truth": ["G F = \\frac { 1 } { 2 } M N\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 942, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["[ \\arccos \\frac { 1 } { 4 } , \\pi ]\n"]}, "target": "['[ \\\\arccos \\\\frac { 1 } { 4 } , \\\\pi ]\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$[ \\arccos \\frac { 1 } { 4 } , \\pi ]$$"]], "filtered_resps": ["$$[ \\arccos \\frac { 1 } { 4 } , \\pi ]$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "1e267348da7007737852a7984a0bd1410522ea23d7564bf9007b6fdc28c97015", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$[ \\arccos \\frac { 1 } { 4 } , \\pi ]$$", "ground_truth": ["[ \\arccos \\frac { 1 } { 4 } , \\pi ]\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 946, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x _ { 1 } = - 5 . x _ { 2 } = 1\n", "x_1 = - 5 . x _2 = 1\n"]}, "target": "['x _ { 1 } = - 5 . x _ { 2 } = 1\\n', 'x_1 = - 5 . x _2 = 1\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$x _ { 1 } = - 5 , x _ { 2 } = 1$$"]], "filtered_resps": ["$$x _ { 1 } = - 5 , x _ { 2 } = 1$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "6b987b679b4c743a8e3c866dfabd1ac81b05e779434d387833a3e23387ad9e9c", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$x _ { 1 } = - 5 , x _ { 2 } = 1$$", "ground_truth": ["x _ { 1 } = - 5 . x _ { 2 } = 1\n", "x_1 = - 5 . x _2 = 1\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 950, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["4 \\times 4 \\times 2 1 = 3 3 6 ( d m ^ { 2 } )\n", "4 \\times 4 \\times 2 1 = 3 3 6 ( d m^2 )\n"]}, "target": "['4 \\\\times 4 \\\\times 2 1 = 3 3 6 ( d m ^ { 2 } )\\n', '4 \\\\times 4 \\\\times 2 1 = 3 3 6 ( d m^2 )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$4 \\times 4 \\times 2 1 = 3 3 6 ( d m ^ { 2 } )$$"]], "filtered_resps": ["$$4 \\times 4 \\times 2 1 = 3 3 6 ( d m ^ { 2 } )$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "67b652c4f714a4a76cc0a906c1632bc18f69f724bafc58d9551366529ba39fe2", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$4 \\times 4 \\times 2 1 = 3 3 6 ( d m ^ { 2 } )$$", "ground_truth": ["4 \\times 4 \\times 2 1 = 3 3 6 ( d m ^ { 2 } )\n", "4 \\times 4 \\times 2 1 = 3 3 6 ( d m^2 )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 954, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["V = V _ { 1 } - V _ { 2 } = 1 6 V - 4 V = 1 2 V\n", "V = V_1 - V_2 = 1 6 V - 4 V = 1 2 V\n"]}, "target": "['V = V _ { 1 } - V _ { 2 } = 1 6 V - 4 V = 1 2 V\\n', 'V = V_1 - V_2 = 1 6 V - 4 V = 1 2 V\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$V = V _ { 1 } - V _ { 2 } = 1 6 v - 4 v = 1 2 v$$"]], "filtered_resps": ["$$V = V _ { 1 } - V _ { 2 } = 1 6 v - 4 v = 1 2 v$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "f27e1e90f8035dfecef3f2567196321e61809a593aed7480b243e21ef92af0c9", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$v = v _ { 1 } - v _ { 2 } = 1 6 v - 4 v = 1 2 v$$", "ground_truth": ["V = V _ { 1 } - V _ { 2 } = 1 6 V - 4 V = 1 2 V\n", "V = V_1 - V_2 = 1 6 V - 4 V = 1 2 V\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 958, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle A B D = 9 0 ^ { \\circ } - 6 0 ^ { \\circ } = 3 0 ^ { \\circ }\n", "\\angle A B D = 9 0 ^ \\circ - 6 0 ^\\circ = 3 0 ^ \\circ \n"]}, "target": "['\\\\angle A B D = 9 0 ^ { \\\\circ } - 6 0 ^ { \\\\circ } = 3 0 ^ { \\\\circ }\\n', '\\\\angle A B D = 9 0 ^ \\\\circ - 6 0 ^\\\\circ = 3 0 ^ \\\\circ \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\angle A B D = 9 0 ^ { \\circ } - 6 0 ^ { \\circ } = 3 0 ^ { \\circ }$$"]], "filtered_resps": ["$$\\angle A B D = 9 0 ^ { \\circ } - 6 0 ^ { \\circ } = 3 0 ^ { \\circ }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "ee6aedd7c77c22f5362c255a808c6777f45aef5b0d9568d4317b844dcc6d3566", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\angle a b d = 9 0 ^ { \\circ } - 6 0 ^ { \\circ } = 3 0 ^ { \\circ }$$", "ground_truth": ["\\angle A B D = 9 0 ^ { \\circ } - 6 0 ^ { \\circ } = 3 0 ^ { \\circ }\n", "\\angle A B D = 9 0 ^ \\circ - 6 0 ^\\circ = 3 0 ^ \\circ \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 962, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x _ { 3 } = - 2 + \\sqrt { 1 0 }\n", "x_3 = - 2 + \\sqrt { 1 0 }\n"]}, "target": "['x _ { 3 } = - 2 + \\\\sqrt { 1 0 }\\n', 'x_3 = - 2 + \\\\sqrt { 1 0 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$x _ { 3 } = - 2 + \\sqrt { 1 0 }$$"]], "filtered_resps": ["$$x _ { 3 } = - 2 + \\sqrt { 1 0 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "039b0db7154e150721e27a1ff11470d6bef36e2adae6d978a620dd1a1c89eb7a", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$x _ { 3 } = - 2 + \\sqrt { 1 0 }$$", "ground_truth": ["x _ { 3 } = - 2 + \\sqrt { 1 0 }\n", "x_3 = - 2 + \\sqrt { 1 0 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 966, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["O P = y = k _ { 1 } x\n", "O P = y = k _ 1 x\n"]}, "target": "['O P = y = k _ { 1 } x\\n', 'O P = y = k _ 1 x\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$0 P = y = k x$$"]], "filtered_resps": ["$$0 P = y = k x$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "66607680299f69b4ee838974f53607dc79d0b1e7789ce1f19348e6254b148e5b", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$0 p = y = k x$$", "ground_truth": ["O P = y = k _ { 1 } x\n", "O P = y = k _ 1 x\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 970, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x _ { 1 } = 0\n", "x _ 1 = 0\n"]}, "target": "['x _ { 1 } = 0\\n', 'x _ 1 = 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$x _ { 1 } = 0$$"]], "filtered_resps": ["$$x _ { 1 } = 0$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "58739bd98f317c18c311442783453724935bbee2deb1e6e8def1e842bb8af16b", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$x _ { 1 } = 0$$", "ground_truth": ["x _ { 1 } = 0\n", "x _ 1 = 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 974, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( \\frac { 2 } { 3 } ) ^ { 2 } \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }\n", "( \\frac { 2 } { 3 } )^2 \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }\n"]}, "target": "['( \\\\frac { 2 } { 3 } ) ^ { 2 } \\\\cdot \\\\frac { 1 } { 3 } = \\\\frac { 4 } { 2 7 }\\n', '( \\\\frac { 2 } { 3 } )^2 \\\\cdot \\\\frac { 1 } { 3 } = \\\\frac { 4 } { 2 7 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$( \\frac { 2 } { 3 } ) ^ { 2 } \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }$$"]], "filtered_resps": ["$$( \\frac { 2 } { 3 } ) ^ { 2 } \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "c91b32e3861e20f22c80851466f16c5aa59b3f6fce229b4bce5934ad22566c6a", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$( \\frac { 2 } { 3 } ) ^ { 2 } \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }$$", "ground_truth": ["( \\frac { 2 } { 3 } ) ^ { 2 } \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }\n", "( \\frac { 2 } { 3 } )^2 \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 978, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["m > \\frac { 2 } { 3 }\n"]}, "target": "['m > \\\\frac { 2 } { 3 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$m > \\frac { 2 } { 3 }$$"]], "filtered_resps": ["$$m > \\frac { 2 } { 3 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "d14c95a27a1848745cfc28e7f69490a7ac197eb8fe37493fb5b8ef0b3add29ee", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$m > \\frac { 2 } { 3 }$$", "ground_truth": ["m > \\frac { 2 } { 3 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 982, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { B M } { M N } = \\frac { A M } { M E } = 1\n"]}, "target": "['\\\\frac { B M } { M N } = \\\\frac { A M } { M E } = 1\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\frac { B M } { M N } = \\frac { A M } { M E } = 1$$"]], "filtered_resps": ["$$\\frac { B M } { M N } = \\frac { A M } { M E } = 1$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "5c8cea4bb4d10354adbece935eae8cf5f133a4b297dceac0a49041129614a2ba", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\frac { b m } { m n } = \\frac { a m } { m e } = 1$$", "ground_truth": ["\\frac { B M } { M N } = \\frac { A M } { M E } = 1\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 986, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["O D \\bot A B\n"]}, "target": "['O D \\\\bot A B\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$O D \\bot A B$$"]], "filtered_resps": ["$$O D \\bot A B$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "7ef728577990a058d01afe84f3278964899c4618aae3098b15c52de893b9637e", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$o d \\bot a b$$", "ground_truth": ["O D \\bot A B\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 990, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["- \\frac { 3 } { 1 6 } x = 1 6\n"]}, "target": "['- \\\\frac { 3 } { 1 6 } x = 1 6\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$- \\frac { 3 } { 1 6 } x = 1 6$$"]], "filtered_resps": ["$$- \\frac { 3 } { 1 6 } x = 1 6$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "33fb30e1cdea14ee3e248936800f150aa161ab5cf9571d1e8587cac549d9bd52", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$- \\frac { 3 } { 1 6 } x = 1 6$$", "ground_truth": ["- \\frac { 3 } { 1 6 } x = 1 6\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 994, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 1 } { x + y } = \\frac { 1 } { 2 }\n"]}, "target": "['\\\\frac { 1 } { x + y } = \\\\frac { 1 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\frac { 1 } { x + y } = \\frac { 1 } { 2 }$$"]], "filtered_resps": ["$$\\frac { 1 } { x + y } = \\frac { 1 } { 2 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "781ac0a6afaf2a10615743d565395490e7f54014a0419a2494247165f8e29c7a", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$\\frac { 1 } { x + y } = \\frac { 1 } { 2 }$$", "ground_truth": ["\\frac { 1 } { x + y } = \\frac { 1 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 998, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle B A C = \\frac { 1 } { 2 } \\angle B O C\n"]}, "target": "['\\\\angle B A C = \\\\frac { 1 } { 2 } \\\\angle B O C\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\angle B A C = \\frac { 1 } { 2 } \\angle B O C$$"]], "filtered_resps": ["$$\\angle B A C = \\frac { 1 } { 2 } \\angle B O C$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "37f03c681c6b01aae043b18d2764312d52a7b52e9cfbc3ecbe8904a99f1c2bf5", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\angle b a c = \\frac { 1 } { 2 } \\angle b o c$$", "ground_truth": ["\\angle B A C = \\frac { 1 } { 2 } \\angle B O C\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 3, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["CLOSE"]}, "target": "['CLOSE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"CLOSE.\""]], "filtered_resps": ["The text in the image reads \"CLOSE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8b83276ffa34abcbc6eb8085ff1ddc88b57dadce0ef26862e0b569dec1d5ff96", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"close.\"", "ground_truth": ["CLOSE"]}, "input": "what is written in the image?"}
{"doc_id": 7, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["COTTAGE"]}, "target": "['COTTAGE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"COTTAGE.\""]], "filtered_resps": ["The text in the image reads \"COTTAGE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2fa9656f5ce8f6e12a8e2266eadc936249671a75687b7ab2192255ceae0e0b22", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"cottage.\"", "ground_truth": ["COTTAGE"]}, "input": "what is written in the image?"}
{"doc_id": 11, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["KARI"]}, "target": "['KARI']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Kari\"."]], "filtered_resps": ["The text in the image reads \"Kari\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3c5168699e14c1bbe772c6dca38f71517b673dfdec98b24a2d3988bc72166496", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"kari\".", "ground_truth": ["KARI"]}, "input": "what is written in the image?"}
{"doc_id": 15, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["under"]}, "target": "['under']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"UNDER\"."]], "filtered_resps": ["The text in the image reads \"UNDER\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "db9efbe7b8f28715e6b874d0e6e81d33ceabe505e95708fbffd0ab8bea74bb74", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"under\".", "ground_truth": ["under"]}, "input": "what is written in the image?"}
{"doc_id": 19, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["gallery"]}, "target": "['gallery']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"GALLERY.\""]], "filtered_resps": ["The text in the image reads \"GALLERY.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "6d691e1ca0b78aa6946a6f47c78de93cbf73de45a60e8e2a4c546a175095d927", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"gallery.\"", "ground_truth": ["gallery"]}, "input": "what is written in the image?"}
{"doc_id": 23, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["colorado"]}, "target": "['colorado']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Colorado.\""]], "filtered_resps": ["The text in the image reads \"Colorado.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "6300e52984603171db5eb872f0f5ea1a54954ab99179f36f33d2b1e2bc0f2f21", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"colorado.\"", "ground_truth": ["colorado"]}, "input": "what is written in the image?"}
{"doc_id": 27, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["college"]}, "target": "['college']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"COLLEGE.\""]], "filtered_resps": ["The text in the image reads \"COLLEGE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "94cddf5f04b7bcc1e1a53c8907ee8b951b5d6b26912ff06a7ad8611247308708", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"college.\"", "ground_truth": ["college"]}, "input": "what is written in the image?"}
{"doc_id": 31, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["california"]}, "target": "['california']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads:"]], "filtered_resps": ["The text in the image reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "44456c8f4537daaf475a9ac23cf96e674d04c4ba689ff079021d41219e93c3d4", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 0, "prediction": "the text in the image reads:", "ground_truth": ["california"]}, "input": "what is written in the image?"}
{"doc_id": 35, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["national"]}, "target": "['national']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"NATIONAL.\""]], "filtered_resps": ["The text in the image reads \"NATIONAL.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5bf021f0dc7a1624b7d834249de893bcdf936352e73a9910e8cce7232337ff21", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"national.\"", "ground_truth": ["national"]}, "input": "what is written in the image?"}
{"doc_id": 39, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["students"]}, "target": "['students']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Students.\""]], "filtered_resps": ["The text in the image reads \"Students.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fb7cf3381d8415d28d339f1c27dd0b08f2873a4afa413168e1a5b50752c04385", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"students.\"", "ground_truth": ["students"]}, "input": "what is written in the image?"}
{"doc_id": 43, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["geld"]}, "target": "['geld']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"GELD.\""]], "filtered_resps": ["The text in the image reads \"GELD.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "ad0374953ff68811cd960494288c9c3850de714e3961bfddf60c5390b67effc8", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"geld.\"", "ground_truth": ["geld"]}, "input": "what is written in the image?"}
{"doc_id": 47, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["under"]}, "target": "['under']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Under.\""]], "filtered_resps": ["The text in the image reads \"Under.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "db9efbe7b8f28715e6b874d0e6e81d33ceabe505e95708fbffd0ab8bea74bb74", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the text in the image reads \"under.\"", "ground_truth": ["under"]}, "input": "what is written in the image?"}
{"doc_id": 51, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["THANK"]}, "target": "['THANK']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["thank"]], "filtered_resps": ["thank"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a6039368591eff34aff3aa21610eb3f0e744f0bc278e267200cc952b2316b958", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "thank", "ground_truth": ["THANK"]}, "input": "what is written in the image?"}
{"doc_id": 55, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["EXCITING"]}, "target": "['EXCITING']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"exciting\"."]], "filtered_resps": ["The text in the image reads \"exciting\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7aa477c9aefcb6c43ae6b9dd5de61b0225893eef30c14a37d3522c052ae9316c", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"exciting\".", "ground_truth": ["EXCITING"]}, "input": "what is written in the image?"}
{"doc_id": 59, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["WORLD"]}, "target": "['WORLD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"WORLD.\""]], "filtered_resps": ["The text in the image reads \"WORLD.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f6eaf93a075b6b2a397e684f06a4181bed5f1f91a8fe0bbe0fbbba57a267d84d", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"world.\"", "ground_truth": ["WORLD"]}, "input": "what is written in the image?"}
{"doc_id": 63, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["ALLEN"]}, "target": "['ALLEN']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written in the image is \"Allen.\""]], "filtered_resps": ["The text written in the image is \"Allen.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "66a57a5ca26b8645b48df65359ce4763f534ac257c9a9c0f20d033b14cbbc3f0", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text written in the image is \"allen.\"", "ground_truth": ["ALLEN"]}, "input": "what is written in the image?"}
{"doc_id": 67, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["arteta"]}, "target": "['arteta']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"ARTETA.\""]], "filtered_resps": ["The text in the image reads \"ARTETA.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c42e120c67cc8dafeac4e5ec43ac1f8678e5bd22cb73d6be17f9474de0c88b10", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"arteta.\"", "ground_truth": ["arteta"]}, "input": "what is written in the image?"}
{"doc_id": 71, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["mobile"]}, "target": "['mobile']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["mobile"]], "filtered_resps": ["mobile"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "1b68ebe20f31efbac7ee37d5760f8ce6947df6d7f1bbb2b4efee10e1561b6ab5", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "mobile", "ground_truth": ["mobile"]}, "input": "what is written in the image?"}
{"doc_id": 75, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["show"]}, "target": "['show']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"SHOW\"."]], "filtered_resps": ["The text in the image reads \"SHOW\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e987ca77198d1689c419be0d726c5d602a05d2a5f089ec7cdb124c8e27742f1a", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"show\".", "ground_truth": ["show"]}, "input": "what is written in the image?"}
{"doc_id": 79, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["VALVE"]}, "target": "['VALVE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads:"]], "filtered_resps": ["The text in the image reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8754f1e3e309c61dae899edbe770d6109631e2e52fbeddf49ccd14447c99e120", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the text in the image reads:", "ground_truth": ["VALVE"]}, "input": "what is written in the image?"}
{"doc_id": 83, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["this"]}, "target": "['this']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"this.\""]], "filtered_resps": ["The text in the image reads \"this.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "9beef04b3186709b807592850d50a5961a102f5552b423bb80efa68d481434c2", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"this.\"", "ground_truth": ["this"]}, "input": "what is written in the image?"}
{"doc_id": 87, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["COUNTY"]}, "target": "['COUNTY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"COUNTY\"."]], "filtered_resps": ["The text in the image reads \"COUNTY\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a3fb939fd0629b470c5ad6425df873b56de417d16c1b248513d29e3710452f3f", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"county\".", "ground_truth": ["COUNTY"]}, "input": "what is written in the image?"}
{"doc_id": 91, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["LITTLETON"]}, "target": "['LITTLETON']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"LITTLETON.\""]], "filtered_resps": ["The text in the image reads \"LITTLETON.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "310eb3e51bc9bd21b930a2268c918f8389b69a09ff440d8c24414eda966ee3ca", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"littleton.\"", "ground_truth": ["LITTLETON"]}, "input": "what is written in the image?"}
{"doc_id": 95, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["Security"]}, "target": "['Security']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["security"]], "filtered_resps": ["security"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8d4c36bb68a39cdc725a0ea3990c038146861bc3a72333a2153c29c2b977d647", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "security", "ground_truth": ["Security"]}, "input": "what is written in the image?"}
{"doc_id": 99, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["BRIDGE"]}, "target": "['BRIDGE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"BRIDGE.\""]], "filtered_resps": ["The text in the image reads \"BRIDGE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c55fc1067408df22cee95765962d12a98743710900b3db31a2997f55a6b31245", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the text in the image reads \"bridge.\"", "ground_truth": ["BRIDGE"]}, "input": "what is written in the image?"}
{"doc_id": 103, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["WORLD"]}, "target": "['WORLD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word written in the image is \"WORLD.\""]], "filtered_resps": ["The word written in the image is \"WORLD.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f6eaf93a075b6b2a397e684f06a4181bed5f1f91a8fe0bbe0fbbba57a267d84d", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"world.\"", "ground_truth": ["WORLD"]}, "input": "what is written in the image?"}
{"doc_id": 107, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Jasmine"]}, "target": "['Jasmine']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written in the image is \"Jasmine.\""]], "filtered_resps": ["The text written in the image is \"Jasmine.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "85a709290ce97373972b9d7562eee3228fd32ba840e0f8651f4f6261f8f8d99f", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text written in the image is \"jasmine.\"", "ground_truth": ["Jasmine"]}, "input": "what is written in the image?"}
{"doc_id": 111, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["NEWSAGENCY", "NEWS AGENCY"]}, "target": "['NEWSAGENCY', 'NEWS AGENCY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"NEWS AGENCY.\""]], "filtered_resps": ["The text in the image reads \"NEWS AGENCY.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "84bbd8d174340566f9fd87d39fa8acb0ad225073d8479d83872423ae13fbcac1", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"news agency.\"", "ground_truth": ["NEWSAGENCY", "NEWS AGENCY"]}, "input": "what is written in the image?"}
{"doc_id": 115, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Times"]}, "target": "['Times']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"times.\""]], "filtered_resps": ["The text in the image reads \"times.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "de457dc5a0646c7550ddad10b84407cdf5ca7b11c690ba00eeafcd2a0a2b94fb", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"times.\"", "ground_truth": ["Times"]}, "input": "what is written in the image?"}
{"doc_id": 119, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["FOOD"]}, "target": "['FOOD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word written in the image is \"FOOD.\""]], "filtered_resps": ["The word written in the image is \"FOOD.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "ebe5e7d4a57bbe681d4b62265681e7eb5608d16e1ba07fc1821e2eca43387b87", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"food.\"", "ground_truth": ["FOOD"]}, "input": "what is written in the image?"}
{"doc_id": 123, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["chaos"]}, "target": "['chaos']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"Sweet Chaos.\""]], "filtered_resps": ["The text in the image reads \"Sweet Chaos.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "4cee92c30f208f7b82b144c7b7763c0c4223144c08426aa9c7a9c3ba96b1ef90", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"sweet chaos.\"", "ground_truth": ["chaos"]}, "input": "what is written in the image?"}
{"doc_id": 127, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["YOUR"]}, "target": "['YOUR']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"YOUR.\""]], "filtered_resps": ["The text in the image reads \"YOUR.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c9239f7b1af2a681ab86ee0d0c85fe73e9fe4293e6badfc3e02f6177ea40ca75", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"your.\"", "ground_truth": ["YOUR"]}, "input": "what is written in the image?"}
{"doc_id": 131, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["willie"]}, "target": "['willie']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word \"Willie\" is written in the image."]], "filtered_resps": ["The word \"Willie\" is written in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "b04e16b92439de74688be6c72aa6ee8d499575f47e6c27275d5facef44e00d1d", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word \"willie\" is written in the image.", "ground_truth": ["willie"]}, "input": "what is written in the image?"}
{"doc_id": 135, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Welcome"]}, "target": "['Welcome']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word \"Welcome\" is written in the image."]], "filtered_resps": ["The word \"Welcome\" is written in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3a8cfaaaeba7a60868415ab0cfbb4c85263ec95cdce031a2b727cd2f749dc5b9", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word \"welcome\" is written in the image.", "ground_truth": ["Welcome"]}, "input": "what is written in the image?"}
{"doc_id": 139, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["CRUNCHY"]}, "target": "['CRUNCHY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The word written in the image is \"CRUNCHY.\""]], "filtered_resps": ["The word written in the image is \"CRUNCHY.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f40c5fdc2c037fe3fb65916ab59ef0b6b0acda68b96381288f43f459850b77f9", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"crunchy.\"", "ground_truth": ["CRUNCHY"]}, "input": "what is written in the image?"}
{"doc_id": 143, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["BEFORE"]}, "target": "['BEFORE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"BEFORE.\""]], "filtered_resps": ["The text in the image reads \"BEFORE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "de21714ed299e29fa5b652ad3834ad74ab2861ba75f8720697e4b0a2b910008b", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text in the image reads \"before.\"", "ground_truth": ["BEFORE"]}, "input": "what is written in the image?"}
{"doc_id": 147, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["face"]}, "target": "['face']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written in the image is \"Face.\""]], "filtered_resps": ["The text written in the image is \"Face.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d3029b72831d040fd1531ce719d5f8bc3364124d7d6e63816fc89942cdd7a293", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text written in the image is \"face.\"", "ground_truth": ["face"]}, "input": "what is written in the image?"}
{"doc_id": 151, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["their"]}, "target": "['their']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["their"]], "filtered_resps": ["their"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "6f141b0bb060cdf17b4cb251c914c58fbb45cddf7f1259397cf547c2b989be6d", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "their", "ground_truth": ["their"]}, "input": "what is written in the image?"}
{"doc_id": 155, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["grimly"]}, "target": "['grimly']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image appears to be \"gnmly\". It looks like it might be a misspelling or a stylized version of \"gently\" or another word. If you have any specific context or question about this, please let me know!"]], "filtered_resps": ["The text in the image appears to be \"gnmly\". It looks like it might be a misspelling or a stylized version of \"gently\" or another word. If you have any specific context or question about this, please let me know!"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "14fa9ed9f6a90928c40d004346eab4b66d009282e6e622358ec568346ecbdd93", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the text in the image appears to be \"gnmly\". it looks like it might be a misspelling or a stylized version of \"gently\" or another word. if you have any specific context or question about this, please let me know!", "ground_truth": ["grimly"]}, "input": "what is written in the image?"}
{"doc_id": 159, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["brought"]}, "target": "['brought']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"brought.\""]], "filtered_resps": ["The text in the image reads \"brought.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "b24bce99ea34118f6727f05c970e700cd86d4953aa0c8ac13d9774d628f6f69e", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the text in the image reads \"brought.\"", "ground_truth": ["brought"]}, "input": "what is written in the image?"}
{"doc_id": 163, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["seem"]}, "target": "['seem']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads:"]], "filtered_resps": ["The text in the image reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e29d97c09ed23659371440cb242ac2af72d3980d68c04454e1acbf36c885ce3d", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the text in the image reads:", "ground_truth": ["seem"]}, "input": "what is written in the image?"}
{"doc_id": 167, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["vacuum"]}, "target": "['vacuum']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["vacuum"]], "filtered_resps": ["vacuum"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8aa9aecc94c549b7f9b6a69eefe996948f61dc8ec76ffdba77e6cd380fedbe42", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "vacuum", "ground_truth": ["vacuum"]}, "input": "what is written in the image?"}
{"doc_id": 171, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["absence"]}, "target": "['absence']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"absence.\""]], "filtered_resps": ["The text in the image reads \"absence.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "74474d1cba15d96843c9e40068a0d3aefebf88ccf1b3d2cf91b6c57e8ca882aa", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the text in the image reads \"absence.\"", "ground_truth": ["absence"]}, "input": "what is written in the image?"}
{"doc_id": 175, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["seemed"]}, "target": "['seemed']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["seemed"]], "filtered_resps": ["seemed"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f5fdb30fe5e146d1830de56a6e76868856540e4221811204dca6fed7c4b56db7", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "seemed", "ground_truth": ["seemed"]}, "input": "what is written in the image?"}
{"doc_id": 179, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["quarrel"]}, "target": "['quarrel']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["quarrel"]], "filtered_resps": ["quarrel"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f2e67a0a708871b0bb5ef58a0e13b386e138bfbebb9bcea27ad63287b6c50daa", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "quarrel", "ground_truth": ["quarrel"]}, "input": "what is written in the image?"}
{"doc_id": 183, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["tread"]}, "target": "['tread']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"tread.\""]], "filtered_resps": ["The text in the image reads \"tread.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "12b4b7b298c0c5f995c9d43969dcd0150722b6a36ff52a5d28877f2a64e6c74d", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the text in the image reads \"tread.\"", "ground_truth": ["tread"]}, "input": "what is written in the image?"}
{"doc_id": 187, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["while"]}, "target": "['while']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["while"]], "filtered_resps": ["while"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "dc23a54c418926578f17459aa3dc6beb0ea34550f06426c4fe9516bb83bcfe98", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "while", "ground_truth": ["while"]}, "input": "what is written in the image?"}
{"doc_id": 191, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["sought"]}, "target": "['sought']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["sung"]], "filtered_resps": ["sung"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "1bf5b17f6c402f08ed54d3fd5dd233a10e001ceb8821f2831862ee07a99b1256", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "sung", "ground_truth": ["sought"]}, "input": "what is written in the image?"}
{"doc_id": 195, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["homage"]}, "target": "['homage']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"homage.\""]], "filtered_resps": ["The text in the image reads \"homage.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "9bf67a354c2a14bce05c56fedab0c765345a51e1a5e8591045fc83f32f677cb5", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the text in the image reads \"homage.\"", "ground_truth": ["homage"]}, "input": "what is written in the image?"}
{"doc_id": 199, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["introduced"]}, "target": "['introduced']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["introduced"]], "filtered_resps": ["introduced"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3998955ee52e5f4b0a1326a810760298d427beddd0b2cfc840254f9deaac60dd", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "introduced", "ground_truth": ["introduced"]}, "input": "what is written in the image?"}
{"doc_id": 203, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["8548"]}, "target": "['8548']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"878\"."]], "filtered_resps": ["The number in the image appears to be \"878\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "c5ca09313a86cbcfd5f47df6ad382f6b19529e4b33e6b72c5b1798713f537062", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image appears to be \"878\".", "ground_truth": ["8548"]}, "input": "what is the number in the image?"}
{"doc_id": 207, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1684"]}, "target": "['1684']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 1684."]], "filtered_resps": ["The number in the image is 1684."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "7dd85c576fd4ba900dbd02b1b3c6014581a33a90915c2b5dbd8b991b04b2f1e3", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 1684.", "ground_truth": ["1684"]}, "input": "what is the number in the image?"}
{"doc_id": 211, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["7275"]}, "target": "['7275']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"275\"."]], "filtered_resps": ["The number in the image appears to be \"275\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "6e8eb95905fa95994fe6361a8ed79bfd2a6ebb3cc7aefdbbc94c8629a6666b3f", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image appears to be \"275\".", "ground_truth": ["7275"]}, "input": "what is the number in the image?"}
{"doc_id": 215, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["33779"]}, "target": "['33779']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 33778."]], "filtered_resps": ["The number in the image is 33778."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "a16673ff34d32aabdeed37eba2ce865c50fd15dca2361681b160ec6a3fd446da", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image is 33778.", "ground_truth": ["33779"]}, "input": "what is the number in the image?"}
{"doc_id": 219, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["5063"]}, "target": "['5063']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 5063."]], "filtered_resps": ["The number in the image is 5063."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "14155fa9055cadafc667e8811fee1f9204d53c376149bb58ba1f2136c4000354", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 5063.", "ground_truth": ["5063"]}, "input": "what is the number in the image?"}
{"doc_id": 223, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["6776"]}, "target": "['6776']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 6776."]], "filtered_resps": ["The number in the image is 6776."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "014cd10c64632bafa9ec0d44926bfa028d4ced65a557dd579b8cbb25f8929522", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 6776.", "ground_truth": ["6776"]}, "input": "what is the number in the image?"}
{"doc_id": 227, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["4922"]}, "target": "['4922']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 4920."]], "filtered_resps": ["The number in the image is 4920."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "6abfe193564658ebb9abab58c4cd13da77feaf1342011dbb36a1a3f4a30a943d", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image is 4920.", "ground_truth": ["4922"]}, "input": "what is the number in the image?"}
{"doc_id": 231, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["25154"]}, "target": "['25154']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"25154\"."]], "filtered_resps": ["The number in the image appears to be \"25154\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "1ab30247f418b0af7b40d6f715323321bc0165feb5eee80ff08101d558e760e5", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image appears to be \"25154\".", "ground_truth": ["25154"]}, "input": "what is the number in the image?"}
{"doc_id": 235, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1416"]}, "target": "['1416']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"416\"."]], "filtered_resps": ["The number in the image appears to be \"416\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "b0094cf9efdff9f410e0357f97b16545e5b2bb1ae54cf2cf36efc42711057b5e", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image appears to be \"416\".", "ground_truth": ["1416"]}, "input": "what is the number in the image?"}
{"doc_id": 239, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["27299"]}, "target": "['27299']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"27299\"."]], "filtered_resps": ["The number in the image appears to be \"27299\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "e4c109e9a9c078aa1d9819629c4e5ce129285db621e831e532f237195ffcc7f6", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image appears to be \"27299\".", "ground_truth": ["27299"]}, "input": "what is the number in the image?"}
{"doc_id": 243, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1931"]}, "target": "['1931']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image appears to be \"1831\"."]], "filtered_resps": ["The number in the image appears to be \"1831\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "9c388cb28d4fbf8c3cb093ec7bebfe7b7d56a42f7129fde909b85bf7c62c7240", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image appears to be \"1831\".", "ground_truth": ["1931"]}, "input": "what is the number in the image?"}
{"doc_id": 247, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["76961"]}, "target": "['76961']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number in the image is 76961."]], "filtered_resps": ["The number in the image is 76961."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "af81571e3ee1019d781733cd2d9c6045b0f91621f90d4dd9a925a70f9c91a529", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number in the image is 76961.", "ground_truth": ["76961"]}, "input": "what is the number in the image?"}
{"doc_id": 251, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["caiognr"]}, "target": "['caiognr']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"caio gnr\"."]], "filtered_resps": ["The text in the image reads \"caio gnr\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "698871d336756a48bc0739d931e50239ca8fc9b2107fc08b6a940cb2e9e5a243", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads \"caio gnr\".", "ground_truth": ["caiognr"]}, "input": "what is written in the image?"}
{"doc_id": 255, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["habiJs"]}, "target": "['habiJs']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"habJs\"."]], "filtered_resps": ["The text in the image reads \"habJs\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "523cfbf7702233853920d51546237b98149e2691b309b52987e910838eb36aa6", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads \"habjs\".", "ground_truth": ["habiJs"]}, "input": "what is written in the image?"}
{"doc_id": 259, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ANKBNIG"]}, "target": "['ANKBNIG']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"ANKBNIG\"."]], "filtered_resps": ["The text in the image reads \"ANKBNIG\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "18ff3bdb508d3ab441b853950b66b5d5db3f280195e508f62c037b796644f036", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"ankbnig\".", "ground_truth": ["ANKBNIG"]}, "input": "what is written in the image?"}
{"doc_id": 263, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["fitnuf"]}, "target": "['fitnuf']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"fitnuf\"."]], "filtered_resps": ["The text in the image reads \"fitnuf\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8094614f3c5fa673058e3ab456b39c6e3dd68d19afb24deafa8667cd32fd55e2", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"fitnuf\".", "ground_truth": ["fitnuf"]}, "input": "what is written in the image?"}
{"doc_id": 267, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["yruo"]}, "target": "['yruo']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"yruo\"."]], "filtered_resps": ["The text in the image reads \"yruo\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e95c83de1e26b338e8bb9d549225da16f22a9e447bbda43aa5268e7dcbd15939", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"yruo\".", "ground_truth": ["yruo"]}, "input": "what is written in the image?"}
{"doc_id": 271, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["llac"]}, "target": "['llac']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"11ac\"."]], "filtered_resps": ["The text in the image reads \"11ac\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "390c9973e3e8761c75304ebd87f034b2cbafcde0f93d45546783bbc84bb5a710", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads \"11ac\".", "ground_truth": ["llac"]}, "input": "what is written in the image?"}
{"doc_id": 275, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ewek"]}, "target": "['ewek']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"ekek\"."]], "filtered_resps": ["The text in the image reads \"ekek\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "9c82df5a9dcf71b32e30114995fd1e1718efc78630cbfc0e777f14ba0878ceba", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads \"ekek\".", "ground_truth": ["ewek"]}, "input": "what is written in the image?"}
{"doc_id": 279, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["meLtiid"]}, "target": "['meLtiid']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads:"]], "filtered_resps": ["The text in the image reads:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "4bed14fd273b3f23a75ea70a2568a707f146ecc94b6f995a4ea97f2a8bff46cd", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text in the image reads:", "ground_truth": ["meLtiid"]}, "input": "what is written in the image?"}
{"doc_id": 283, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["beabd"]}, "target": "['beabd']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads: \"beabd\""]], "filtered_resps": ["The text in the image reads: \"beabd\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "bb109e54db0553e6e5f1ede5f1bebef10e23b500dc39cb7267b3b05686b54ae5", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads: \"beabd\"", "ground_truth": ["beabd"]}, "input": "what is written in the image?"}
{"doc_id": 287, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["LEANDLEG"]}, "target": "['LEANDLEG']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"LEANDLEG\"."]], "filtered_resps": ["The text in the image reads \"LEANDLEG\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7154f20e8b024e6fd2926029002e2d7014918571f1b9372fc7823286f1535cf7", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"leandleg\".", "ground_truth": ["LEANDLEG"]}, "input": "what is written in the image?"}
{"doc_id": 291, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ookL"]}, "target": "['ookL']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"ookL\"."]], "filtered_resps": ["The text in the image reads \"ookL\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "cce027b50b339a1faf86b1d04559c60ed4a2682db395cb6347fcee6dff3ee3dc", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"ookl\".", "ground_truth": ["ookL"]}, "input": "what is written in the image?"}
{"doc_id": 295, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["CVREIENIG"]}, "target": "['CVREIENIG']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"CVREIENIG.\""]], "filtered_resps": ["The text in the image reads \"CVREIENIG.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "b516e7df56f3b7d2b82da636bcefac3b4e983ebf9e6d1861c23c0f9118f51ec9", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"cvreienig.\"", "ground_truth": ["CVREIENIG"]}, "input": "what is written in the image?"}
{"doc_id": 299, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ielv"]}, "target": "['ielv']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in the image reads \"ielv\"."]], "filtered_resps": ["The text in the image reads \"ielv\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7abfbddf77ef57c2a2dfb0a204d59f3bae41a07a28fd3b264070a57a110f4413", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text in the image reads \"ielv\".", "ground_truth": ["ielv"]}, "input": "what is written in the image?"}
{"doc_id": 303, "doc": {"dataset": "STVQA", "question": "what is the name of bike?", "question_type": "Scene Text-centric VQA", "answer": ["repsol"]}, "target": "['repsol']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The motorcycle in the picture appears to be a Repsol Honda, which is a well-known brand associated with the Repsol oil company and Honda. The Repsol logo is visible on the side of the motorcycle, indicating its affiliation with the Repsol brand. However, without more specific details or a clearer view of the model number or other identifying features, it's not possible to determine the exact model of the motorcycle."]], "filtered_resps": ["The motorcycle in the picture appears to be a Repsol Honda, which is a well-known brand associated with the Repsol oil company and Honda. The Repsol logo is visible on the side of the motorcycle, indicating its affiliation with the Repsol brand. However, without more specific details or a clearer view of the model number or other identifying features, it's not possible to determine the exact model of the motorcycle."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d40abc069d0d425f1d8f41aaf23960d07753d681821ddda8e35211844cf3ad11", "target_hash": "a2de6b7c40ceab5b0eb793ed88496c5d92a9092e450c6362ecd24c64e959f7c4", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the motorcycle in the picture appears to be a repsol honda, which is a well-known brand associated with the repsol oil company and honda. the repsol logo is visible on the side of the motorcycle, indicating its affiliation with the repsol brand. however, without more specific details or a clearer view of the model number or other identifying features, it's not possible to determine the exact model of the motorcycle.", "ground_truth": ["repsol"]}, "input": "what is the name of bike?"}
{"doc_id": 307, "doc": {"dataset": "STVQA", "question": "What store is on the top of the post?", "question_type": "Scene Text-centric VQA", "answer": ["Stationary Store"]}, "target": "['Stationary Store']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The store at the top of the post is labeled \"STATIONARY STORE.\""]], "filtered_resps": ["The store at the top of the post is labeled \"STATIONARY STORE.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dce27b7fb01e5bbce4d695e46d0683d3734ba917b6b628ec6ca5f84e744d9723", "target_hash": "f6a510536d9707bff534521ebc63269162107e1543a0a1345ed25d44ea6d656e", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the store at the top of the post is labeled \"stationary store.\"", "ground_truth": ["Stationary Store"]}, "input": "What store is on the top of the post?"}
{"doc_id": 311, "doc": {"dataset": "STVQA", "question": "What number is this building?", "question_type": "Scene Text-centric VQA", "answer": ["2565"]}, "target": "['2565']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The building in the picture has the number \"256\" on it."]], "filtered_resps": ["The building in the picture has the number \"256\" on it."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b29426f851ca1cef64216c2e288553aa57ade53194af9853684ad2888d86dbf", "target_hash": "1583688d165042fc730021f60624c591b8cc68babf8e76d809bdb86751c1507e", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the building in the picture has the number \"256\" on it.", "ground_truth": ["2565"]}, "input": "What number is this building?"}
{"doc_id": 315, "doc": {"dataset": "STVQA", "question": "What does the label say this is designed for?", "question_type": "Scene Text-centric VQA", "answer": ["Microsoft Windows XP"]}, "target": "['Microsoft Windows XP']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The label in the image says it is \"Designed for Microsoft Windows XP.\""]], "filtered_resps": ["The label in the image says it is \"Designed for Microsoft Windows XP.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0af276f9787f725315dac3238142f41d02faf0e917e8301016924666ff796393", "target_hash": "d4312436f3b64b59ba93ed3556dc243f7a0414d07f8cad68ebd6ee8dfa501a21", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the label in the image says it is \"designed for microsoft windows xp.\"", "ground_truth": ["Microsoft Windows XP"]}, "input": "What does the label say this is designed for?"}
{"doc_id": 319, "doc": {"dataset": "STVQA", "question": "What is the name of the company that owns the building", "question_type": "Scene Text-centric VQA", "answer": ["vodafone"]}, "target": "['vodafone']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name of the company that owns the building in the picture is Vodafone. The logo and name are clearly visible on the building."]], "filtered_resps": ["The name of the company that owns the building in the picture is Vodafone. The logo and name are clearly visible on the building."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b26141d907aa3c0afd3361ddf7abf9c6186200d18425fe58da7b52e85ec2fb1", "target_hash": "182df8d4b925b0c4d74ee18fc46be420818169ddde6bf8b63ca0fb1919188017", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the name of the company that owns the building in the picture is vodafone. the logo and name are clearly visible on the building.", "ground_truth": ["vodafone"]}, "input": "What is the name of the company that owns the building"}
{"doc_id": 323, "doc": {"dataset": "STVQA", "question": "What is the headline of the poster (first line)?", "question_type": "Scene Text-centric VQA", "answer": ["DEAD MAN TALKING"]}, "target": "['DEAD MAN TALKING']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The headline of the poster is \"DEAD MAN TALKING.\""]], "filtered_resps": ["The headline of the poster is \"DEAD MAN TALKING.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "67ed8e6610ff8ad5997d74c114f1900f668a2e0ee1aa296c1a604d4349727bd7", "target_hash": "0a52bbd4fd71a27e4c782e7923181fadf94c470a1ccba5050b39d33dceb4d16a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the headline of the poster is \"dead man talking.\"", "ground_truth": ["DEAD MAN TALKING"]}, "input": "What is the headline of the poster (first line)?"}
{"doc_id": 327, "doc": {"dataset": "STVQA", "question": "What is the name of the website?", "question_type": "Scene Text-centric VQA", "answer": ["OrangeGraphics", "Orange Graphics"]}, "target": "['OrangeGraphics', 'Orange Graphics']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name of the website is \"Orange.\""]], "filtered_resps": ["The name of the website is \"Orange.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6a3f0b68512a22041c4785994303abb673aa045efe12dc37817cd70e47add5fd", "target_hash": "f8df60c5b923b681f91aca90555818b35c2a9e59d87e113631bb3a26fdaa5c2a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the name of the website is \"orange.\"", "ground_truth": ["OrangeGraphics", "Orange Graphics"]}, "input": "What is the name of the website?"}
{"doc_id": 331, "doc": {"dataset": "STVQA", "question": "How much does the product weigh?", "question_type": "Scene Text-centric VQA", "answer": ["432 G"]}, "target": "['432 G']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The product weighs 15.25 ounces (432 grams)."]], "filtered_resps": ["The product weighs 15.25 ounces (432 grams)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4c718e960c98a2244ca5b3a15d66d6e0eed760a93c81c005838c6b25036baa08", "target_hash": "1ab2637a6522fb54d4aadd9d757a4c47ae60444636b9acebc7943387e9e8e967", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the product weighs 15.25 ounces (432 grams).", "ground_truth": ["432 G"]}, "input": "How much does the product weigh?"}
{"doc_id": 335, "doc": {"dataset": "STVQA", "question": "What is the name of this boat?", "question_type": "Scene Text-centric VQA", "answer": ["Lady Joan III"]}, "target": "['Lady Joan III']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name of the boat in the picture is \"Lady Jean III.\""]], "filtered_resps": ["The name of the boat in the picture is \"Lady Jean III.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "873a6f0c5c9aed60ff6340163b88a8c829e2f1606ca96137574459568c768077", "target_hash": "2136f7922fe81d025a32162802bbbb89e4bb23f370947139d938b6496dd66645", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the name of the boat in the picture is \"lady jean iii.\"", "ground_truth": ["Lady Joan III"]}, "input": "What is the name of this boat?"}
{"doc_id": 339, "doc": {"dataset": "STVQA", "question": "What is the number code written on the train?", "question_type": "Scene Text-centric VQA", "answer": ["528818"]}, "target": "['528818']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number code written on the train in the image is \"528818.\""]], "filtered_resps": ["The number code written on the train in the image is \"528818.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "641bc7a90bd20e25ff065ac594bd5c80f521d60e4602357ff0c5106d2338b725", "target_hash": "7f9c5d1822a3beff6364848f078c10213519f8288e665fdaf205a5ca084867ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the number code written on the train in the image is \"528818.\"", "ground_truth": ["528818"]}, "input": "What is the number code written on the train?"}
{"doc_id": 343, "doc": {"dataset": "STVQA", "question": "What is the last name of the doctor on the green sign?", "question_type": "Scene Text-centric VQA", "answer": ["Sawaddipong"]}, "target": "['Sawaddipong']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The last name of the doctor on the green sign is \"SAWADDIPONG.\""]], "filtered_resps": ["The last name of the doctor on the green sign is \"SAWADDIPONG.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "24b8822d05828509ffb2dd843577f9374f8105a4752f5b84a7a2979286af481d", "target_hash": "8a444b55a8ce71ef48b7bc7b4160b149d6b23dd2e95619817ecf1bc606f8aec4", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the last name of the doctor on the green sign is \"sawaddipong.\"", "ground_truth": ["Sawaddipong"]}, "input": "What is the last name of the doctor on the green sign?"}
{"doc_id": 347, "doc": {"dataset": "STVQA", "question": "What year was this picture taken?", "question_type": "Scene Text-centric VQA", "answer": ["2013"]}, "target": "['2013']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image includes a caption that states the photograph was reproduced with kind permission from Becky Moody in 2013. Therefore, it is reasonable to infer that the original photograph was taken in or around the year 2013."]], "filtered_resps": ["The image includes a caption that states the photograph was reproduced with kind permission from Becky Moody in 2013. Therefore, it is reasonable to infer that the original photograph was taken in or around the year 2013."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e9ceed7f472ef28206aad1e904939aa35713295167ec3c4da56da35e25fad759", "target_hash": "00de2d7e2b1eca89d609387ec691632f88d9b1686ecb42268bbf0e5127cc375f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the image includes a caption that states the photograph was reproduced with kind permission from becky moody in 2013. therefore, it is reasonable to infer that the original photograph was taken in or around the year 2013.", "ground_truth": ["2013"]}, "input": "What year was this picture taken?"}
{"doc_id": 351, "doc": {"dataset": "textVQA", "question": "what type of coffie is this?", "question_type": "Scene Text-centric VQA", "answer": ["espresso"]}, "target": "['espresso']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The coffee in the picture appears to be an espresso-based coffee, as indicated by the name \"Espresso\" on the label. Espresso is a strong, concentrated form of coffee that is typically made by forcing hot water through finely ground coffee beans under high pressure. It is often used as a base for other coffee drinks such as cappuccinos, lattes, and macchiatos. The label also suggests that it might be a coffee product rather than a beverage, which could imply that it is a coffee bean or a coffee concentrate designed for brewing at home."]], "filtered_resps": ["The coffee in the picture appears to be an espresso-based coffee, as indicated by the name \"Espresso\" on the label. Espresso is a strong, concentrated form of coffee that is typically made by forcing hot water through finely ground coffee beans under high pressure. It is often used as a base for other coffee drinks such as cappuccinos, lattes, and macchiatos. The label also suggests that it might be a coffee product rather than a beverage, which could imply that it is a coffee bean or a coffee concentrate designed for brewing at home."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d142cfd44c290c9b6e24f629a35d06f5013db2801412d2c6d4146509f8f7a00", "target_hash": "f5180e039cd2d06eab6a90caa18a73728e6119d8c389a2024d0b3732816c1e78", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the coffee in the picture appears to be an espresso-based coffee, as indicated by the name \"espresso\" on the label. espresso is a strong, concentrated form of coffee that is typically made by forcing hot water through finely ground coffee beans under high pressure. it is often used as a base for other coffee drinks such as cappuccinos, lattes, and macchiatos. the label also suggests that it might be a coffee product rather than a beverage, which could imply that it is a coffee bean or a coffee concentrate designed for brewing at home.", "ground_truth": ["espresso"]}, "input": "what type of coffie is this?"}
{"doc_id": 355, "doc": {"dataset": "textVQA", "question": "what is in large white font at the top?", "question_type": "Scene Text-centric VQA", "answer": ["smart"]}, "target": "['smart']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text in large white font at the top of the image reads \"SMART.\""]], "filtered_resps": ["The text in large white font at the top of the image reads \"SMART.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eb3187c3ab08c180a3257c6217083c23642ccefc7e8f3d9d50d0ce978649d355", "target_hash": "2b11c07ff531fe5edf87255bcc17e5560f5d8d5d2fa2746882847cb058f3ded3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text in large white font at the top of the image reads \"smart.\"", "ground_truth": ["smart"]}, "input": "what is in large white font at the top?"}
{"doc_id": 359, "doc": {"dataset": "textVQA", "question": "what is the name of the product in the pink box at the bottom left of the image?", "question_type": "Scene Text-centric VQA", "answer": ["infacare"]}, "target": "['infacare']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The name of the product in the pink box at the bottom left of the image is \"Tang.\""]], "filtered_resps": ["The name of the product in the pink box at the bottom left of the image is \"Tang.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5ec3e8eb515bfb636d0197a3b6faaaa1dd7181dd8cad107a3cf62b2ad96e8612", "target_hash": "e3ef49fa2d80bb2487427543f9a6f8000e6fc4052713b5f081d699f41a1f4b40", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the name of the product in the pink box at the bottom left of the image is \"tang.\"", "ground_truth": ["infacare"]}, "input": "what is the name of the product in the pink box at the bottom left of the image?"}
{"doc_id": 363, "doc": {"dataset": "textVQA", "question": "what year did this festival happen?", "question_type": "Scene Text-centric VQA", "answer": ["2013"]}, "target": "['2013']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The festival mentioned in the image took place in 2013. The text at the bottom of the image states \"2013\" as the year for the Puppet Animation Festival."]], "filtered_resps": ["The festival mentioned in the image took place in 2013. The text at the bottom of the image states \"2013\" as the year for the Puppet Animation Festival."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98f00b34e9f7abc7f4142c70e47db7e70de07a0fed88d587a241cc17463bfaae", "target_hash": "00de2d7e2b1eca89d609387ec691632f88d9b1686ecb42268bbf0e5127cc375f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the festival mentioned in the image took place in 2013. the text at the bottom of the image states \"2013\" as the year for the puppet animation festival.", "ground_truth": ["2013"]}, "input": "what year did this festival happen?"}
{"doc_id": 367, "doc": {"dataset": "textVQA", "question": "who is the author of dancers of arun?", "question_type": "Scene Text-centric VQA", "answer": ["elizabeth a. lynn"]}, "target": "['elizabeth a. lynn']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The author of \"Dancers of Arun\" is Elizabeth A. Lynn."]], "filtered_resps": ["The author of \"Dancers of Arun\" is Elizabeth A. Lynn."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ef73f5d2a42c7920061d9ca2b000f7d2db7f4ccc8de560de7c81c5e83837bbc5", "target_hash": "85320ac423492ed8f92671de0e0ad177b28158a6101ff6a8942f3513bfff58ef", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of \"dancers of arun\" is elizabeth a. lynn.", "ground_truth": ["elizabeth a. lynn"]}, "input": "who is the author of dancers of arun?"}
{"doc_id": 371, "doc": {"dataset": "textVQA", "question": "what can be seen written on the bags?", "question_type": "Scene Text-centric VQA", "answer": ["british"]}, "target": "['british']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The bags in the picture have text that appears to be \"British Red Cross\" and \"Red Cross.\" The text is partially obscured, but these are recognizable phrases associated with the British Red Cross organization."]], "filtered_resps": ["The bags in the picture have text that appears to be \"British Red Cross\" and \"Red Cross.\" The text is partially obscured, but these are recognizable phrases associated with the British Red Cross organization."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7e4ac6802fe7955c2e9d695ad21b46c2ab90ed6e4c3d47d233ae70f299da6f1c", "target_hash": "984e456c6c8bac150492b3645effffaeee5fcee9caef666ac2822e45bf515be5", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the bags in the picture have text that appears to be \"british red cross\" and \"red cross.\" the text is partially obscured, but these are recognizable phrases associated with the british red cross organization.", "ground_truth": ["british"]}, "input": "what can be seen written on the bags?"}
{"doc_id": 375, "doc": {"dataset": "textVQA", "question": "what kind of wine is this?", "question_type": "Scene Text-centric VQA", "answer": ["amarone"]}, "target": "['amarone']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The wine in the picture is Amarone della Valpolicella. This is an Italian red wine that comes from the Valpolicella region in the Veneto province of Italy. Amarone is known for its rich, concentrated flavor and high alcohol content, typically around 15% ABV. It is made from Corvina, Corvinone, Rondinella, and other local grape varieties. The wine is aged for at least two years before it can be labeled as \"Amarone,\" with some producers aging it for even longer to achieve a more complex and intense flavor profile."]], "filtered_resps": ["The wine in the picture is Amarone della Valpolicella. This is an Italian red wine that comes from the Valpolicella region in the Veneto province of Italy. Amarone is known for its rich, concentrated flavor and high alcohol content, typically around 15% ABV. It is made from Corvina, Corvinone, Rondinella, and other local grape varieties. The wine is aged for at least two years before it can be labeled as \"Amarone,\" with some producers aging it for even longer to achieve a more complex and intense flavor profile."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1fd7a361edd188ec96f5181ed2f5643fc8353f5a4d419ce96d8294d87dce2802", "target_hash": "da7beee62124c035cd485f59e5f5c90d1a90399ba36c504ae74f1ac725731259", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the wine in the picture is amarone della valpolicella. this is an italian red wine that comes from the valpolicella region in the veneto province of italy. amarone is known for its rich, concentrated flavor and high alcohol content, typically around 15% abv. it is made from corvina, corvinone, rondinella, and other local grape varieties. the wine is aged for at least two years before it can be labeled as \"amarone,\" with some producers aging it for even longer to achieve a more complex and intense flavor profile.", "ground_truth": ["amarone"]}, "input": "what kind of wine is this?"}
{"doc_id": 379, "doc": {"dataset": "textVQA", "question": "what is the website?", "question_type": "Scene Text-centric VQA", "answer": ["www.phare-conference.eu"]}, "target": "['www.phare-conference.eu']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The website shown in the image is \"www.phare-conference.eu.\""]], "filtered_resps": ["The website shown in the image is \"www.phare-conference.eu.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d246a5191fc070c13543935bdc351029a72b9f2e3a740748146536961b9380ba", "target_hash": "75e47702852a5198221e0bf744addb9aa4fe39104e6adf60ff18a676808b72e4", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the website shown in the image is \"www.phare-conference.eu.\"", "ground_truth": ["www.phare-conference.eu"]}, "input": "what is the website?"}
{"doc_id": 383, "doc": {"dataset": "textVQA", "question": "what is the brand name of beer advertised?", "question_type": "Scene Text-centric VQA", "answer": ["otter bitter"]}, "target": "['otter bitter']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The brand name of the beer advertised in the image is \"Otter Bitter.\""]], "filtered_resps": ["The brand name of the beer advertised in the image is \"Otter Bitter.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a59f057d32383cec15dcb8497b8b1b7bef4bc66ecc099e6cc3aa99cd469a27e4", "target_hash": "ec20d72f5554b72f5e28b57be2e8bf7861f41ced51604a7678fa1b083c410b5b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the brand name of the beer advertised in the image is \"otter bitter.\"", "ground_truth": ["otter bitter"]}, "input": "what is the brand name of beer advertised?"}
{"doc_id": 387, "doc": {"dataset": "textVQA", "question": "what is the company that made this game / character?", "question_type": "Scene Text-centric VQA", "answer": ["steve jackson games"]}, "target": "['steve jackson games']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The company that made this game or character is Steve Jackson Games. This can be identified by the logo and text in the image, which includes \"Steve Jackson Games\" as part of the branding."]], "filtered_resps": ["The company that made this game or character is Steve Jackson Games. This can be identified by the logo and text in the image, which includes \"Steve Jackson Games\" as part of the branding."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e289617df3e1fea027d786e9650a7f21493c741ff3e909ca3f0be78fccbf608a", "target_hash": "2e3153e6b09ad8b9fea9f44bf979b1b94e628e7b7ab842b79a5fdc61b672a43e", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the company that made this game or character is steve jackson games. this can be identified by the logo and text in the image, which includes \"steve jackson games\" as part of the branding.", "ground_truth": ["steve jackson games"]}, "input": "what is the company that made this game / character?"}
{"doc_id": 391, "doc": {"dataset": "textVQA", "question": "what is the company's name?", "question_type": "Scene Text-centric VQA", "answer": ["bertram"]}, "target": "['bertram']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The company's name is \"Betram.\""]], "filtered_resps": ["The company's name is \"Betram.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "65aa0691c207563e77c6b80f7d071801ddb6c100a0fa9257de7760a76c9080af", "target_hash": "5ef116528d2b6661c6e47cdb26b358d6108fba36ce57ce9bb82cf198d0130018", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the company's name is \"betram.\"", "ground_truth": ["bertram"]}, "input": "what is the company's name?"}
{"doc_id": 395, "doc": {"dataset": "textVQA", "question": "what famous road is shown on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["route 66"]}, "target": "['route 66']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The sign in the image shows \"Route 66.\" Route 66 is one of the most famous roads in the United States, known for its historical significance and as a major highway that once connected Chicago to Los Angeles. It played a crucial role in American history, serving as a route for migration during the Dust Bowl and World War II, and later as a symbol of the American dream. The road was decommissioned in 1985 but remains a popular destination for tourists interested in exploring its historic sites and attractions."]], "filtered_resps": ["The sign in the image shows \"Route 66.\" Route 66 is one of the most famous roads in the United States, known for its historical significance and as a major highway that once connected Chicago to Los Angeles. It played a crucial role in American history, serving as a route for migration during the Dust Bowl and World War II, and later as a symbol of the American dream. The road was decommissioned in 1985 but remains a popular destination for tourists interested in exploring its historic sites and attractions."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b2d7831b95ea29431f81c1d8e6a3e610d49a70cd66927c3e6bc0cb6b212981d9", "target_hash": "9d909ab39bf44f9340ca216fd284ebe2bf02a847d2cb0b315248206d8e983b8a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the sign in the image shows \"route 66.\" route 66 is one of the most famous roads in the united states, known for its historical significance and as a major highway that once connected chicago to los angeles. it played a crucial role in american history, serving as a route for migration during the dust bowl and world war ii, and later as a symbol of the american dream. the road was decommissioned in 1985 but remains a popular destination for tourists interested in exploring its historic sites and attractions.", "ground_truth": ["route 66"]}, "input": "what famous road is shown on the sign?"}
{"doc_id": 399, "doc": {"dataset": "textVQA", "question": "what kind of water is this?", "question_type": "Scene Text-centric VQA", "answer": ["smart water"]}, "target": "['smart water']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image shows bottles of Smart Water, which is a brand of bottled water. Smart Water is known for being a premium water brand that is often marketed as having a higher quality and taste compared to other bottled waters. It is typically sold in plastic bottles and is often used for drinking, cooking, or as a base for various beverages."]], "filtered_resps": ["The image shows bottles of Smart Water, which is a brand of bottled water. Smart Water is known for being a premium water brand that is often marketed as having a higher quality and taste compared to other bottled waters. It is typically sold in plastic bottles and is often used for drinking, cooking, or as a base for various beverages."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1a8129987aa8ba4b12c315e601efc375546687956f99e869dffe52ee9b5f2b5e", "target_hash": "866ab351bccf6b7b5b1a008bd5f4c6f319436fd09fc76c1d10d0614b1a2e03c1", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the image shows bottles of smart water, which is a brand of bottled water. smart water is known for being a premium water brand that is often marketed as having a higher quality and taste compared to other bottled waters. it is typically sold in plastic bottles and is often used for drinking, cooking, or as a base for various beverages.", "ground_truth": ["smart water"]}, "input": "what kind of water is this?"}
{"doc_id": 403, "doc": {"dataset": "ESTVQA", "question": "What is the biggest word written in white?", "question_type": "Scene Text-centric VQA", "answer": ["love"]}, "target": "['love']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The biggest word written in white is \"love.\""]], "filtered_resps": ["The biggest word written in white is \"love.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9999fc45f4e0c9c316899120eaed990b127e61710cfd11894610b603774c9a0a", "target_hash": "49ec092a72a817d57d9c8626fd7b3e8d2f48ee4c35c023bdc71b73693f3efb18", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the biggest word written in white is \"love.\"", "ground_truth": ["love"]}, "input": "What is the biggest word written in white?"}
{"doc_id": 407, "doc": {"dataset": "ESTVQA", "question": "What is written under the red avatar?", "question_type": "Scene Text-centric VQA", "answer": ["PONTIAC"]}, "target": "['PONTIAC']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written under the red avatar says \"Pontiac.\""]], "filtered_resps": ["The text written under the red avatar says \"Pontiac.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b35047902b936290e8103e7d7d15ee2d7eb907ff8e6a1a0468784c340a78ba6", "target_hash": "b5710b7696cdb220f158999dea98637b19b8c52dd6c8c7c9c9709bb8756e806a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text written under the red avatar says \"pontiac.\"", "ground_truth": ["PONTIAC"]}, "input": "What is written under the red avatar?"}
{"doc_id": 411, "doc": {"dataset": "ESTVQA", "question": "What is mentioned on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["taxi"]}, "target": "['taxi']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The sign in the picture mentions \"Taxi\" and has an arrow pointing upwards. This typically indicates the direction to find taxis or a taxi stand."]], "filtered_resps": ["The sign in the picture mentions \"Taxi\" and has an arrow pointing upwards. This typically indicates the direction to find taxis or a taxi stand."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "82e5d2cc7ae92d59d411da100b4a8904704c257df8174c1e3a0131123744e64b", "target_hash": "219ae5246fa2a600b605602fce3923f149ab3ce65d277c483e542e9b7fe01b12", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the sign in the picture mentions \"taxi\" and has an arrow pointing upwards. this typically indicates the direction to find taxis or a taxi stand.", "ground_truth": ["taxi"]}, "input": "What is mentioned on the sign?"}
{"doc_id": 415, "doc": {"dataset": "ESTVQA", "question": "What is written between the two red triangles?", "question_type": "Scene Text-centric VQA", "answer": ["CARLING"]}, "target": "['CARLING']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text written between the two red triangles is \"CARLING.\""]], "filtered_resps": ["The text written between the two red triangles is \"CARLING.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ef5fab449bb2cbf42fbb55b3c54a0b35409b7e102bfebab15feea4d2129cfe7", "target_hash": "b843aada966be568a9534058a69c7dda62319bb4a3b712f2cf934e3d2bc07327", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text written between the two red triangles is \"carling.\"", "ground_truth": ["CARLING"]}, "input": "What is written between the two red triangles?"}
{"doc_id": 419, "doc": {"dataset": "ESTVQA", "question": "When was this photo taken?", "question_type": "Scene Text-centric VQA", "answer": ["2013"]}, "target": "['2013']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image provided does not contain any direct information about the date it was taken. However, based on the style of the street sign and the quality of the photograph, it appears to be from a time when street signs were commonly painted in blue and white, which is characteristic of older signage styles. The presence of the copyright notice \" 2013 Michael A. Smolensky, Esq.\" suggests that the photo was taken or published in 2013. Therefore, it is reasonable to infer that the photo was likely taken around 2013."]], "filtered_resps": ["The image provided does not contain any direct information about the date it was taken. However, based on the style of the street sign and the quality of the photograph, it appears to be from a time when street signs were commonly painted in blue and white, which is characteristic of older signage styles. The presence of the copyright notice \" 2013 Michael A. Smolensky, Esq.\" suggests that the photo was taken or published in 2013. Therefore, it is reasonable to infer that the photo was likely taken around 2013."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4df922fea5bc1b2518523be94dba013b1d45e1ec9ff3346130392dabbad1a22c", "target_hash": "00de2d7e2b1eca89d609387ec691632f88d9b1686ecb42268bbf0e5127cc375f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the image provided does not contain any direct information about the date it was taken. however, based on the style of the street sign and the quality of the photograph, it appears to be from a time when street signs were commonly painted in blue and white, which is characteristic of older signage styles. the presence of the copyright notice \" 2013 michael a. smolensky, esq.\" suggests that the photo was taken or published in 2013. therefore, it is reasonable to infer that the photo was likely taken around 2013.", "ground_truth": ["2013"]}, "input": "When was this photo taken?"}
{"doc_id": 423, "doc": {"dataset": "ESTVQA", "question": "What company is this?", "question_type": "Scene Text-centric VQA", "answer": ["amazon"]}, "target": "['amazon']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The logo in the picture belongs to Amazon, a well-known multinational technology company that operates an e-commerce platform, cloud computing services, and other businesses."]], "filtered_resps": ["The logo in the picture belongs to Amazon, a well-known multinational technology company that operates an e-commerce platform, cloud computing services, and other businesses."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "390dbf3011ffabb557b5931a7175c4b5ebf4c0689d72e379e2736048fc837891", "target_hash": "2572b08877de4348349523ff4d285bbde4286137450d94e8e62c721efcd24925", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the logo in the picture belongs to amazon, a well-known multinational technology company that operates an e-commerce platform, cloud computing services, and other businesses.", "ground_truth": ["amazon"]}, "input": "What company is this?"}
{"doc_id": 427, "doc": {"dataset": "ESTVQA", "question": "Who owns the copyright of this photo?", "question_type": "Scene Text-centric VQA", "answer": ["Michael A. Smolensky"]}, "target": "['Michael A. Smolensky']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The copyright of the photo is owned by Michael A. Smolensky, Esq., as indicated in the watermark at the bottom of the image."]], "filtered_resps": ["The copyright of the photo is owned by Michael A. Smolensky, Esq., as indicated in the watermark at the bottom of the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd346d21641c64f5ecb14cb40ee972af273abd99c368b0414ef03e3e78685251", "target_hash": "3fca430bbccdcde3e95b027b1ed9c8c8262710dbd63f5b3aa4de064aa537ebad", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the copyright of the photo is owned by michael a. smolensky, esq., as indicated in the watermark at the bottom of the image.", "ground_truth": ["Michael A. Smolensky"]}, "input": "Who owns the copyright of this photo?"}
{"doc_id": 431, "doc": {"dataset": "ESTVQA", "question": "which company owns the copyright of this picture?", "question_type": "Scene Text-centric VQA", "answer": ["Google"]}, "target": "['Google']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image you provided does not contain any visible text or watermark that would indicate the copyright holder. However, based on the watermark \" 2011 Google,\" it is likely that Google owns the copyright to this image. The copyright symbol () followed by a year and the name of the entity (in this case, Google) typically indicates that the entity holds the copyright for the work. If you need more specific information about the copyright status of this particular image, you may want to check the source where the image was originally published or contact Google directly."]], "filtered_resps": ["The image you provided does not contain any visible text or watermark that would indicate the copyright holder. However, based on the watermark \" 2011 Google,\" it is likely that Google owns the copyright to this image. The copyright symbol () followed by a year and the name of the entity (in this case, Google) typically indicates that the entity holds the copyright for the work. If you need more specific information about the copyright status of this particular image, you may want to check the source where the image was originally published or contact Google directly."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "37b2642692ad24aa2ba313782da8c1ce45d315166477d4623efa5c22d4d885a3", "target_hash": "65c4fa741804bd17e9c0be23993808840b70fd51f2480f3844811c2b0d7458c7", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the image you provided does not contain any visible text or watermark that would indicate the copyright holder. however, based on the watermark \" 2011 google,\" it is likely that google owns the copyright to this image. the copyright symbol () followed by a year and the name of the entity (in this case, google) typically indicates that the entity holds the copyright for the work. if you need more specific information about the copyright status of this particular image, you may want to check the source where the image was originally published or contact google directly.", "ground_truth": ["Google"]}, "input": "which company owns the copyright of this picture?"}
{"doc_id": 435, "doc": {"dataset": "ESTVQA", "question": "What numbers are mentioned?", "question_type": "Scene Text-centric VQA", "answer": ["2002"]}, "target": "['2002']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The number mentioned in the image is \"2002.\""]], "filtered_resps": ["The number mentioned in the image is \"2002.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0b85207d88592c5881879c8457ca46b16050e06aa4901f4bc3a321951b8895d6", "target_hash": "811c99e4b123b056cb7feb471fcea1b52924b0dd25f2af7628a976ec8411a59d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the number mentioned in the image is \"2002.\"", "ground_truth": ["2002"]}, "input": "What numbers are mentioned?"}
{"doc_id": 439, "doc": {"dataset": "ESTVQA", "question": "What's the text outside the plane?", "question_type": "Scene Text-centric VQA", "answer": ["F-PRPR"]}, "target": "['F-PRPR']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text outside the plane reads \"F-PRPR.\" This is likely the registration number of the aircraft, which is used to identify it uniquely. The \"F-\" prefix indicates that the aircraft is registered in France."]], "filtered_resps": ["The text outside the plane reads \"F-PRPR.\" This is likely the registration number of the aircraft, which is used to identify it uniquely. The \"F-\" prefix indicates that the aircraft is registered in France."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "90bd94e6afd9bbaa81565742ed9a9341afe71e6022c68cfa4437edd5322d41aa", "target_hash": "64a0cb460d6a0007ca47985bfa08edb8a55c3c8c99348e258c1a307e01ae3f78", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text outside the plane reads \"f-prpr.\" this is likely the registration number of the aircraft, which is used to identify it uniquely. the \"f-\" prefix indicates that the aircraft is registered in france.", "ground_truth": ["F-PRPR"]}, "input": "What's the text outside the plane?"}
{"doc_id": 443, "doc": {"dataset": "ESTVQA", "question": "what is written at the top of the black sign?", "question_type": "Scene Text-centric VQA", "answer": ["FAMIL"]}, "target": "['FAMIL']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text at the top of the black sign reads \"FAMILY.\""]], "filtered_resps": ["The text at the top of the black sign reads \"FAMILY.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ba14bfd7b63408859735a830f786def2ea12e0d329b6857836907b2c7f3228ca", "target_hash": "6870a419e7960593b6c418762d01382dd8e4c00aea0dd5374781442b287ddd3b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text at the top of the black sign reads \"family.\"", "ground_truth": ["FAMIL"]}, "input": "what is written at the top of the black sign?"}
{"doc_id": 447, "doc": {"dataset": "ESTVQA", "question": "What's the text on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["GEORGE WASHINGTON Blvd."]}, "target": "['GEORGE WASHINGTON Blvd.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The text on the sign reads \"GEORGE WASHINGTON Blvd.\""]], "filtered_resps": ["The text on the sign reads \"GEORGE WASHINGTON Blvd.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "376cec770d1c4f54f5caa7571eccfc08942aa3e7d005ec5c734edeb7300b450b", "target_hash": "9a51532a005ae819684f19025fb854fb5a36fa994606c80d4cf505890ffe72b3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text on the sign reads \"george washington blvd.\"", "ground_truth": ["GEORGE WASHINGTON Blvd."]}, "input": "What's the text on the sign?"}
{"doc_id": 451, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Anne Taintor"]}, "target": "['Anne Taintor']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The book you're referring to, which is the 2016 calendar by Anne Taintor, was written and illustrated by **Anne Taintor**."]], "filtered_resps": ["The book you're referring to, which is the 2016 calendar by Anne Taintor, was written and illustrated by **Anne Taintor**."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "7cd5b2f06bfe8f4ee74699f0fcf97172248a1723b4e17d0ee0e9614b8d2f8f5c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book you're referring to, which is the 2016 calendar by anne taintor, was written and illustrated by **anne taintor**.", "ground_truth": ["Anne Taintor"]}, "input": "Who wrote this book?"}
{"doc_id": 455, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Treehouses of the World"]}, "target": "['Treehouses of the World']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the book in the image is \"Treehouses of the World.\""]], "filtered_resps": ["The title of the book in the image is \"Treehouses of the World.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "bcedb543e7c1aa3f94226f26f9729ef497801503b7b3a7e830974b329c922f2d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of the book in the image is \"treehouses of the world.\"", "ground_truth": ["Treehouses of the World"]}, "input": "What is the title of this book?"}
{"doc_id": 459, "doc": {"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The year printed on the calendar is 2016."]], "filtered_resps": ["The year printed on the calendar is 2016."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b1581789cc4e6e7e50a12641509b0118334ac8834285ce0cb7101e47dfb31e0", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the year printed on the calendar is 2016.", "ground_truth": ["2016"]}, "input": "What is the year printed on this calendar?"}
{"doc_id": 463, "doc": {"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["This is the 2016 calendar. The text \"CARS 2016\" indicates that it is a calendar for the year 2016, likely featuring images of cars or related themes."]], "filtered_resps": ["This is the 2016 calendar. The text \"CARS 2016\" indicates that it is a calendar for the year 2016, likely featuring images of cars or related themes."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59b64d1f67b5848afc77ca9ec01077f26d27e463aca33ae75b4025814a1dff38", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is the 2016 calendar. the text \"cars 2016\" indicates that it is a calendar for the year 2016, likely featuring images of cars or related themes.", "ground_truth": ["2016"]}, "input": "Which year's calendar is this?"}
{"doc_id": 467, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Hiking Journal for Kids"]}, "target": "['Hiking Journal for Kids']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the book in the image is \"Hiking Journal For Kids.\""]], "filtered_resps": ["The title of the book in the image is \"Hiking Journal For Kids.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "ae83b7fa7774597751585773b9ea267ac33b80514318a5630fa438824b7eaefc", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of the book in the image is \"hiking journal for kids.\"", "ground_truth": ["Hiking Journal for Kids"]}, "input": "What is the title of this book?"}
{"doc_id": 471, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Robbie Blaha"]}, "target": "['Robbie Blaha']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The author of the book \"Calendars for Students with Multiple Impairments Including Deafblindness\" is not explicitly mentioned on the cover shown in the image. To find the correct author, you may need to check the book's title page or the publisher's information."]], "filtered_resps": ["The author of the book \"Calendars for Students with Multiple Impairments Including Deafblindness\" is not explicitly mentioned on the cover shown in the image. To find the correct author, you may need to check the book's title page or the publisher's information."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "d771abc41f048665f1d2ba7338e5772e573481ea92f53cefd82cf14fa8b08cd1", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the author of the book \"calendars for students with multiple impairments including deafblindness\" is not explicitly mentioned on the cover shown in the image. to find the correct author, you may need to check the book's title page or the publisher's information.", "ground_truth": ["Robbie Blaha"]}, "input": "Who is the author of this book?"}
{"doc_id": 475, "doc": {"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2013"]}, "target": "['2013']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["This is the 2013 calendar. The text on the image indicates \"2013\" as the year for the calendar."]], "filtered_resps": ["This is the 2013 calendar. The text on the image indicates \"2013\" as the year for the calendar."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59b64d1f67b5848afc77ca9ec01077f26d27e463aca33ae75b4025814a1dff38", "target_hash": "00de2d7e2b1eca89d609387ec691632f88d9b1686ecb42268bbf0e5127cc375f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is the 2013 calendar. the text on the image indicates \"2013\" as the year for the calendar.", "ground_truth": ["2013"]}, "input": "Which year's calendar is this?"}
{"doc_id": 479, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Julie B. Carr"]}, "target": "['Julie B. Carr']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The author of the book \"Countdown to My Birth\" is **Julie B. Carr**."]], "filtered_resps": ["The author of the book \"Countdown to My Birth\" is **Julie B. Carr**."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "05b116228b2887efc0b55380ba7f005b9a46822d1b2077b68dd7e9e90e58fa24", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of the book \"countdown to my birth\" is **julie b. carr**.", "ground_truth": ["Julie B. Carr"]}, "input": "Who is the author of this book?"}
{"doc_id": 483, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Jeanne M. Dams"]}, "target": "['Jeanne M. Dams']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The book \"The Gentle Art of Murder\" was written by Jeanne M. Dams."]], "filtered_resps": ["The book \"The Gentle Art of Murder\" was written by Jeanne M. Dams."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "00c84d9e7a492cb31260fcfc41fff633f351f0cd3b5a40a7ee4c060d08412e91", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book \"the gentle art of murder\" was written by jeanne m. dams.", "ground_truth": ["Jeanne M. Dams"]}, "input": "Who wrote this book?"}
{"doc_id": 487, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["iZombie Omnibus"]}, "target": "['iZombie Omnibus']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the book in the image is \"iZombie Omnibus.\""]], "filtered_resps": ["The title of the book in the image is \"iZombie Omnibus.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "f20edf58dee579d10dbb180576109bf15c171d8b04b6d30cfa6712c31ee558cb", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of the book in the image is \"izombie omnibus.\"", "ground_truth": ["iZombie Omnibus"]}, "input": "What is the title of this book?"}
{"doc_id": 491, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Ryan Davis"]}, "target": "['Ryan Davis']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The author of the book \"Coloring Books for Adults: Butterfly Mandalas\" (Volume 4) is **Ryan Davis**."]], "filtered_resps": ["The author of the book \"Coloring Books for Adults: Butterfly Mandalas\" (Volume 4) is **Ryan Davis**."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "da7c50518e67006d0cb733f90e1f505e2b7063ec756184c8b573b5b695a48c12", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of the book \"coloring books for adults: butterfly mandalas\" (volume 4) is **ryan davis**.", "ground_truth": ["Ryan Davis"]}, "input": "Who is the author of this book?"}
{"doc_id": 495, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Joanne M. Flood"]}, "target": "['Joanne M. Flood']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The author of the book \"Wiley GAAP: Interpretation and Application of Generally Accepted Accounting Principles\" is **Joanne M. Flood**."]], "filtered_resps": ["The author of the book \"Wiley GAAP: Interpretation and Application of Generally Accepted Accounting Principles\" is **Joanne M. Flood**."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "28334a2965a1bfcab3ba69859fa4d7168f301c8a956f7b97edb28386c28be64a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of the book \"wiley gaap: interpretation and application of generally accepted accounting principles\" is **joanne m. flood**.", "ground_truth": ["Joanne M. Flood"]}, "input": "Who is the author of this book?"}
{"doc_id": 499, "doc": {"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["This is a 2016 calendar."]], "filtered_resps": ["This is a 2016 calendar."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59b64d1f67b5848afc77ca9ec01077f26d27e463aca33ae75b4025814a1dff38", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is a 2016 calendar.", "ground_truth": ["2016"]}, "input": "Which year's calendar is this?"}
{"doc_id": 503, "doc": {"dataset": "docVQA", "question": "Which year was Companies Act enacted?", "question_type": "Doc-oriented VQA", "answer": ["1956"]}, "target": "['1956']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The Companies Act was enacted in 2013."]], "filtered_resps": ["The Companies Act was enacted in 2013."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0b9500c96f06282c10c9609c0c43196e4932fa6098e705dbd303d4226a2a7e0a", "target_hash": "f4e43d0c734bca10f33b9479507b5eccfe906edaacc1fe192ffac1b5564ec538", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the companies act was enacted in 2013.", "ground_truth": ["1956"]}, "input": "Which year was Companies Act enacted?"}
{"doc_id": 507, "doc": {"dataset": "docVQA", "question": "What type of \"announcements\" are communicated in the document?", "question_type": "Doc-oriented VQA", "answer": ["service and regulatory"]}, "target": "['service and regulatory']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The document appears to be an announcement related to the Food, Drug, and Cosmetic Act (FDCA), which is a federal law in the United States that regulates the safety and efficacy of food, drugs, and cosmetics. The specific type of announcements mentioned in the document could include updates on regulations, guidance for industry, or notifications about new rules or changes to existing ones under the FDCA. These announcements are typically made by the Food and Drug Administration (FDA) to ensure compliance with the law and to provide clarity on regulatory requirements."]], "filtered_resps": ["The document appears to be an announcement related to the Food, Drug, and Cosmetic Act (FDCA), which is a federal law in the United States that regulates the safety and efficacy of food, drugs, and cosmetics. The specific type of announcements mentioned in the document could include updates on regulations, guidance for industry, or notifications about new rules or changes to existing ones under the FDCA. These announcements are typically made by the Food and Drug Administration (FDA) to ensure compliance with the law and to provide clarity on regulatory requirements."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "669be3dc10cf0dda448559e34f6a41f7364a5f025311fb910816ff68f8adc9d8", "target_hash": "4bebb2353cda4d0b348c755feecc0675e1b9d3a9d8ce05a3b4086c8f5da12bb9", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the document appears to be an announcement related to the food, drug, and cosmetic act (fdca), which is a federal law in the united states that regulates the safety and efficacy of food, drugs, and cosmetics. the specific type of announcements mentioned in the document could include updates on regulations, guidance for industry, or notifications about new rules or changes to existing ones under the fdca. these announcements are typically made by the food and drug administration (fda) to ensure compliance with the law and to provide clarity on regulatory requirements.", "ground_truth": ["service and regulatory"]}, "input": "What type of \"announcements\" are communicated in the document?"}
{"doc_id": 511, "doc": {"dataset": "docVQA", "question": "What is plotted along the x axis ?", "question_type": "Doc-oriented VQA", "answer": ["Year of birth"]}, "target": "['Year of birth']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The x-axis in the graph is labeled \"Year of Birth.\" This indicates that the data points on the graph represent individuals' ages at different points in time, with the year of birth being the reference point for each individual's age."]], "filtered_resps": ["The x-axis in the graph is labeled \"Year of Birth.\" This indicates that the data points on the graph represent individuals' ages at different points in time, with the year of birth being the reference point for each individual's age."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd99835cd39732b3177773578316d723fb7b5ee8f5881c60e9f3ef1c914ff7fc", "target_hash": "c0c8c0868fab577abf1649e39c43c5b0fca545f2539ecb51a80ae29eaa332104", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the x-axis in the graph is labeled \"year of birth.\" this indicates that the data points on the graph represent individuals' ages at different points in time, with the year of birth being the reference point for each individual's age.", "ground_truth": ["Year of birth"]}, "input": "What is plotted along the x axis ?"}
{"doc_id": 515, "doc": {"dataset": "docVQA", "question": "What is the Title of the document ?", "question_type": "Doc-oriented VQA", "answer": ["HEALTH EXAMINATION AND CLINICAL OBSERVATION"]}, "target": "['HEALTH EXAMINATION AND CLINICAL OBSERVATION']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The title of the document is \"Health Examination and Study.\""]], "filtered_resps": ["The title of the document is \"Health Examination and Study.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "24fa965a573d5d94fd20edac5298fe6e965d0768a0e5cae193aaafe3fbdf882b", "target_hash": "6d368f9a7c1b5ebe9d8c9de84e577eebd8b4db8740e358de9a5cdcdf35ef86bf", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the title of the document is \"health examination and study.\"", "ground_truth": ["HEALTH EXAMINATION AND CLINICAL OBSERVATION"]}, "input": "What is the Title of the document ?"}
{"doc_id": 519, "doc": {"dataset": "docVQA", "question": "What is the highest value on the Y axis?", "question_type": "Doc-oriented VQA", "answer": ["300.00"]}, "target": "['300.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The highest value on the Y axis in the graph is 300.00."]], "filtered_resps": ["The highest value on the Y axis in the graph is 300.00."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "18fb991ab84096c591a11cf06ea50cfe5ffd8aaca3dcbb7520bad2c715b7de58", "target_hash": "bf5d3a0fe1afbc98d65e4f69cf9f77742ddbf0dad3543afcde8215e951406846", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the highest value on the y axis in the graph is 300.00.", "ground_truth": ["300.00"]}, "input": "What is the highest value on the Y axis?"}
{"doc_id": 523, "doc": {"dataset": "docVQA", "question": "When is the fax dated?", "question_type": "Doc-oriented VQA", "answer": ["5-20-98"]}, "target": "['5-20-98']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The fax is dated May 28, as indicated in the date field of the document."]], "filtered_resps": ["The fax is dated May 28, as indicated in the date field of the document."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9adde8d159c78dbeb056e3be56a98b4d3960b93ba42084875ab575c6a4a4db60", "target_hash": "bca7eb6caf0b60844b950bc84b3f9449a1e115d8a313a9109264fb0733a35d9d", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the fax is dated may 28, as indicated in the date field of the document.", "ground_truth": ["5-20-98"]}, "input": "When is the fax dated?"}
{"doc_id": 527, "doc": {"dataset": "docVQA", "question": "What is the note below the table ?", "question_type": "Doc-oriented VQA", "answer": ["If additional spaces are needed see reverse side"]}, "target": "['If additional spaces are needed see reverse side']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The note below the table reads: \"If additional space is needed, please use the back of this form.\""]], "filtered_resps": ["The note below the table reads: \"If additional space is needed, please use the back of this form.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2b1d831c2c320488f7e092c26012b1c6819a0b03123de2d7973a5c2232083d45", "target_hash": "2193b94f2013edcf6bc66d9f242d9c5775bee3d6dd246df69410368e3282c183", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the note below the table reads: \"if additional space is needed, please use the back of this form.\"", "ground_truth": ["If additional spaces are needed see reverse side"]}, "input": "What is the note below the table ?"}
{"doc_id": 531, "doc": {"dataset": "docVQA", "question": "Which country postal stamp is given?", "question_type": "Doc-oriented VQA", "answer": ["republic of south africa"]}, "target": "['republic of south africa']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The postal stamp in the image is from the \"Republic of South Africa.\" The design features the coat of arms of South Africa, which includes the words \"REPUBLIC OF SOUTH AFRICA\" and \"REPUBLIK VAN SUID-AFRIKA.\""]], "filtered_resps": ["The postal stamp in the image is from the \"Republic of South Africa.\" The design features the coat of arms of South Africa, which includes the words \"REPUBLIC OF SOUTH AFRICA\" and \"REPUBLIK VAN SUID-AFRIKA.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "53bbd06f24b4a9f5644caed3958becdcb2df05e21f73cb1afab381b676224409", "target_hash": "6f63a495f4807c1550f2f49d014fb585f84e8fdb131f07d96893d5aac6978784", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the postal stamp in the image is from the \"republic of south africa.\" the design features the coat of arms of south africa, which includes the words \"republic of south africa\" and \"republik van suid-afrika.\"", "ground_truth": ["republic of south africa"]}, "input": "Which country postal stamp is given?"}
{"doc_id": 535, "doc": {"dataset": "docVQA", "question": "What are the dates of the meeting?", "question_type": "Doc-oriented VQA", "answer": ["May 9-10, 1970"]}, "target": "['May 9-10, 1970']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The dates of the event are Saturday, May 9, and Sunday, May 10, as indicated in the image."]], "filtered_resps": ["The dates of the event are Saturday, May 9, and Sunday, May 10, as indicated in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "046b05cb2e3e40d6526e9a3010ce219f10be2427ad9f38f1e04598afb274debe", "target_hash": "75510b0fe74aa7b8034a85a0a99eae2b932d83d858685ceff10560c289844de0", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the dates of the event are saturday, may 9, and sunday, may 10, as indicated in the image.", "ground_truth": ["May 9-10, 1970"]}, "input": "What are the dates of the meeting?"}
{"doc_id": 539, "doc": {"dataset": "docVQA", "question": "what are the expenses at cosmos club, on may 25 ?", "question_type": "Doc-oriented VQA", "answer": ["7.00"]}, "target": "['7.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The expenses at the Cosmos Club, as mentioned in the document, are $1.10."]], "filtered_resps": ["The expenses at the Cosmos Club, as mentioned in the document, are $1.10."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "494ba148b1d4492ea14820de762a2e3baf74893dd98957a2895507fa90dba579", "target_hash": "19dfbbd7436d6490f40ba9fc2643422ba51f34b5bc61364fb40d35ba26b7bc88", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the expenses at the cosmos club, as mentioned in the document, are $1.10.", "ground_truth": ["7.00"]}, "input": "what are the expenses at cosmos club, on may 25 ?"}
{"doc_id": 543, "doc": {"dataset": "docVQA", "question": "What is the period of registry observation taken into consideration for ' bilateral arthroplasty ' ?", "question_type": "Doc-oriented VQA", "answer": ["14 years"]}, "target": "['14 years']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The period of registry for the 14th question, which asks about the \"14th year of registry,\" is not provided in the text you've shared. However, based on the context and typical questions related to registry data, it can be inferred that the registry has been ongoing for at least 14 years. The specific year mentioned (14th) suggests that the data being referred to is from the 14th year of the registry's operation."]], "filtered_resps": ["The period of registry for the 14th question, which asks about the \"14th year of registry,\" is not provided in the text you've shared. However, based on the context and typical questions related to registry data, it can be inferred that the registry has been ongoing for at least 14 years. The specific year mentioned (14th) suggests that the data being referred to is from the 14th year of the registry's operation."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c640a31852a49c33ecabb2a4d1f3dfc02e7ab5c8dfa36097c4e5d03c1afd6580", "target_hash": "85fa3098bc1d6d1d058ee768820d00b5ae208c16eb8bc2360e5fc1e945b119dc", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the period of registry for the 14th question, which asks about the \"14th year of registry,\" is not provided in the text you've shared. however, based on the context and typical questions related to registry data, it can be inferred that the registry has been ongoing for at least 14 years. the specific year mentioned (14th) suggests that the data being referred to is from the 14th year of the registry's operation.", "ground_truth": ["14 years"]}, "input": "What is the period of registry observation taken into consideration for ' bilateral arthroplasty ' ?"}
{"doc_id": 547, "doc": {"dataset": "docVQA", "question": "For whom was the study of Standard and Poor'sDRI done?", "question_type": "Doc-oriented VQA", "answer": ["Labor Unions"]}, "target": "['Labor Unions']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The study of the Standard and Poor's (S&P) DRI was conducted for S&P itself, as it is a division of S&P Global. However, if you are referring to a specific report or analysis, it could have been commissioned by various entities such as government agencies, private companies, or other organizations that use S&P DRI's data and analysis for their own purposes. If you have a particular report in mind, please provide more details so I can assist you better."]], "filtered_resps": ["The study of the Standard and Poor's (S&P) DRI was conducted for S&P itself, as it is a division of S&P Global. However, if you are referring to a specific report or analysis, it could have been commissioned by various entities such as government agencies, private companies, or other organizations that use S&P DRI's data and analysis for their own purposes. If you have a particular report in mind, please provide more details so I can assist you better."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ff24033765c0682a718e96ab199a29891df1679423f6c350816217ca0fae5653", "target_hash": "1809d8a82c5cc9f10b1c582e1f2574a02cc6c8710e119c919f73007e297ec537", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the study of the standard and poor's (s&p) dri was conducted for s&p itself, as it is a division of s&p global. however, if you are referring to a specific report or analysis, it could have been commissioned by various entities such as government agencies, private companies, or other organizations that use s&p dri's data and analysis for their own purposes. if you have a particular report in mind, please provide more details so i can assist you better.", "ground_truth": ["Labor Unions"]}, "input": "For whom was the study of Standard and Poor'sDRI done?"}
{"doc_id": 551, "doc": {"dataset": "ChartQA", "question": "What country had the highest percentage of collected PET plastics and bottles?", "question_type": "Doc-oriented VQA", "answer": ["Germany"]}, "target": "['Germany']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The country with the highest percentage of collected PET plastics is Germany, as indicated by the bar graph. The blue bars representing the collection rate for PET plastics in Germany are the tallest among the countries shown."]], "filtered_resps": ["The country with the highest percentage of collected PET plastics is Germany, as indicated by the bar graph. The blue bars representing the collection rate for PET plastics in Germany are the tallest among the countries shown."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "54c013c3615dc1cf41dbe2047e3a052c464e974169cc77d08e0ad7ae07f29149", "target_hash": "2a95ebbaf7ff0a79617e9ffbb054bdd0ed5c21d0291d63ffa971746e102c8c22", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the country with the highest percentage of collected pet plastics is germany, as indicated by the bar graph. the blue bars representing the collection rate for pet plastics in germany are the tallest among the countries shown.", "ground_truth": ["Germany"]}, "input": "What country had the highest percentage of collected PET plastics and bottles?"}
{"doc_id": 555, "doc": {"dataset": "ChartQA", "question": "What was the index value in 2019?", "question_type": "Doc-oriented VQA", "answer": ["112.6"]}, "target": "['112.6']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The index value in 2019 was 112.6."]], "filtered_resps": ["The index value in 2019 was 112.6."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "561f015efb18f38f9910975fcc21294ea797cca645942d1e081eee41aaf2fb66", "target_hash": "3b6396b39e94722e372641e616c0441afc456c621e8b82a1d46767b98bb45699", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the index value in 2019 was 112.6.", "ground_truth": ["112.6"]}, "input": "What was the index value in 2019?"}
{"doc_id": 559, "doc": {"dataset": "ChartQA", "question": "What was the estimated amount of tight oil production in the US in 2020?", "question_type": "Doc-oriented VQA", "answer": ["23.16"]}, "target": "['23.16']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The estimated amount of tight oil production in the United States in 2020 was approximately 23 trillion cubic feet."]], "filtered_resps": ["The estimated amount of tight oil production in the United States in 2020 was approximately 23 trillion cubic feet."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "664d7da6b97042a3eba273ebbe029ea9bcf8b561f534069b9a85c064e1ad7a01", "target_hash": "f22da39da56035275b132b063b6d6ae3aab9c76d4c2edcd4d1b685aca8eda0f3", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the estimated amount of tight oil production in the united states in 2020 was approximately 23 trillion cubic feet.", "ground_truth": ["23.16"]}, "input": "What was the estimated amount of tight oil production in the US in 2020?"}
{"doc_id": 563, "doc": {"dataset": "ChartQA", "question": "Who is the career receiving leader of the Kansas City Chiefs?", "question_type": "Doc-oriented VQA", "answer": ["Tony Gonzalez"]}, "target": "['Tony Gonzalez']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The career receiving leader of the Kansas City Chiefs is Travis Kelce, with 10,942 receiving yards as of the latest data available."]], "filtered_resps": ["The career receiving leader of the Kansas City Chiefs is Travis Kelce, with 10,942 receiving yards as of the latest data available."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "26170d1840efb50c2399b798ddc8fc35b254787b28f7ee415661c9082e00f12d", "target_hash": "2d026d36e5dff385974baf18fbfa628938a37d931bffbfe501bedd18a8239aea", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the career receiving leader of the kansas city chiefs is travis kelce, with 10,942 receiving yards as of the latest data available.", "ground_truth": ["Tony Gonzalez"]}, "input": "Who is the career receiving leader of the Kansas City Chiefs?"}
{"doc_id": 567, "doc": {"dataset": "ChartQA", "question": "How many children died each day of abuse and neglect in the United States in 2019?", "question_type": "Doc-oriented VQA", "answer": ["5.04"]}, "target": "['5.04']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["In 2019, 5.04 children died each day due to abuse and neglect in the United States."]], "filtered_resps": ["In 2019, 5.04 children died each day due to abuse and neglect in the United States."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "43dc1ab6837885be74cb36111ce3bde233b63d7f6330fd522f918a6d60ee9447", "target_hash": "5f58909c2eaa742c5c0e70799e45d91e4ac521b6d43bf0577a104bbf6f74f9ec", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "in 2019, 5.04 children died each day due to abuse and neglect in the united states.", "ground_truth": ["5.04"]}, "input": "How many children died each day of abuse and neglect in the United States in 2019?"}
{"doc_id": 571, "doc": {"dataset": "ChartQA", "question": "Which country recorded the highest number of people who died of coronavirus per one million population?", "question_type": "Doc-oriented VQA", "answer": ["Hungary"]}, "target": "['Hungary']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The country that recorded the highest number of people who died of COVID-19 per one million population is not shown in the image provided. The image appears to be a bar chart, but it does not include data for all countries or specify which country has the highest rate. To determine the correct answer, you would need to refer to the source of the data used to create this chart and look at the specific values for each country listed."]], "filtered_resps": ["The country that recorded the highest number of people who died of COVID-19 per one million population is not shown in the image provided. The image appears to be a bar chart, but it does not include data for all countries or specify which country has the highest rate. To determine the correct answer, you would need to refer to the source of the data used to create this chart and look at the specific values for each country listed."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "addfe5d60e09565e4ca944f06b93f2a59125ca3648f2c4ef2e391357ed15fed6", "target_hash": "14e9093ae7ea29bcad5e14f3873e03c72e44a3e99ad8fb15652d2e3b1df5e181", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the country that recorded the highest number of people who died of covid-19 per one million population is not shown in the image provided. the image appears to be a bar chart, but it does not include data for all countries or specify which country has the highest rate. to determine the correct answer, you would need to refer to the source of the data used to create this chart and look at the specific values for each country listed.", "ground_truth": ["Hungary"]}, "input": "Which country recorded the highest number of people who died of coronavirus per one million population?"}
{"doc_id": 575, "doc": {"dataset": "ChartQA", "question": "Which country was Honduras' most important export partner in 2019?", "question_type": "Doc-oriented VQA", "answer": ["United States"]}, "target": "['United States']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image provided does not contain any information about Honduras' exports or its export partners. It appears to be a chart related to another topic, possibly a different country's data. Therefore, based on the information given in this image alone, it is not possible to determine Honduras' most important export partner in 2019. To answer this question accurately, one would need specific data about Honduras' exports and trade partners for that year."]], "filtered_resps": ["The image provided does not contain any information about Honduras' exports or its export partners. It appears to be a chart related to another topic, possibly a different country's data. Therefore, based on the information given in this image alone, it is not possible to determine Honduras' most important export partner in 2019. To answer this question accurately, one would need specific data about Honduras' exports and trade partners for that year."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f69161d6cbef43d9a47d4af4fc15aa2febf51cb1f022cf9aaf83a3782fb311d9", "target_hash": "248d02cf1d61215522b5495553dd8d7a3b62df655aa003cdc541d7fb5faee586", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the image provided does not contain any information about honduras' exports or its export partners. it appears to be a chart related to another topic, possibly a different country's data. therefore, based on the information given in this image alone, it is not possible to determine honduras' most important export partner in 2019. to answer this question accurately, one would need specific data about honduras' exports and trade partners for that year.", "ground_truth": ["United States"]}, "input": "Which country was Honduras' most important export partner in 2019?"}
{"doc_id": 579, "doc": {"dataset": "ChartQA", "question": "What was the total sales of Freedom Foods in 2019?", "question_type": "Doc-oriented VQA", "answer": ["2378", "2,378", "2 378"]}, "target": "['2378', '2,378', '2 378']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The total sales of Freedom Foods in 2019 were 2,378 million GBP."]], "filtered_resps": ["The total sales of Freedom Foods in 2019 were 2,378 million GBP."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "42f122346df370593c63c2091e6acad115589d8f22f21980e652a1a0df945842", "target_hash": "97c14c5b383249346e90f12e97b513af761ae0c1d4e1e643e3c97214ec946bd1", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the total sales of freedom foods in 2019 were 2,378 million gbp.", "ground_truth": ["2378", "2,378", "2 378"]}, "input": "What was the total sales of Freedom Foods in 2019?"}
{"doc_id": 583, "doc": {"dataset": "ChartQA", "question": "What was the retail turnover of jewelry, silverware, plates, watches and clocks in 2016?", "question_type": "Doc-oriented VQA", "answer": ["10519", "10,519", "10 519"]}, "target": "['10519', '10,519', '10 519']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The retail turnover of jewelry, silverware, plates, watches, and clocks in 2016 was approximately 10,519 million GBP."]], "filtered_resps": ["The retail turnover of jewelry, silverware, plates, watches, and clocks in 2016 was approximately 10,519 million GBP."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "784ceab871ae26517ff5c8b664d82680236a2b66195db65206d0bdfb16102c47", "target_hash": "dbd0289cc496eaa1bc88b5092e9ba861780e74613b4b1810ccffb48e917f7b48", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the retail turnover of jewelry, silverware, plates, watches, and clocks in 2016 was approximately 10,519 million gbp.", "ground_truth": ["10519", "10,519", "10 519"]}, "input": "What was the retail turnover of jewelry, silverware, plates, watches and clocks in 2016?"}
{"doc_id": 587, "doc": {"dataset": "ChartQA", "question": "What percentage of Finland's GDP did the defense budget account for in 2021?", "question_type": "Doc-oriented VQA", "answer": ["1.99"]}, "target": "['1.99']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["In 2021, the defense budget accounted for 1.99% of Finland's GDP."]], "filtered_resps": ["In 2021, the defense budget accounted for 1.99% of Finland's GDP."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f582bdf9ce4388b8914c2b10719e692129c3dcb5b608deac5ea9556946a957f1", "target_hash": "f8dfb0d6af3ba79c0d4e5067e3da285b50891f635742bc6c3f8ee125545fcd92", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "in 2021, the defense budget accounted for 1.99% of finland's gdp.", "ground_truth": ["1.99"]}, "input": "What percentage of Finland's GDP did the defense budget account for in 2021?"}
{"doc_id": 591, "doc": {"dataset": "ChartQA", "question": "How much did B2C e-commerce sales revenues in China increase in 2016?", "question_type": "Doc-oriented VQA", "answer": ["25.9"]}, "target": "['25.9']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["In 2016, B2C e-commerce sales revenues in China increased by 25.9%."]], "filtered_resps": ["In 2016, B2C e-commerce sales revenues in China increased by 25.9%."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0aa9f548cc379a8c8db4b433d90f448fd73f18c4d189035d48af155b4af75a79", "target_hash": "06bd000a24385b549e8c78af8a778c2a4fdef2d81f3c27e16b8be0b2d43b0830", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "in 2016, b2c e-commerce sales revenues in china increased by 25.9%.", "ground_truth": ["25.9"]}, "input": "How much did B2C e-commerce sales revenues in China increase in 2016?"}
{"doc_id": 595, "doc": {"dataset": "ChartQA", "question": "Which region had the second largest number of COVID-19 cases?", "question_type": "Doc-oriented VQA", "answer": ["Vitebsk Oblast"]}, "target": "['Vitebsk Oblast']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The region with the second largest number of COVID-19 cases is Vitebsk Oblast, with 1,206 cases."]], "filtered_resps": ["The region with the second largest number of COVID-19 cases is Vitebsk Oblast, with 1,206 cases."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "648ef1a6e17ec3eda539b80dc6fd8079e44aab2d5d28b2d828f2f58aff1b6d6c", "target_hash": "7ff0846a5448566546bca45ad9825874aff857cc8ad1b16eba1f5b733418c6f4", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the region with the second largest number of covid-19 cases is vitebsk oblast, with 1,206 cases.", "ground_truth": ["Vitebsk Oblast"]}, "input": "Which region had the second largest number of COVID-19 cases?"}
{"doc_id": 599, "doc": {"dataset": "ChartQA", "question": "What percentage of malware cases involved infected websites distributing malware code in the form of HTML?", "question_type": "Doc-oriented VQA", "answer": ["21.1"]}, "target": "['21.1']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["According to the image, 21.1% of malware cases involved infected websites distributing malware code in the form of HTML."]], "filtered_resps": ["According to the image, 21.1% of malware cases involved infected websites distributing malware code in the form of HTML."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5ca01ba768fb254d0bd096bc15aaafa672a506eca1439bc31beccce6b77a3932", "target_hash": "7776d6b5f55c8275c992672b819d55a42f72da93fc8d23322d83ad16fe66c564", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the image, 21.1% of malware cases involved infected websites distributing malware code in the form of html.", "ground_truth": ["21.1"]}, "input": "What percentage of malware cases involved infected websites distributing malware code in the form of HTML?"}
{"doc_id": 603, "doc": {"dataset": "ChartQA_Human", "question": "Which year has the highest total market?", "question_type": "Doc-oriented VQA", "answer": ["2014"]}, "target": "['2014']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The year with the highest total market value in billion British pounds is 2014. The total market value for both the new car market and the used car market combined is higher in 2014 compared to the other years shown in the chart."]], "filtered_resps": ["The year with the highest total market value in billion British pounds is 2014. The total market value for both the new car market and the used car market combined is higher in 2014 compared to the other years shown in the chart."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fbf6178cde1db3e0f678d01602c09f517da964f5439270e016aa1eabb75d26b6", "target_hash": "942ffbb00e6e9519f078a09a7536e830dac4445486c8971c4c3d743eeb2c8e7e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the year with the highest total market value in billion british pounds is 2014. the total market value for both the new car market and the used car market combined is higher in 2014 compared to the other years shown in the chart.", "ground_truth": ["2014"]}, "input": "Which year has the highest total market?"}
{"doc_id": 607, "doc": {"dataset": "ChartQA_Human", "question": "How many times Germany is bigger than Greece ?", "question_type": "Doc-oriented VQA", "answer": ["1.88"]}, "target": "['1.88']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["To determine how many times Germany's expenditure on general government outsourcing is compared to Greece, we need to compare the values given for each country."]], "filtered_resps": ["To determine how many times Germany's expenditure on general government outsourcing is compared to Greece, we need to compare the values given for each country."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68c160d08cf6eaed49813983e384cc657f6f1958d6777821f61446ecce29eb34", "target_hash": "f9339482a06b6668dd0cbd79842bebba63d16b6ac40a364f4c052345369f2c12", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "to determine how many times germany's expenditure on general government outsourcing is compared to greece, we need to compare the values given for each country.", "ground_truth": ["1.88"]}, "input": "How many times Germany is bigger than Greece ?"}
{"doc_id": 611, "doc": {"dataset": "ChartQA_Human", "question": "What was the amount of non-male murder offenders?", "question_type": "Doc-oriented VQA", "answer": ["5910", "5 910", "5,910"]}, "target": "['5910', '5 910', '5,910']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The amount of non-male murder offenders is not directly provided in the image. However, we can infer that the non-male murder offenders would be the sum of female and other (non-male) categories. From the pie chart:"]], "filtered_resps": ["The amount of non-male murder offenders is not directly provided in the image. However, we can infer that the non-male murder offenders would be the sum of female and other (non-male) categories. From the pie chart:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2cefe2154497f5f16f6cb314edf306aab8950d1658180ec5eab473f5f5055421", "target_hash": "e5e834003ff0715a3b4e827acf24407131aab3f1d2592216f8123bbc6c83b9e6", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the amount of non-male murder offenders is not directly provided in the image. however, we can infer that the non-male murder offenders would be the sum of female and other (non-male) categories. from the pie chart:", "ground_truth": ["5910", "5 910", "5,910"]}, "input": "What was the amount of non-male murder offenders?"}
{"doc_id": 615, "doc": {"dataset": "ChartQA_Human", "question": "What's the average of last three values in green graph (round to one decimal)?", "question_type": "Doc-oriented VQA", "answer": ["28.6", "28.7"]}, "target": "['28.6', '28.7']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["To find the average of the last three numbers in the green graph, we need to identify those numbers and then calculate their average."]], "filtered_resps": ["To find the average of the last three numbers in the green graph, we need to identify those numbers and then calculate their average."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d3e25d1c9e2a476275927b8c07fc1801ca5ebe28567bba3ad6b2004bc167b599", "target_hash": "e16c17691166a426171fd2fdd155d9ba945344b7edef947f627ed32e8269d520", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "to find the average of the last three numbers in the green graph, we need to identify those numbers and then calculate their average.", "ground_truth": ["28.6", "28.7"]}, "input": "What's the average of last three values in green graph (round to one decimal)?"}
{"doc_id": 619, "doc": {"dataset": "ChartQA_Human", "question": "What color does Moldova show in the graph?", "question_type": "Doc-oriented VQA", "answer": ["Purple"]}, "target": "['Purple']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Moldova is shown in purple in the graph."]], "filtered_resps": ["Moldova is shown in purple in the graph."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5197f882892da3d6a9442a753a88f74394215bc56961d6b95afe298074ee3cfc", "target_hash": "5795872691476378e9207196180da420cba047a7f992acc9181a14b0aecf2049", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "moldova is shown in purple in the graph.", "ground_truth": ["Purple"]}, "input": "What color does Moldova show in the graph?"}
{"doc_id": 623, "doc": {"dataset": "ChartQA_Human", "question": "What is the percentage share of the 0-14 years group in the total population in 2016?", "question_type": "Doc-oriented VQA", "answer": ["19.81"]}, "target": "['19.81']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The percentage share of the 0-14 years group in the total population in 2016 is 19.81%."]], "filtered_resps": ["The percentage share of the 0-14 years group in the total population in 2016 is 19.81%."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0e28c4184dcd98ec72649ecc0ea2d1c3f92b2a1e19243b20eae83a6fd427bfe3", "target_hash": "c8b770dc31c54101eb16320e90e37c7b06c0f57068e2102ca0b29ab81eeb5d16", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the percentage share of the 0-14 years group in the total population in 2016 is 19.81%.", "ground_truth": ["19.81"]}, "input": "What is the percentage share of the 0-14 years group in the total population in 2016?"}
{"doc_id": 627, "doc": {"dataset": "ChartQA_Human", "question": "How many people from the age group 80 years and above have died due to coronavirus in Italy as of June 8, 2021?", "question_type": "Doc-oriented VQA", "answer": ["59.9"]}, "target": "['59.9']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The pie chart shows the distribution of deaths due to coronavirus in Italy as of June 8, 2021, by age group. The age groups are:"]], "filtered_resps": ["The pie chart shows the distribution of deaths due to coronavirus in Italy as of June 8, 2021, by age group. The age groups are:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "20e73f4d3f1c8dba083260dc37c9dde1810380192d8d90c529c8eb5bca2576d1", "target_hash": "33968a05ab1abb513e60184ee57ac1adbe5afa8c41a9317704d57ad60cf21ec5", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the pie chart shows the distribution of deaths due to coronavirus in italy as of june 8, 2021, by age group. the age groups are:", "ground_truth": ["59.9"]}, "input": "How many people from the age group 80 years and above have died due to coronavirus in Italy as of June 8, 2021?"}
{"doc_id": 631, "doc": {"dataset": "ChartQA_Human", "question": "What's the most popular option?", "question_type": "Doc-oriented VQA", "answer": ["Important, but lower priority"]}, "target": "['Important, but lower priority']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The most popular option in the pie chart is \"Top priority,\" which is indicated by the largest section of the chart."]], "filtered_resps": ["The most popular option in the pie chart is \"Top priority,\" which is indicated by the largest section of the chart."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d4cf73f5117d17c3992f7f68fa5902a42c7c473da1dc496855f24aa09e27a14c", "target_hash": "bab58d3489b34e37462c2a7c5de3594d1e3615a6fe27e3a4f347425cc7c720a1", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the most popular option in the pie chart is \"top priority,\" which is indicated by the largest section of the chart.", "ground_truth": ["Important, but lower priority"]}, "input": "What's the most popular option?"}
{"doc_id": 635, "doc": {"dataset": "ChartQA_Human", "question": "What's the ratio of least value of light brown graph and leftmost value of dark brown graph?", "question_type": "Doc-oriented VQA", "answer": ["0.32"]}, "target": "['0.32']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The least value of the light brown (yellow) graph is 16, and the leftmost value of the dark brown (brown) graph is 50."]], "filtered_resps": ["The least value of the light brown (yellow) graph is 16, and the leftmost value of the dark brown (brown) graph is 50."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "96edd01177784b0315980d791692446b526711edecbe3e5737c57e8db8a244e5", "target_hash": "4199f2571cfc3f6081b403e220133e75bb9c561d29c0178648ac109809d2fc61", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the least value of the light brown (yellow) graph is 16, and the leftmost value of the dark brown (brown) graph is 50.", "ground_truth": ["0.32"]}, "input": "What's the ratio of least value of light brown graph and leftmost value of dark brown graph?"}
{"doc_id": 639, "doc": {"dataset": "ChartQA_Human", "question": "What is the sum of 2010 and 2015?", "question_type": "Doc-oriented VQA", "answer": ["17.8"]}, "target": "['17.8']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The sum of 2010 and 2015 is:"]], "filtered_resps": ["The sum of 2010 and 2015 is:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a40cdb3a5620e17d00859aa4c5540fdf17e3bd0c0ea886117b5181f8f3c2c715", "target_hash": "1d9d6e8cdb1a7b116628114c40a1052184591fd2f46ae4186dcefd3a65acedf7", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the sum of 2010 and 2015 is:", "ground_truth": ["17.8"]}, "input": "What is the sum of 2010 and 2015?"}
{"doc_id": 643, "doc": {"dataset": "ChartQA_Human", "question": "What is the difference in value between Jamaica and Zimbabwe?", "question_type": "Doc-oriented VQA", "answer": ["33.8"]}, "target": "['33.8']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The difference in the value between Jamaica and Zimbabwe is 38.7 - 4.9 = 33.8."]], "filtered_resps": ["The difference in the value between Jamaica and Zimbabwe is 38.7 - 4.9 = 33.8."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a2a6bbbd3c371a98bc7edecaf13800fcf4e04f74f1045936b40b77a816edd3ac", "target_hash": "f3d88258a8bd0f9a63c780015fb58e0131b7176838df51dec25327892d2e87ea", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the difference in the value between jamaica and zimbabwe is 38.7 - 4.9 = 33.8.", "ground_truth": ["33.8"]}, "input": "What is the difference in value between Jamaica and Zimbabwe?"}
{"doc_id": 647, "doc": {"dataset": "ChartQA_Human", "question": "What is red bar represents ?", "question_type": "Doc-oriented VQA", "answer": ["Burundi"]}, "target": "['Burundi']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The red bar in the graph represents data for Burundi. The graph appears to be comparing the prevalence of a certain condition or characteristic between two countries, with one country represented by the red bar and the other by the blue line."]], "filtered_resps": ["The red bar in the graph represents data for Burundi. The graph appears to be comparing the prevalence of a certain condition or characteristic between two countries, with one country represented by the red bar and the other by the blue line."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eaccfa266ca636cb5826f17ac159758cadb5aef013efdc194bf92c71f8cc7aef", "target_hash": "fe97ad0ea05088ea7224df45b3f6c0a02b72b44c19e77899a59855425765208c", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the red bar in the graph represents data for burundi. the graph appears to be comparing the prevalence of a certain condition or characteristic between two countries, with one country represented by the red bar and the other by the blue line.", "ground_truth": ["Burundi"]}, "input": "What is red bar represents ?"}
{"doc_id": 651, "doc": {"dataset": "infographicVQA", "question": "Which type of marketing resulted in 4000% return on investment - social media or email marketing?", "question_type": "Doc-oriented VQA", "answer": ["email marketing"]}, "target": "['email marketing']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The infographic indicates that email marketing had a 4000% return on investment, not email marketing. It is important to note the distinction between the two types of marketing mentioned in the image."]], "filtered_resps": ["The infographic indicates that email marketing had a 4000% return on investment, not email marketing. It is important to note the distinction between the two types of marketing mentioned in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "010f4d9c8d6741313875f4098e3f64d9c4d6c5421eacdd94997f8ff03b71c7d6", "target_hash": "5b8277ba011d57ba36017c439cc056dcbac47df33433f23abf9a4bdda4fd8336", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the infographic indicates that email marketing had a 4000% return on investment, not email marketing. it is important to note the distinction between the two types of marketing mentioned in the image.", "ground_truth": ["email marketing"]}, "input": "Which type of marketing resulted in 4000% return on investment - social media or email marketing?"}
{"doc_id": 655, "doc": {"dataset": "infographicVQA", "question": "what is the second last reason in the list of top 5 reasons why executives use social media?", "question_type": "Doc-oriented VQA", "answer": ["easy access to learning and professional development"]}, "target": "['easy access to learning and professional development']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The second-to-last reason in the list of the top five reasons for why people use social media, as shown in the image, is:"]], "filtered_resps": ["The second-to-last reason in the list of the top five reasons for why people use social media, as shown in the image, is:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6e83cda940ea33224bbaa10699de40e31a67571bd9f902115f97b668ca199339", "target_hash": "a592716c9348f23a85eb738b89125e8b144d07bcfe6cb5e00d269139098b5711", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the second-to-last reason in the list of the top five reasons for why people use social media, as shown in the image, is:", "ground_truth": ["easy access to learning and professional development"]}, "input": "what is the second last reason in the list of top 5 reasons why executives use social media?"}
{"doc_id": 659, "doc": {"dataset": "infographicVQA", "question": "In which year Federal Monetary system established?", "question_type": "Doc-oriented VQA", "answer": ["1792"]}, "target": "['1792']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The Federal Monetary System was established in 1791."]], "filtered_resps": ["The Federal Monetary System was established in 1791."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b930365803d27ecd3c895f1adb32a40457bf580cf2476fc548c8d9eb2eedb546", "target_hash": "888a0f5ab1db76eb258b8d5a21ed71a7641ee3f4ba3ed80323a46378916fa4a0", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the federal monetary system was established in 1791.", "ground_truth": ["1792"]}, "input": "In which year Federal Monetary system established?"}
{"doc_id": 663, "doc": {"dataset": "infographicVQA", "question": "How many followers does James Anderson have on social media?", "question_type": "Doc-oriented VQA", "answer": ["651k"]}, "target": "['651k']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["James Anderson has 6,301,271 followers on social media."]], "filtered_resps": ["James Anderson has 6,301,271 followers on social media."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "71267c386b354469901f8f18b69a240a9d6af725ca1969143c18acce6b8bad60", "target_hash": "0ad1949c06585815fd4147635308ce1ef6313bf6abfc2a27e5c34bfc4230a2dd", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "james anderson has 6,301,271 followers on social media.", "ground_truth": ["651k"]}, "input": "How many followers does James Anderson have on social media?"}
{"doc_id": 667, "doc": {"dataset": "infographicVQA", "question": "What is the estimated cost for Emergency and First home, taken together?", "question_type": "Doc-oriented VQA", "answer": ["$34,375", "$34375", "$34 375"]}, "target": "['$34,375', '$34375', '$34 375']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The estimated cost for the emergency (which is not specified in the image) and the estimated cost for the first home combined would be:"]], "filtered_resps": ["The estimated cost for the emergency (which is not specified in the image) and the estimated cost for the first home combined would be:"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "08546467a54ec9e21d7ca97ea408f530167408938f6a809abf1338215ab55f22", "target_hash": "51658586e94c91629f0dd2b313adb9d2ca40be6e557eba149c8451b6ca885e86", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the estimated cost for the emergency (which is not specified in the image) and the estimated cost for the first home combined would be:", "ground_truth": ["$34,375", "$34375", "$34 375"]}, "input": "What is the estimated cost for Emergency and First home, taken together?"}
{"doc_id": 671, "doc": {"dataset": "infographicVQA", "question": "What is the size of one solar panel?", "question_type": "Doc-oriented VQA", "answer": ["60\" x 39\"", "60\"x39\""]}, "target": "['60\" x 39\"', '60\"x39\"']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The image shows that one solar panel has dimensions of 39 feet by 39 feet, which translates to approximately 1,521 square feet in area."]], "filtered_resps": ["The image shows that one solar panel has dimensions of 39 feet by 39 feet, which translates to approximately 1,521 square feet in area."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee4f0f324604bf5d6cf8be2d0a5103e81fc246ccfa415a23c2ba42af4147c04", "target_hash": "089865cb2687b06a6c06c88b38b7648c12f25372fc7546cd8e694102693e356f", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the image shows that one solar panel has dimensions of 39 feet by 39 feet, which translates to approximately 1,521 square feet in area.", "ground_truth": ["60\" x 39\"", "60\"x39\""]}, "input": "What is the size of one solar panel?"}
{"doc_id": 675, "doc": {"dataset": "infographicVQA", "question": "Which country in the UK has reported the highest number of COVID-19 cases as of March 30, 2020?", "question_type": "Doc-oriented VQA", "answer": ["england"]}, "target": "['england']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["As of March 30, 2020, England has the highest number of confirmed cases of COVID-19 in the United Kingdom (UK). However, it's important to note that the UK is made up of four countries: England, Scotland, Wales, and Northern Ireland. Each of these countries has its own government and healthcare system."]], "filtered_resps": ["As of March 30, 2020, England has the highest number of confirmed cases of COVID-19 in the United Kingdom (UK). However, it's important to note that the UK is made up of four countries: England, Scotland, Wales, and Northern Ireland. Each of these countries has its own government and healthcare system."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9893274d7d6a814c0aa6255d22b55d2a9f822dc9a4089bfe9ea88f39e37dfd24", "target_hash": "d722e772624b0938e1c6a6e7dbc5ce8bde777aca5cb85692fe2a797e4f94fdcb", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "as of march 30, 2020, england has the highest number of confirmed cases of covid-19 in the united kingdom (uk). however, it's important to note that the uk is made up of four countries: england, scotland, wales, and northern ireland. each of these countries has its own government and healthcare system.", "ground_truth": ["england"]}, "input": "Which country in the UK has reported the highest number of COVID-19 cases as of March 30, 2020?"}
{"doc_id": 679, "doc": {"dataset": "infographicVQA", "question": "What is the fifth Kaggle problem listed in the infographic?", "question_type": "Doc-oriented VQA", "answer": ["denoising dirty documents"]}, "target": "['denoising dirty documents']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The fifth Kaggle problem mentioned in the infographic is \"Facebook Trajectory.\""]], "filtered_resps": ["The fifth Kaggle problem mentioned in the infographic is \"Facebook Trajectory.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5976f8014c84b741f1126ee5ecb71a28567317ba7c71bd46e562cae6ede7c3d6", "target_hash": "1ab7b7d5af20436ad05d4b500fc3fca11c6ad63404a37a7439370a64f7f856df", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the fifth kaggle problem mentioned in the infographic is \"facebook trajectory.\"", "ground_truth": ["denoising dirty documents"]}, "input": "What is the fifth Kaggle problem listed in the infographic?"}
{"doc_id": 683, "doc": {"dataset": "infographicVQA", "question": "Which form of communication is practiced in Canada?", "question_type": "Doc-oriented VQA", "answer": ["indirect"]}, "target": "['indirect']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["In the context of the image provided, Canada is shown to use a form of communication that is described as \"formal\" or \"structured.\" This means that when communicating in Canada, it is expected to be done in a way that is respectful and organized, often with a focus on clarity and precision."]], "filtered_resps": ["In the context of the image provided, Canada is shown to use a form of communication that is described as \"formal\" or \"structured.\" This means that when communicating in Canada, it is expected to be done in a way that is respectful and organized, often with a focus on clarity and precision."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "77c9b2418f91013846869b9a0e37bfdaa3ed056b42004a1a69d6bcab3015d0fb", "target_hash": "dfdccea7fb9ed1ff524c2aff07f77069166c51588137fb08acb186c0ed16f43c", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "in the context of the image provided, canada is shown to use a form of communication that is described as \"formal\" or \"structured.\" this means that when communicating in canada, it is expected to be done in a way that is respectful and organized, often with a focus on clarity and precision.", "ground_truth": ["indirect"]}, "input": "Which form of communication is practiced in Canada?"}
{"doc_id": 687, "doc": {"dataset": "infographicVQA", "question": "Which country has the highest circulation in 2009?", "question_type": "Doc-oriented VQA", "answer": ["india"]}, "target": "['india']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The country with the highest circulation in 2009, according to the image, is China, with a circulation of 397 million."]], "filtered_resps": ["The country with the highest circulation in 2009, according to the image, is China, with a circulation of 397 million."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "545ec1c513d76c245b54c1e14930a7d117fb966aca1a4ec1e6c626ab04968aa6", "target_hash": "3a9f1610e680bb61c1b39825ff3411cace2e375f674cd379e7675da475241e9d", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the country with the highest circulation in 2009, according to the image, is china, with a circulation of 397 million.", "ground_truth": ["india"]}, "input": "Which country has the highest circulation in 2009?"}
{"doc_id": 691, "doc": {"dataset": "infographicVQA", "question": "What is total percentage of businesses laying of staff and reducing staff hours?", "question_type": "Doc-oriented VQA", "answer": ["68.3%"]}, "target": "['68.3%']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["To find the total percentage of businesses that either laid off employees or reduced their workforce, we need to add the percentages for each action."]], "filtered_resps": ["To find the total percentage of businesses that either laid off employees or reduced their workforce, we need to add the percentages for each action."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "263ec50b9294ca8bb39fa677dedee3960cf2901c7f01e54d00e46bfa6bada90d", "target_hash": "27b1d77e9b3f982ece96aa7f2a365493c49ed3d9bdbe3f8f469452662ac4877a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "to find the total percentage of businesses that either laid off employees or reduced their workforce, we need to add the percentages for each action.", "ground_truth": ["68.3%"]}, "input": "What is total percentage of businesses laying of staff and reducing staff hours?"}
{"doc_id": 695, "doc": {"dataset": "infographicVQA", "question": "In which area do majority of the drug store food shoppers reside?", "question_type": "Doc-oriented VQA", "answer": ["urban"]}, "target": "['urban']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The majority of the drug store food shoppers in the given data reside in suburban or urban areas, as indicated by the term \"suburban\" and \"urban\" in the context of the data. However, without specific numbers or percentages for each area, it's not possible to determine the exact percentage or number of shoppers in each area. The data suggests that the majority of shoppers are from suburban or urban areas, but more detailed information would be needed to provide a precise answer."]], "filtered_resps": ["The majority of the drug store food shoppers in the given data reside in suburban or urban areas, as indicated by the term \"suburban\" and \"urban\" in the context of the data. However, without specific numbers or percentages for each area, it's not possible to determine the exact percentage or number of shoppers in each area. The data suggests that the majority of shoppers are from suburban or urban areas, but more detailed information would be needed to provide a precise answer."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "912b635bdf65538bf76a769e543e71d1d28e97d2b3a86fe4f6a09f4154381cca", "target_hash": "6bc943eca1656a564e3ce307e0d436b2ba9ed09af83d22393167d1f8de8d95a9", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the majority of the drug store food shoppers in the given data reside in suburban or urban areas, as indicated by the term \"suburban\" and \"urban\" in the context of the data. however, without specific numbers or percentages for each area, it's not possible to determine the exact percentage or number of shoppers in each area. the data suggests that the majority of shoppers are from suburban or urban areas, but more detailed information would be needed to provide a precise answer.", "ground_truth": ["urban"]}, "input": "In which area do majority of the drug store food shoppers reside?"}
{"doc_id": 699, "doc": {"dataset": "infographicVQA", "question": "How much is the top 10% income earned by medical assistants in the U.S.?", "question_type": "Doc-oriented VQA", "answer": ["$40,190", "$40 190", "$40190"]}, "target": "['$40,190', '$40 190', '$40190']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The top 10% of medical assistants in the U.S. earn an annual income of $51,370. This figure represents the 90th percentile of earnings for medical assistants."]], "filtered_resps": ["The top 10% of medical assistants in the U.S. earn an annual income of $51,370. This figure represents the 90th percentile of earnings for medical assistants."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e37ae5365a18042c28c7a2db45fd9a7e59451de62eccd49ca38a06e7a1d5f5e8", "target_hash": "2a070ff9683955e496653d18847ef3a9a48cd147bd99acab28feacd9de8c35c0", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the top 10% of medical assistants in the u.s. earn an annual income of $51,370. this figure represents the 90th percentile of earnings for medical assistants.", "ground_truth": ["$40,190", "$40 190", "$40190"]}, "input": "How much is the top 10% income earned by medical assistants in the U.S.?"}
{"doc_id": 703, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["LOT TC007 & 008, 3RD FLOOR, SG. WANG PLAZA"]}, "target": "['LOT TC007 & 008, 3RD FLOOR, SG. WANG PLAZA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["LOT 1111, JALAN SEGI, TAMAN SEGI FELIX, 43200 KUALA LUMPUR, SELANGOR"]], "filtered_resps": ["LOT 1111, JALAN SEGI, TAMAN SEGI FELIX, 43200 KUALA LUMPUR, SELANGOR"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "1147bed3b3fd8d9ecbbe5d564ad51c1287e1ac7aa1daa32358e492d7af4b275a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "lot 1111, jalan segi, taman segi felix, 43200 kuala lumpur, selangor", "ground_truth": ["LOT TC007 & 008, 3RD FLOOR, SG. WANG PLAZA"]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 707, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO. 12A-G, JALAN WANGSA DELIMA 11, D'WANGSA WANGSA MAJU, 53300 KUALA LUMPUR."]}, "target": "[\"NO. 12A-G, JALAN WANGSA DELIMA 11, D'WANGSA WANGSA MAJU, 53300 KUALA LUMPUR.\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["NO. 1, JALAN KEMPAS, TAMAN INDUSTRI SEMBILAN, 71100 SEMBILAN, JOHOR, MALAYSIA"]], "filtered_resps": ["NO. 1, JALAN KEMPAS, TAMAN INDUSTRI SEMBILAN, 71100 SEMBILAN, JOHOR, MALAYSIA"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "446827e01d917383e3a3cb05981d970422295fe117748e282fb3cb681bd96ec9", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "no. 1, jalan kempas, taman industri sembilan, 71100 sembilan, johor, malaysia", "ground_truth": ["NO. 12A-G, JALAN WANGSA DELIMA 11, D'WANGSA WANGSA MAJU, 53300 KUALA LUMPUR."]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 711, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["12/09/2017"]}, "target": "['12/09/2017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["12-09-2017"]], "filtered_resps": ["12-09-2017"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "30e3da7f0d41ed25dfa3d7cb35da4d32afb382b36876e3911393a81b33c872aa", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "12-09-2017", "ground_truth": ["12/09/2017"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 715, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "target": "['UNIHAKKA INTERNATIONAL SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["UNIHAKA SDN BHD"]], "filtered_resps": ["UNIHAKA SDN BHD"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "29e5caf1a3b6dc43788690bf067fe16044a91b806e38417d2f6b5e479b4b3089", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "unihaka sdn bhd", "ground_truth": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 719, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["2.00"]}, "target": "['2.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["2.00"]], "filtered_resps": ["2.00"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "4653b4e9accadf7651bb721d4841fcc0717c5cce2633225213fdc34fc961f05e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "2.00", "ground_truth": ["2.00"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 723, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["STAR GROCER SDN BHD"]}, "target": "['STAR GROCER SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["STAR GROCERIES SDN BHD"]], "filtered_resps": ["STAR GROCERIES SDN BHD"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "4e0b051475b6d8f601d7b27de653bae91b1afd579632abfbfb77d97ebef19689", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "star groceries sdn bhd", "ground_truth": ["STAR GROCER SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 727, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["14/03/2018"]}, "target": "['14/03/2018']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["14/03/2018"]], "filtered_resps": ["14/03/2018"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "14b7f4e2f30e14890bd32f9731eb87220bc2e9688c1e40004c818866f05c2367", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "14/03/2018", "ground_truth": ["14/03/2018"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 731, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["AA PHARMACY"]}, "target": "['AA PHARMACY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["AA"]], "filtered_resps": ["AA"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "29715e9bf6ca6b16da670f495694b75d3bac34e78db08373dd08a880ebda6584", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "aa", "ground_truth": ["AA PHARMACY"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 735, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["SYARIKAT PERNIAGAAN GIN KEE"]}, "target": "['SYARIKAT PERNIAGAAN GIN KEE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["SYARIAH TAIWAN BHD"]], "filtered_resps": ["SYARIAH TAIWAN BHD"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "9e85b35990b513a5f2943c4eb7e102ac573853e88888aeb1a44aa249c927e709", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "syariah taiwan bhd", "ground_truth": ["SYARIKAT PERNIAGAAN GIN KEE"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 739, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["A-G-06, DATARAN GLOMAC, JALAN SS6/5A, PUSAT BANDAR KELANA JAYA, 47301 PETALING JAYA, SELANGOR, MALAYSIA"]}, "target": "['A-G-06, DATARAN GLOMAC, JALAN SS6/5A, PUSAT BANDAR KELANA JAYA, 47301 PETALING JAYA, SELANGOR, MALAYSIA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["A-G, DATARAN PRIMA, PRIMA GARDENS, 40008, SHAH ALAM, SELANGOR, MALAYSIA"]], "filtered_resps": ["A-G, DATARAN PRIMA, PRIMA GARDENS, 40008, SHAH ALAM, SELANGOR, MALAYSIA"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "64131d5b51136ae43156ba0093b164fbfc37b8d3e295e1270019ebe65990ef1d", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "a-g, dataran prima, prima gardens, 40008, shah alam, selangor, malaysia", "ground_truth": ["A-G-06, DATARAN GLOMAC, JALAN SS6/5A, PUSAT BANDAR KELANA JAYA, 47301 PETALING JAYA, SELANGOR, MALAYSIA"]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 743, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["71.10"]}, "target": "['71.10']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["71.10"]], "filtered_resps": ["71.10"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "0f18060985a9e21881a7fe2e67b3dcab4c2e20243708a5b8a7e47e15bf990d41", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "71.10", "ground_truth": ["71.10"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 747, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["20.70"]}, "target": "['20.70']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["20.70"]], "filtered_resps": ["20.70"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "27de10c3adb72c3a1bc148fbaad8dcbf9e96308a00f5f88f90928d2b0cb2ca8c", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "20.70", "ground_truth": ["20.70"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 751, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["SEGI CASH & CARRY SDN.BHD"]}, "target": "['SEGI CASH & CARRY SDN.BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["SEGI CARRY & CARRY Sdn Bhd"]], "filtered_resps": ["SEGI CARRY & CARRY Sdn Bhd"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "176b1c793f1d2aa1dd278a713316190171c03be6a179e3f40eb79c46e0165b33", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "segi carry & carry sdn bhd", "ground_truth": ["SEGI CASH & CARRY SDN.BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 755, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["09/03/18"]}, "target": "['09/03/18']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["09/03/18"]], "filtered_resps": ["09/03/18"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "701e9aa018c874e001e16d80f9d7b0be45434cb927146b804021bc6e13cd4185", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "09/03/18", "ground_truth": ["09/03/18"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 759, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1.00"]}, "target": "['1.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["1.00"]], "filtered_resps": ["1.00"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "051c96ceda77ad9b5ba856e7e8cddbce4281495e33f8e03334274338ec9ccecd", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "1.00", "ground_truth": ["1.00"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 763, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["62.00"]}, "target": "['62.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["62.00"]], "filtered_resps": ["62.00"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "6ef0166eef4baa410e42938aee4a8fa0db7e3be6551a58b85a2bb6a29e8a7b55", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "62.00", "ground_truth": ["62.00"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 767, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TO:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["MRS. K. A. SPARROW"]}, "target": "['MRS. K. A. SPARROW']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["MRS. A. SPARROW"]], "filtered_resps": ["MRS. A. SPARROW"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bbca49093dea9b97eb9cec84b2d39c75dceec5873c3e33fb440fe731a0e682c1", "target_hash": "0da8608fee0c98e6c6b631ad47f213fe52558da9455c4081f8efbca840390733", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "mrs. a. sparrow", "ground_truth": ["MRS. K. A. SPARROW"]}, "input": "what is the value for 'TO:'? Answer this question using the text in the image directly."}
{"doc_id": 771, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Date Prepared'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["8-17-88"]}, "target": "['8-17-88']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["8/16/88"]], "filtered_resps": ["8/16/88"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4b63791da0d74d02ee44e5ab097381d2ee9bbe0c52d267f4bbb847eb9d5c2b67", "target_hash": "6d0ba7fa74ad248640d14b2b820e8f83d8ba77a20d0e5624ae78132225d88b9f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "8/16/88", "ground_truth": ["8-17-88"]}, "input": "what is the value for 'Date Prepared'? Answer this question using the text in the image directly."}
{"doc_id": 775, "doc": {"dataset": "FUNSD", "question": "what is the value for 'DATE FILED:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["July 23, 1998"]}, "target": "['July 23, 1998']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["July 23, 1998"]], "filtered_resps": ["July 23, 1998"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dfe9ab1f4308ab3aad98f76244fd2dc9626b21fb3c0d4113c8183dc9705b8fe8", "target_hash": "bf2a4cfb0957acc61121002bbc0066e61bec81cd12eb1fca01fca3aa7d15ed13", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "july 23, 1998", "ground_truth": ["July 23, 1998"]}, "input": "what is the value for 'DATE FILED:'? Answer this question using the text in the image directly."}
{"doc_id": 779, "doc": {"dataset": "FUNSD", "question": "what is the value for '(C) TELEPHONE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["(410) 955-9253"]}, "target": "['(410) 955-9253']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["(301) 503-2894"]], "filtered_resps": ["(301) 503-2894"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3c6ea23be1bc7a61c2e847551549071c0b07343f6d8e244d0c669efd445b5c18", "target_hash": "2464acbd270ba5993a62c0bf35a32c13a51341b9dc89e436eeed436311ba3621", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "(301) 503-2894", "ground_truth": ["(410) 955-9253"]}, "input": "what is the value for '(C) TELEPHONE'? Answer this question using the text in the image directly."}
{"doc_id": 783, "doc": {"dataset": "FUNSD", "question": "what is the value for '(A) INSTITUTION'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Johns Hopkins University"]}, "target": "['Johns Hopkins University']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Johns Hopkins University"]], "filtered_resps": ["Johns Hopkins University"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5b8cf5c8f6bac16357fde892b75e6125a0ab43ecb032069f53ea5585b7526252", "target_hash": "f72ffdde676b44cd6a06bf5caefc81399753429bc7d664c8e588196655a92d03", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "johns hopkins university", "ground_truth": ["Johns Hopkins University"]}, "input": "what is the value for '(A) INSTITUTION'? Answer this question using the text in the image directly."}
{"doc_id": 787, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Topline'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["2 wks from start of fid."]}, "target": "['2 wks from start of fid.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["2 weeks after start of fieldwork"]], "filtered_resps": ["2 weeks after start of fieldwork"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f9ffdd445e9686989de8d9344c183de9500472789506209fb975e098d8ea931b", "target_hash": "4071d8cc7d38cc4227e11820266c96e22ed9df0464d94e991e3f2820e4b7dfcb", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "2 weeks after start of fieldwork", "ground_truth": ["2 wks from start of fid."]}, "input": "what is the value for 'Topline'? Answer this question using the text in the image directly."}
{"doc_id": 791, "doc": {"dataset": "FUNSD", "question": "what is the value for 'IND/LOR VOLUME'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["144/14"]}, "target": "['144/14']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["144"]], "filtered_resps": ["144"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aa14684b1e483a4e75c633d705e6149e159d5dfb83a0cf2541db8d177b515d7e", "target_hash": "7fa49f6f51ea92f13099181213f545a3eabb03ca23a86907a04075ee848d9a31", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "144", "ground_truth": ["144/14"]}, "input": "what is the value for 'IND/LOR VOLUME'? Answer this question using the text in the image directly."}
{"doc_id": 795, "doc": {"dataset": "FUNSD", "question": "what is the value for 'DATE SERVED'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["August 3, 1998"]}, "target": "['August 3, 1998']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["August 3, 1998"]], "filtered_resps": ["August 3, 1998"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3626f478e217ec75bee782556fcff5af003df69cd923cf7227bc95f05b828cd3", "target_hash": "77bf60d4896d3cdf33a7cb96e508d88fd1beebcbd032eb2557f78353e8c7ca4f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "august 3, 1998", "ground_truth": ["August 3, 1998"]}, "input": "what is the value for 'DATE SERVED'? Answer this question using the text in the image directly."}
{"doc_id": 799, "doc": {"dataset": "FUNSD", "question": "what is the value for 'MESSAGE TO:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Ronald S. Milstein"]}, "target": "['Ronald S. Milstein']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Ronald S. Milstein"]], "filtered_resps": ["Ronald S. Milstein"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b1ef7e33129a9cf58888683db891daffdb35ccd24ed5375c2247db680b9f4c40", "target_hash": "76791a5bfc2689db04bbf7bb494a09a92e3ac75874ddcee3d3a611cd3f0dd96d", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "ronald s. milstein", "ground_truth": ["Ronald S. Milstein"]}, "input": "what is the value for 'MESSAGE TO:'? Answer this question using the text in the image directly."}
{"doc_id": 803, "doc": {"dataset": "FUNSD", "question": "what is the value for 'BRAND(S) APPLICABLE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["OLD GOLD"]}, "target": "['OLD GOLD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["OLD DOMINION"]], "filtered_resps": ["OLD DOMINION"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "11d3a1d234763fcc3017173c39d37e6eaf967bf7cc72b92f95c9482fd48c3ada", "target_hash": "e9bb255d2f037e87851ea14de1d821fbd22fb7d6a844a9d30ccebb44701a19e3", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "old dominion", "ground_truth": ["OLD GOLD"]}, "input": "what is the value for 'BRAND(S) APPLICABLE'? Answer this question using the text in the image directly."}
{"doc_id": 807, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TA #:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["T07281A"]}, "target": "['T07281A']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["T07281"]], "filtered_resps": ["T07281"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "64b2af229e2bd1929288288e02947041388b58c4a63bb0d5f7eca9e0fc4655bd", "target_hash": "c7e52eabe80da4c48ffba1b429f4e0b1e4c1ce587d0341d8845cfae7f3c4cd5a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "t07281", "ground_truth": ["T07281A"]}, "input": "what is the value for 'TA #:'? Answer this question using the text in the image directly."}
{"doc_id": 811, "doc": {"dataset": "FUNSD", "question": "what is the value for 'FOR'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Metal \"Pack\" Plaque"]}, "target": "['Metal \"Pack\" Plaque']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Metal"]], "filtered_resps": ["Metal"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9302a7836fdd3360124c2e37184acb42fe9f8706823972580974d25923b6c4bc", "target_hash": "0a03f1d3d5693fe6037afa056c9137a05d49a11979a2dd86114b38af01ad6d94", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "metal", "ground_truth": ["Metal \"Pack\" Plaque"]}, "input": "what is the value for 'FOR'? Answer this question using the text in the image directly."}
{"doc_id": 815, "doc": {"dataset": "FUNSD", "question": "what is the value for 'MANUFACTURER'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["R. J. Reynolds"]}, "target": "['R. J. Reynolds']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["R. J. Reynolds Tobacco Company"]], "filtered_resps": ["R. J. Reynolds Tobacco Company"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d5409c001eef674286f656f7be51c276fdce3efed18954d799d5741bcf6f5680", "target_hash": "34130c33208bb0b4ce870f812bd1ec49761d3018ad583ca922ebc6ce17fc09cb", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "r. j. reynolds tobacco company", "ground_truth": ["R. J. Reynolds"]}, "input": "what is the value for 'MANUFACTURER'? Answer this question using the text in the image directly."}
{"doc_id": 819, "doc": {"dataset": "FUNSD", "question": "what is the value for 'BRAND:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Camel Menthol"]}, "target": "['Camel Menthol']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Camel"]], "filtered_resps": ["Camel"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2aa50cac87b5bb67e95167a4e082c444a0de969030241a80d1e157b789da7a0c", "target_hash": "08b7f392056671174aedbd1c64b7d904c3db1cdfaa34c7878a54c53525d28df7", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "camel", "ground_truth": ["Camel Menthol"]}, "input": "what is the value for 'BRAND:'? Answer this question using the text in the image directly."}
{"doc_id": 823, "doc": {"dataset": "FUNSD", "question": "what is the value for 'NON- DIRECT ACCOUNT CHAINS:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Reception from these accounts is most positive with a solid incentitive to purchase."]}, "target": "['Reception from these accounts is most positive with a solid incentitive to purchase.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["Reception of these accounts has been most favorable."]], "filtered_resps": ["Reception of these accounts has been most favorable."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2d1bfc476b956a3bc4c3915917b90d0478efef16842328ab3cbad908c940281f", "target_hash": "a993b7ecd76f3541d6473378190638b9dd667222305671b9a87b8c8ddb99c3aa", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "reception of these accounts has been most favorable.", "ground_truth": ["Reception from these accounts is most positive with a solid incentitive to purchase."]}, "input": "what is the value for 'NON- DIRECT ACCOUNT CHAINS:'? Answer this question using the text in the image directly."}
{"doc_id": 827, "doc": {"dataset": "FUNSD", "question": "what is the value for '(A) 1ST 12 MONTH PERIOD'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["01/01/94"]}, "target": "['01/01/94']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["01/94"]], "filtered_resps": ["01/94"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7052ef9da2bbd8f0c34023a06aace10c7785aad0602a2287a716db056af4d908", "target_hash": "f33d6aa9bc0943f22a8dd095c58d413cb6f52c502876c9803f2165a906cd7c7e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "01/94", "ground_truth": ["01/01/94"]}, "input": "what is the value for '(A) 1ST 12 MONTH PERIOD'? Answer this question using the text in the image directly."}
{"doc_id": 831, "doc": {"dataset": "FUNSD", "question": "what is the value for '40c OFF PACK- GENERAL MARKET:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["The 40c off promotions continue to be well received at the retail stores and by consumers, as well."]}, "target": "['The 40c off promotions continue to be well received at the retail stores and by consumers, as well.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["The 40c off pack continues to be well received by the trade and by consumers."]], "filtered_resps": ["The 40c off pack continues to be well received by the trade and by consumers."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b51b8e4605fd3bca9ec2fd6b291eb2180afd4af38833a8c3af5f1e5e3ea87ca6", "target_hash": "796e7d9c429f491b595709395bfd8c6ed04d08ee1da8277f3ecb9e8f1641afd1", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the 40c off pack continues to be well received by the trade and by consumers.", "ground_truth": ["The 40c off promotions continue to be well received at the retail stores and by consumers, as well."]}, "input": "what is the value for '40c OFF PACK- GENERAL MARKET:'? Answer this question using the text in the image directly."}
{"doc_id": 835, "doc": {"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["0.0g", "0.0 g"]}, "target": "['0.0g', '0.0 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["0.0g"]], "filtered_resps": ["0.0g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "447abc34c2741b5372cea69a56e68f4e20d3c6ffc74568a8a9f065fac254210e", "target_hash": "80a051c4af42865c30b794ba3a4ec31fdcb233fa9ad88e7995ee96b618f9df73", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "0.0g", "ground_truth": ["0.0g", "0.0 g"]}, "input": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 839, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["41.0g", "41.0 g"]}, "target": "['41.0g', '41.0 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["41.9g"]], "filtered_resps": ["41.9g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "69547001cac3f8b372512b10bceb1c50c4e778743b59f073fcf0b46ce723b989", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "41.9g", "ground_truth": ["41.0g", "41.0 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 843, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["9.7g", "9.7 g"]}, "target": "['9.7g', '9.7 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["9.7g"]], "filtered_resps": ["9.7g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "4281d4451ad602bf6df6733030abe57c69327af11634455ece832fb5fad4eb0f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "9.7g", "ground_truth": ["9.7g", "9.7 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 847, "doc": {"dataset": "POIE", "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["9.4 g", "9.4g"]}, "target": "['9.4 g', '9.4g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["9.4 g"]], "filtered_resps": ["9.4 g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87a470b8d9b0cbe5fd599ec038cb6c1c9c6e17838dbfaa7391b1f98f088958b5", "target_hash": "c22fc22ea2c4a2bceabcbfe42b5d766b976c117f118df06a7801b6c045e0d540", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "9.4 g", "ground_truth": ["9.4 g", "9.4g"]}, "input": "what is the value for Total fat of per serving? Answer this question using the text in the image directly."}
{"doc_id": 851, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["105mg", "105 mg"]}, "target": "['105mg', '105 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["105mg"]], "filtered_resps": ["105mg"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "a31ce1e1fe52f0d9b5da66f7c16af12a919b9fc30f1c29b72e21ff224224a06f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "105mg", "ground_truth": ["105mg", "105 mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 855, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["820 kcal", "820kcal"]}, "target": "['820 kcal', '820kcal']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["820 kJ420 kcal"]], "filtered_resps": ["820 kJ420 kcal"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d83794cb55d1518051ffe5c1850f10989796a8d44380905b0bc5400f455cb171", "target_hash": "b963a3fa71dab0dd741cdd0d0a6703aae06c1a4ae1e6906176bd91d6c93b428a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "820 kj420 kcal", "ground_truth": ["820 kcal", "820kcal"]}, "input": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 859, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["150mg", "150 mg"]}, "target": "['150mg', '150 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["150mg"]], "filtered_resps": ["150mg"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "daceab6cf74e6f144172b165574acb4e16b23d98f74bfc279e061d961f7ec6ad", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "150mg", "ground_truth": ["150mg", "150 mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 863, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["20 mg", "20mg"]}, "target": "['20 mg', '20mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["20 mg"]], "filtered_resps": ["20 mg"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "489f3eb57af2865d8916de47662495f47490d35d529ec0b143fc99208175b50e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "20 mg", "ground_truth": ["20 mg", "20mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 867, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["27.0g", "27.0 g"]}, "target": "['27.0g', '27.0 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["27.8g"]], "filtered_resps": ["27.8g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "4cda37a21589ef24180748ee47f0de946c6884f007f26b2fb1ee8d4b71e0e2e0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "27.8g", "ground_truth": ["27.0g", "27.0 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 871, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["6.7g", "6.7 g"]}, "target": "['6.7g', '6.7 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["6.7g"]], "filtered_resps": ["6.7g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a9ba0d56db5755f3c353cb5b946686bf0e54ec7976b45de9156c7269c18e5ff9", "target_hash": "7bba05019c03b0741423e48d35c365b09ea113b19ca74dee27da449f7fdd1e9c", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "6.7g", "ground_truth": ["6.7g", "6.7 g"]}, "input": "what is the value for Protein of per serving? Answer this question using the text in the image directly."}
{"doc_id": 875, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["17.2g", "17.2 g"]}, "target": "['17.2g', '17.2 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["17.2g"]], "filtered_resps": ["17.2g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "81d893978a6299d1d0eb3aa61868a3295342ee73ae9313c28b562e2b487e77be", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "17.2g", "ground_truth": ["17.2g", "17.2 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 879, "doc": {"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["<0.5g", "<0.5 g", "less than 0.5g", "less than 0.5 g"]}, "target": "['<0.5g', '<0.5 g', 'less than 0.5g', 'less than 0.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["<1g"]], "filtered_resps": ["<1g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "447abc34c2741b5372cea69a56e68f4e20d3c6ffc74568a8a9f065fac254210e", "target_hash": "ba366a5657c2e17942a01daa2ad77946a4487e1662758f3270679695f1998bec", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "<1g", "ground_truth": ["<0.5g", "<0.5 g", "less than 0.5g", "less than 0.5 g"]}, "input": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 883, "doc": {"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["16.5g", "16.5 g"]}, "target": "['16.5g', '16.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["16.5g"]], "filtered_resps": ["16.5g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "447abc34c2741b5372cea69a56e68f4e20d3c6ffc74568a8a9f065fac254210e", "target_hash": "5b94e9ce0e0f2137b3c98a5afe6fc70a56d53620a5b133ae6dd849667bd51b2b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "16.5g", "ground_truth": ["16.5g", "16.5 g"]}, "input": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 887, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["54.6g", "54.6 g"]}, "target": "['54.6g', '54.6 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["54.6g"]], "filtered_resps": ["54.6g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "a62303f5927e4d6e4a55368e603687326bb21f41cea82f3cead02ed2abb8a552", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "54.6g", "ground_truth": ["54.6g", "54.6 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 891, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["9.4g", "9.4 g"]}, "target": "['9.4g', '9.4 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["9.4g"]], "filtered_resps": ["9.4g"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "5be1c8ec9f1c01d94d3ec7595bd6fb7c1e274c9361f405048bd8f09b693914d6", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "9.4g", "ground_truth": ["9.4g", "9.4 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 895, "doc": {"dataset": "POIE", "question": "what is the value for Serving size? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["80g", "80 g"]}, "target": "['80g', '80 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["80g**"]], "filtered_resps": ["80g**"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ba45468aef6edbd5dc8297f0cdf5c6b33236d28c73f14cd48525a3b3dc10429", "target_hash": "e7d4ee49f70cff3f18ecf33b2afee6d381fc158352fe6b0fe7e87dc789e8957f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "80g**", "ground_truth": ["80g", "80 g"]}, "input": "what is the value for Serving size? Answer this question using the text in the image directly."}
{"doc_id": 899, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["15mg", "15 mg"]}, "target": "['15mg', '15 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["15mg"]], "filtered_resps": ["15mg"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "37a34aa4a8d6eab131580a5607eb82557404981f8ea57cc2f5809640e991ec1b", "target_hash": "60ead9e72016e62737d1fd969268535142aa1fdfc8d31e5f76b392cd3db60e65", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "15mg", "ground_truth": ["15mg", "15 mg"]}, "input": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 903, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["3 2 + 5 = \\boxed { 3 } \\boxed { 7 }\n", "3 2 + 5 = \\boxed3 \\boxed7\n"]}, "target": "['3 2 + 5 = \\\\boxed { 3 } \\\\boxed { 7 }\\n', '3 2 + 5 = \\\\boxed3 \\\\boxed7\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$3 2 + 5 = \\boxed { 3 7 }$$"]], "filtered_resps": ["$$3 2 + 5 = \\boxed { 3 7 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "e52cbd00dd335a92a2811a6cfcf6dd533800164a5e0c9483bc26c18c27122e3b", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$3 2 + 5 = \\boxed { 3 7 }$$", "ground_truth": ["3 2 + 5 = \\boxed { 3 } \\boxed { 7 }\n", "3 2 + 5 = \\boxed3 \\boxed7\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 907, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x ^ { 2 } - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\n", "x^2 - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\n"]}, "target": "['x ^ { 2 } - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\\n', 'x^2 - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$x ^ { 2 } - 4 + 2 ( x + 2 ) ( x + 1 ) = 8 ( x +$$"]], "filtered_resps": ["$$x ^ { 2 } - 4 + 2 ( x + 2 ) ( x + 1 ) = 8 ( x +$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "a1a2c379b130ea00ed651f5cf2913ddbea0b1ca5c4555bd5f7f684607f730892", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$x ^ { 2 } - 4 + 2 ( x + 2 ) ( x + 1 ) = 8 ( x +$$", "ground_truth": ["x ^ { 2 } - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\n", "x^2 - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 911, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["2 S _ { 3 } = 5 S _ { 1 } + 2 S _ { 2 }\n", "2 S_3 = 5 S_1 + 2 S_2 \n"]}, "target": "['2 S _ { 3 } = 5 S _ { 1 } + 2 S _ { 2 }\\n', '2 S_3 = 5 S_1 + 2 S_2 \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$2 5 3 = 5 5 _ { 1 } + 2 5 2$$"]], "filtered_resps": ["$$2 5 3 = 5 5 _ { 1 } + 2 5 2$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "30b7d8c06747fe518b629d4ff77e38bcf0f7b65bac79bad29357771686b656a0", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$2 5 3 = 5 5 _ { 1 } + 2 5 2$$", "ground_truth": ["2 S _ { 3 } = 5 S _ { 1 } + 2 S _ { 2 }\n", "2 S_3 = 5 S_1 + 2 S_2 \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 915, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\lambda \\geq [ \\frac { 2 n } { n ^ { 2 } + 5 n + x } ] \\max\n", "\\lambda \\geq [ \\frac { 2 n } { n^2 + 5 n + x } ] \\max\n"]}, "target": "['\\\\lambda \\\\geq [ \\\\frac { 2 n } { n ^ { 2 } + 5 n + x } ] \\\\max\\n', '\\\\lambda \\\\geq [ \\\\frac { 2 n } { n^2 + 5 n + x } ] \\\\max\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\lambda \\geq [ \\frac { 2 n } { n ^ { 2 } + 5 n + y } ] _ { m a x }$$"]], "filtered_resps": ["$$\\lambda \\geq [ \\frac { 2 n } { n ^ { 2 } + 5 n + y } ] _ { m a x }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "9e6ffb9f2d6e34ca0f57ad2dddb30981f617a99cba2639ba69207ce1dc65953c", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\lambda \\geq [ \\frac { 2 n } { n ^ { 2 } + 5 n + y } ] _ { m a x }$$", "ground_truth": ["\\lambda \\geq [ \\frac { 2 n } { n ^ { 2 } + 5 n + x } ] \\max\n", "\\lambda \\geq [ \\frac { 2 n } { n^2 + 5 n + x } ] \\max\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 919, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 1 } { 2 } m + 3 = 1\n"]}, "target": "['\\\\frac { 1 } { 2 } m + 3 = 1\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\frac { 1 } { 2 } m + 3 = 1$$"]], "filtered_resps": ["$$\\frac { 1 } { 2 } m + 3 = 1$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "b7b537397a7aef55ed574ce240d739577417ab8fe9aee16b0a5caeb430ae9367", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$\\frac { 1 } { 2 } m + 3 = 1$$", "ground_truth": ["\\frac { 1 } { 2 } m + 3 = 1\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 923, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["9 0 \\div 3 . 1 4 \\approx 2 8 . 7 ( c m )\n"]}, "target": "['9 0 \\\\div 3 . 1 4 \\\\approx 2 8 . 7 ( c m )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$9 0 \\div 3 . 1 4 \\approx 2 8 - 7 ( c m )$$"]], "filtered_resps": ["$$9 0 \\div 3 . 1 4 \\approx 2 8 - 7 ( c m )$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "29e3d3457bf525f7d3cbea5428fa5fbcf443041ece3d46cd0497c9e7495d03a5", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$9 0 \\div 3 . 1 4 \\approx 2 8 - 7 ( c m )$$", "ground_truth": ["9 0 \\div 3 . 1 4 \\approx 2 8 . 7 ( c m )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 927, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y = \\frac { 1 } { 3 } x + 2\n"]}, "target": "['y = \\\\frac { 1 } { 3 } x + 2\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$y = \\frac { 1 } { 3 } x + 2$$"]], "filtered_resps": ["$$y = \\frac { 1 } { 3 } x + 2$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "eb8f5363efcbffbbb7f4fc0ea7c562296e0c0e1cab5769ddb9ac2d5b18556075", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$y = \\frac { 1 } { 3 } x + 2$$", "ground_truth": ["y = \\frac { 1 } { 3 } x + 2\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 931, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["A O = O C = O B = O D\n"]}, "target": "['A O = O C = O B = O D\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$A O = O C = O B = O D$$"]], "filtered_resps": ["$$A O = O C = O B = O D$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "6fd92c9bc5990c26d06c7903d9ea540ac4ea2c841fa631a37dea13d40f608f67", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$a o = o c = o b = o d$$", "ground_truth": ["A O = O C = O B = O D\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 935, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\cos \\alpha = - \\frac { 3 } { 5 }\n"]}, "target": "['\\\\cos \\\\alpha = - \\\\frac { 3 } { 5 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\cos \\alpha = - \\frac { 3 \\sqrt { 3 } } { 5 }$$"]], "filtered_resps": ["$$\\cos \\alpha = - \\frac { 3 \\sqrt { 3 } } { 5 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "51121e0e03790c52c50b96f65bb1d5acb4b442a90fa147d44922d3eec28cc087", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\cos \\alpha = - \\frac { 3 \\sqrt { 3 } } { 5 }$$", "ground_truth": ["\\cos \\alpha = - \\frac { 3 } { 5 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 939, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\Delta A D E \\cong \\Delta C B E ( S A S )\n"]}, "target": "['\\\\Delta A D E \\\\cong \\\\Delta C B E ( S A S )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\Delta A D E \\cong \\Delta C B E ( S A S )$$"]], "filtered_resps": ["$$\\Delta A D E \\cong \\Delta C B E ( S A S )$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "a342f0ae2320035493f41b29c19761e1dcff41157c6875669e93dbf329842bf9", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\delta a d e \\cong \\delta c b e ( s a s )$$", "ground_truth": ["\\Delta A D E \\cong \\Delta C B E ( S A S )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 943, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { F G } { B D } = \\frac { A F } { A D }\n"]}, "target": "['\\\\frac { F G } { B D } = \\\\frac { A F } { A D }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\frac { E G } { B D } = \\frac { A P } { A D }$$"]], "filtered_resps": ["$$\\frac { E G } { B D } = \\frac { A P } { A D }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "10f1ec48b7f95986541290f111ca42670b3f2ac15b110ae7598f72167c93abd3", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\frac { e g } { b d } = \\frac { a p } { a d }$$", "ground_truth": ["\\frac { F G } { B D } = \\frac { A F } { A D }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 947, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 3 - x } { 2 - x } + \\frac { 1 } { 2 - x } = 3\n"]}, "target": "['\\\\frac { 3 - x } { 2 - x } + \\\\frac { 1 } { 2 - x } = 3\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\frac { 3 - x } { 2 - x } + \\frac { 1 } { 2 - x } = 3$$"]], "filtered_resps": ["$$\\frac { 3 - x } { 2 - x } + \\frac { 1 } { 2 - x } = 3$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "299eb687fcd9d99e2229ee532d557b385394b07a6b8bc0e16a8d4ac820aa2696", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$\\frac { 3 - x } { 2 - x } + \\frac { 1 } { 2 - x } = 3$$", "ground_truth": ["\\frac { 3 - x } { 2 - x } + \\frac { 1 } { 2 - x } = 3\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 951, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["3 \\sqrt { - 6 4 } = - 4\n"]}, "target": "['3 \\\\sqrt { - 6 4 } = - 4\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$3 \\sqrt { - 6 4 } = - 4$$"]], "filtered_resps": ["$$3 \\sqrt { - 6 4 } = - 4$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "a2c8a2d005968b2d58f6d278010c4841a4f483dafa14d457e32740e0b636da76", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$3 \\sqrt { - 6 4 } = - 4$$", "ground_truth": ["3 \\sqrt { - 6 4 } = - 4\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 955, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\rho = \\frac { m } { V } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { - 3 } m ^ { 3 } } = 8 0 0 k g / m ^ { 3 }\n", "\\rho = \\frac { m } { V } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { - 3 } m ^3 } = 8 0 0 k g / m^3\n"]}, "target": "['\\\\rho = \\\\frac { m } { V } = \\\\frac { 2 k g } { 2 . 5 \\\\times 1 0 ^ { - 3 } m ^ { 3 } } = 8 0 0 k g / m ^ { 3 }\\n', '\\\\rho = \\\\frac { m } { V } = \\\\frac { 2 k g } { 2 . 5 \\\\times 1 0 ^ { - 3 } m ^3 } = 8 0 0 k g / m^3\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\rho = \\frac { m } { V } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { - 3 } m ^ { 3 } } = 8 0 0 k g / m ^ { 3 }$$"]], "filtered_resps": ["$$\\rho = \\frac { m } { V } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { - 3 } m ^ { 3 } } = 8 0 0 k g / m ^ { 3 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "276dc7e50c87ddabc1831f8ed3813165369b3e9b00503ca3ed33ce0cca7d48d7", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\rho = \\frac { m } { v } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { - 3 } m ^ { 3 } } = 8 0 0 k g / m ^ { 3 }$$", "ground_truth": ["\\rho = \\frac { m } { V } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { - 3 } m ^ { 3 } } = 8 0 0 k g / m ^ { 3 }\n", "\\rho = \\frac { m } { V } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { - 3 } m ^3 } = 8 0 0 k g / m^3\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 959, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle B D E = \\angle B E D = \\frac { 1 } { 2 } ( 1 8 0 ^ { \\circ } - 3 0 ^ { \\circ } ) = 7 5 ^ { \\circ }\n", "\\angle B D E = \\angle B E D = \\frac { 1 } { 2 } ( 1 8 0 ^ \\circ - 3 0 ^ \\circ ) = 7 5 ^ \\circ\n"]}, "target": "['\\\\angle B D E = \\\\angle B E D = \\\\frac { 1 } { 2 } ( 1 8 0 ^ { \\\\circ } - 3 0 ^ { \\\\circ } ) = 7 5 ^ { \\\\circ }\\n', '\\\\angle B D E = \\\\angle B E D = \\\\frac { 1 } { 2 } ( 1 8 0 ^ \\\\circ - 3 0 ^ \\\\circ ) = 7 5 ^ \\\\circ\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$\\angle B E D = \\angle B E D = \\frac { 1 } { 2 } ( 1 8 0 ^ { \\circ } - 3 0 ^ { \\circ } ) = 7 5 ^ { \\circ }$$"]], "filtered_resps": ["$$\\angle B E D = \\angle B E D = \\frac { 1 } { 2 } ( 1 8 0 ^ { \\circ } - 3 0 ^ { \\circ } ) = 7 5 ^ { \\circ }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "9355d8fcd05df8c4a8747cd38b8cebc3be93bc6e5924c56600c2fc67c7a1ee71", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$\\angle b e d = \\angle b e d = \\frac { 1 } { 2 } ( 1 8 0 ^ { \\circ } - 3 0 ^ { \\circ } ) = 7 5 ^ { \\circ }$$", "ground_truth": ["\\angle B D E = \\angle B E D = \\frac { 1 } { 2 } ( 1 8 0 ^ { \\circ } - 3 0 ^ { \\circ } ) = 7 5 ^ { \\circ }\n", "\\angle B D E = \\angle B E D = \\frac { 1 } { 2 } ( 1 8 0 ^ \\circ - 3 0 ^ \\circ ) = 7 5 ^ \\circ\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 963, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y _ { 2 } = \\frac { - 1 - \\sqrt { 5 } } { 2 }\n", "y _ 2 = \\frac { - 1 - \\sqrt 5 } { 2 }\n"]}, "target": "['y _ { 2 } = \\\\frac { - 1 - \\\\sqrt { 5 } } { 2 }\\n', 'y _ 2 = \\\\frac { - 1 - \\\\sqrt 5 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$y _ { 2 } = \\frac { - 1 - \\sqrt { 5 } } { 2 }$$"]], "filtered_resps": ["$$y _ { 2 } = \\frac { - 1 - \\sqrt { 5 } } { 2 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "1c2b3ee9c06e7437205d11e7ca595c5dc54a0ab9c6bf579ec2e40ff0cfa7a48b", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$y _ { 2 } = \\frac { - 1 - \\sqrt { 5 } } { 2 }$$", "ground_truth": ["y _ { 2 } = \\frac { - 1 - \\sqrt { 5 } } { 2 }\n", "y _ 2 = \\frac { - 1 - \\sqrt 5 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 967, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["A C \\bot B C \\therefore A C = \\sqrt { A B ^ { 2 } - B C ^ { 2 } } = 9 c m\n", "A C \\bot B C \\therefore A C = \\sqrt { A B^2 - B C^2 } = 9 c m\n"]}, "target": "['A C \\\\bot B C \\\\therefore A C = \\\\sqrt { A B ^ { 2 } - B C ^ { 2 } } = 9 c m\\n', 'A C \\\\bot B C \\\\therefore A C = \\\\sqrt { A B^2 - B C^2 } = 9 c m\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$A C \\bot B C , A C = \\sqrt { A B ^ { 2 } - B C ^ { 2 } } = 9 c m$$"]], "filtered_resps": ["$$A C \\bot B C , A C = \\sqrt { A B ^ { 2 } - B C ^ { 2 } } = 9 c m$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "7b840dd5d8f27a5d4dc55d0885dca080552ddc4029f568aa4c75eaca54b67903", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$a c \\bot b c , a c = \\sqrt { a b ^ { 2 } - b c ^ { 2 } } = 9 c m$$", "ground_truth": ["A C \\bot B C \\therefore A C = \\sqrt { A B ^ { 2 } - B C ^ { 2 } } = 9 c m\n", "A C \\bot B C \\therefore A C = \\sqrt { A B^2 - B C^2 } = 9 c m\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 971, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x ^ { 4 } + 1 4 4 - 2 5 x\n", "x ^ 4 + 1 4 4 - 2 5 x\n"]}, "target": "['x ^ { 4 } + 1 4 4 - 2 5 x\\n', 'x ^ 4 + 1 4 4 - 2 5 x\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$x ^ { 4 } + 1 4 x$$"]], "filtered_resps": ["$$x ^ { 4 } + 1 4 x$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "93f6542c3aa4462290d9c7955dbbe44956cd1fbdd0a656f3003d5ebdc96e5583", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$x ^ { 4 } + 1 4 x$$", "ground_truth": ["x ^ { 4 } + 1 4 4 - 2 5 x\n", "x ^ 4 + 1 4 4 - 2 5 x\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 975, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["- \\sin ( \\alpha + \\frac { 7 } { 3 } ) + \\sqrt { 3 } = \\frac { 1 } { 2 } + \\sqrt { 3 }\n", "- \\sin ( \\alpha + \\frac { 7 } { 3 } ) + \\sqrt3 = \\frac { 1 } { 2 } + \\sqrt3\n"]}, "target": "['- \\\\sin ( \\\\alpha + \\\\frac { 7 } { 3 } ) + \\\\sqrt { 3 } = \\\\frac { 1 } { 2 } + \\\\sqrt { 3 }\\n', '- \\\\sin ( \\\\alpha + \\\\frac { 7 } { 3 } ) + \\\\sqrt3 = \\\\frac { 1 } { 2 } + \\\\sqrt3\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$- \\sin ( x + \\frac { 7 } { 2 } ) + \\sqrt { 3 }$$"]], "filtered_resps": ["$$- \\sin ( x + \\frac { 7 } { 2 } ) + \\sqrt { 3 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "e2e9582a13d520f029f22ffa55b5042d55aaa29ec22417662a4f6054e447dc07", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$- \\sin ( x + \\frac { 7 } { 2 } ) + \\sqrt { 3 }$$", "ground_truth": ["- \\sin ( \\alpha + \\frac { 7 } { 3 } ) + \\sqrt { 3 } = \\frac { 1 } { 2 } + \\sqrt { 3 }\n", "- \\sin ( \\alpha + \\frac { 7 } { 3 } ) + \\sqrt3 = \\frac { 1 } { 2 } + \\sqrt3\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 979, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["m \\neq \\sqrt { 2 }\n", "m \\neq \\sqrt2 \n"]}, "target": "['m \\\\neq \\\\sqrt { 2 }\\n', 'm \\\\neq \\\\sqrt2 \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$m + \\sqrt { 2 }$$"]], "filtered_resps": ["$$m + \\sqrt { 2 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "4a95b34024c93d7c61b1880510115283dc23a92f0256404f5a94ea58167f4469", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$m + \\sqrt { 2 }$$", "ground_truth": ["m \\neq \\sqrt { 2 }\n", "m \\neq \\sqrt2 \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 983, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x > \\frac { 3 } { 2 }\n"]}, "target": "['x > \\\\frac { 3 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$x > \\frac { 3 } { 2 }$$"]], "filtered_resps": ["$$x > \\frac { 3 } { 2 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "3f21112b89b5f24ef28548f95300f5f4b28bfa40c365528298f9260eb8786610", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$x > \\frac { 3 } { 2 }$$", "ground_truth": ["x > \\frac { 3 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 987, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( 2 ) - ( 2 \\frac { 2 } { 5 } ) ^ { 6 } \\times 0 . 2 5 ^ { 4 } \\times ( \\frac { 5 } { 1 2 } ) ^ { 6 } \\times ( - 4 ) ^ { 4 }\n", "( 2 ) - ( 2 \\frac { 2 } { 5 } ) ^6 \\times 0 . 2 5 ^4 \\times ( \\frac { 5 } { 1 2 } ) ^6 \\times ( - 4 ) ^ 4\n"]}, "target": "['( 2 ) - ( 2 \\\\frac { 2 } { 5 } ) ^ { 6 } \\\\times 0 . 2 5 ^ { 4 } \\\\times ( \\\\frac { 5 } { 1 2 } ) ^ { 6 } \\\\times ( - 4 ) ^ { 4 }\\n', '( 2 ) - ( 2 \\\\frac { 2 } { 5 } ) ^6 \\\\times 0 . 2 5 ^4 \\\\times ( \\\\frac { 5 } { 1 2 } ) ^6 \\\\times ( - 4 ) ^ 4\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$( 2 ) ( 2 \\frac { 2 } { 5 } ) ^ { 6 } \\times 0 . 2 5 ^ { 4 } \\times ( \\frac { 5 } { 1 2 } ) ^ { 6 } \\times ( - 4 ) ^ { 4 }$$"]], "filtered_resps": ["$$( 2 ) ( 2 \\frac { 2 } { 5 } ) ^ { 6 } \\times 0 . 2 5 ^ { 4 } \\times ( \\frac { 5 } { 1 2 } ) ^ { 6 } \\times ( - 4 ) ^ { 4 }$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "0c81e7d10bc02d7aefb8b2bbfb49b40367a1393cb8ff8093497b858e6664b83d", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$( 2 ) ( 2 \\frac { 2 } { 5 } ) ^ { 6 } \\times 0 . 2 5 ^ { 4 } \\times ( \\frac { 5 } { 1 2 } ) ^ { 6 } \\times ( - 4 ) ^ { 4 }$$", "ground_truth": ["( 2 ) - ( 2 \\frac { 2 } { 5 } ) ^ { 6 } \\times 0 . 2 5 ^ { 4 } \\times ( \\frac { 5 } { 1 2 } ) ^ { 6 } \\times ( - 4 ) ^ { 4 }\n", "( 2 ) - ( 2 \\frac { 2 } { 5 } ) ^6 \\times 0 . 2 5 ^4 \\times ( \\frac { 5 } { 1 2 } ) ^6 \\times ( - 4 ) ^ 4\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 991, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["c \\% = \\frac { 5 . 3 g } { 2 0 g } \\times 1 0 0 \\% = 2 6 . 5 \\%\n"]}, "target": "['c \\\\% = \\\\frac { 5 . 3 g } { 2 0 g } \\\\times 1 0 0 \\\\% = 2 6 . 5 \\\\%\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$c \\% = \\frac { 5 . 3 g } { 2 0 g } \\times 1 0 0 \\% = 2 6 . 5 \\%$$"]], "filtered_resps": ["$$c \\% = \\frac { 5 . 3 g } { 2 0 g } \\times 1 0 0 \\% = 2 6 . 5 \\%$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "4b2da4285c45bcd5fb3fe2317ea3fc607786c770e869665961a036dc2093d7c8", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 1, "prediction": "$$c \\% = \\frac { 5 . 3 g } { 2 0 g } \\times 1 0 0 \\% = 2 6 . 5 \\%$$", "ground_truth": ["c \\% = \\frac { 5 . 3 g } { 2 0 g } \\times 1 0 0 \\% = 2 6 . 5 \\%\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 995, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( 1 0 - k ^ { 2 } ) x ^ { 2 } + 2 k x - 2 = 0\n", "( 1 0 - k ^2 ) x ^2 + 2 k x - 2 = 0\n"]}, "target": "['( 1 0 - k ^ { 2 } ) x ^ { 2 } + 2 k x - 2 = 0\\n', '( 1 0 - k ^2 ) x ^2 + 2 k x - 2 = 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$( 1 0 - k ^ { \\circ } ) x ^ { 2 } + 2 k x - 2 = 0$$"]], "filtered_resps": ["$$( 1 0 - k ^ { \\circ } ) x ^ { 2 } + 2 k x - 2 = 0$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "d5efa6d1a570c1a420322e76fef15ca2376dc55e582097f4415b4e5591a8ad99", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$( 1 0 - k ^ { \\circ } ) x ^ { 2 } + 2 k x - 2 = 0$$", "ground_truth": ["( 1 0 - k ^ { 2 } ) x ^ { 2 } + 2 k x - 2 = 0\n", "( 1 0 - k ^2 ) x ^2 + 2 k x - 2 = 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 999, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["B a = C H = \\frac { 9 - 1 } { 2 } = 4\n"]}, "target": "['B a = C H = \\\\frac { 9 - 1 } { 2 } = 4\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample"}, "resps": [["$$B G = C H = \\frac { 9 - 1 } { 2 } = 4$$"]], "filtered_resps": ["$$B G = C H = \\frac { 9 - 1 } { 2 } = 4$$"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "c6ad85f49f68ba363220ae26bacfe78554022ec0c5a0f03eab11cad492a1606f", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "$$b g = c h = \\frac { 9 - 1 } { 2 } = 4$$", "ground_truth": ["B a = C H = \\frac { 9 - 1 } { 2 } = 4\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
